nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2024 NVIDIA Corporation
Built on Thu_Mar_28_02:18:24_PDT_2024
Cuda compilation tools, release 12.4, V12.4.131
Build cuda_12.4.r12.4/compiler.34097967_0
Fri Mar 21 11:17:46 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA L40S                    On  |   00000000:81:00.0 Off |                    0 |
| N/A   36C    P8             32W /  350W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA L40S                    On  |   00000000:C1:00.0 Off |                    0 |
| N/A   36C    P8             32W /  350W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
============================= test session starts ==============================
platform linux -- Python 3.9.21, pytest-8.3.5, pluggy-1.5.0 -- /home/abcheng/miniconda3/envs/hw4/bin/python
cachedir: .pytest_cache
rootdir: /home/abcheng/workspace/llmsys_s25_hw4
configfile: pytest.ini
collecting ... collected 34 items / 30 deselected / 4 selected

tests/test_pipeline.py::test_clock_cycles_0 PASSED                       [ 25%]
tests/test_pipeline.py::test_clock_cycles_1 PASSED                       [ 50%]
tests/test_pipeline.py::test_split_module_0 PASSED                       [ 75%]
tests/test_pipeline.py::test_split_module_1 PASSED                       [100%]

======================= 4 passed, 30 deselected in 2.34s =======================
============================= test session starts ==============================
platform linux -- Python 3.9.21, pytest-8.3.5, pluggy-1.5.0 -- /home/abcheng/miniconda3/envs/hw4/bin/python
cachedir: .pytest_cache
rootdir: /home/abcheng/workspace/llmsys_s25_hw4
configfile: pytest.ini
collecting ... collected 34 items / 14 deselected / 20 selected

tests/test_pipeline.py::test_forward_0[1-1] FAILED                       [  5%]
tests/test_pipeline.py::test_forward_0[1-16] FAILED                      [ 10%]
tests/test_pipeline.py::test_forward_0[1-32] FAILED                      [ 15%]
tests/test_pipeline.py::test_forward_0[1-64] FAILED                      [ 20%]
tests/test_pipeline.py::test_forward_0[2-1] FAILED                       [ 25%]
tests/test_pipeline.py::test_forward_0[2-16] FAILED                      [ 30%]
tests/test_pipeline.py::test_forward_0[2-32] FAILED                      [ 35%]
tests/test_pipeline.py::test_forward_0[2-64] FAILED                      [ 40%]
tests/test_pipeline.py::test_forward_0[4-1] FAILED                       [ 45%]
tests/test_pipeline.py::test_forward_0[4-16] FAILED                      [ 50%]
tests/test_pipeline.py::test_forward_0[4-32] FAILED                      [ 55%]
tests/test_pipeline.py::test_forward_0[4-64] FAILED                      [ 60%]
tests/test_pipeline.py::test_forward_0[8-1] FAILED                       [ 65%]
tests/test_pipeline.py::test_forward_0[8-16] FAILED                      [ 70%]
tests/test_pipeline.py::test_forward_0[8-32] FAILED                      [ 75%]
tests/test_pipeline.py::test_forward_0[8-64] FAILED                      [ 80%]
tests/test_pipeline.py::test_forward_0[16-1] FAILED                      [ 85%]
tests/test_pipeline.py::test_forward_0[16-16] FAILED                     [ 90%]
tests/test_pipeline.py::test_forward_0[16-32] FAILED                     [ 95%]
tests/test_pipeline.py::test_forward_0[16-64] FAILED                     [100%]

=================================== FAILURES ===================================
_____________________________ test_forward_0[1-1] ______________________________

batch_size = 1, split_size = 1

    @pytest.mark.a4_2_2
    @pytest.mark.parametrize("batch_size", [1, 16, 32, 64])
    @pytest.mark.parametrize("split_size", [1, 2, 4, 8, 16])
    # @pytest.mark.parametrize("batch_size", [1])
    # @pytest.mark.parametrize("split_size", [1, 2])
    def test_forward_0(batch_size, split_size):
        model = nn.Sequential(
            nn.Linear(3, 4).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
            nn.Linear(4, 5).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
        )
    
        x = torch.randn(batch_size, 3).to('cuda:0')
        y0 = model(x).to('cpu')
    
        # move the last two layer to another device
        model[-2] = model[-2].to('cuda:1')
        model[-1] = WithDevice(nn.Sigmoid(), 'cuda:1')
        pipe = Pipe(model, split_size=split_size)
        y1 = pipe(x).to('cpu')
>       assert torch.allclose(y0, y1)
E       assert False
E        +  where False = <built-in method allclose of type object at 0x153da78cb760>(tensor([[0.3713, 0.4017, 0.5628, 0.5634, 0.3611]], grad_fn=<ToCopyBackward0>), tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000]], grad_fn=<ToCopyBackward0>))
E        +    where <built-in method allclose of type object at 0x153da78cb760> = torch.allclose

batch_size = 1
model      = Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
  (2): Linear(in_features=4, out_features=5, bias=True)
  (3): WithDevice(
    (_module): Sigmoid()
  )
)
pipe       = Pipe(
  (partitions): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=3, out_features=4, bias=True)
   ...inear(in_features=4, out_features=5, bias=True)
      (1): WithDevice(
        (_module): Sigmoid()
      )
    )
  )
)
split_size = 1
x          = tensor([[-0.1123,  0.3814,  1.1824]], device='cuda:0')
y0         = tensor([[0.3713, 0.4017, 0.5628, 0.5634, 0.3611]], grad_fn=<ToCopyBackward0>)
y1         = tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000]], grad_fn=<ToCopyBackward0>)

tests/test_pipeline.py:83: AssertionError
----------------------------- Captured stdout call -----------------------------
in init
ModuleList(
  (0): Sequential(
    (0): Linear(in_features=3, out_features=4, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
  (1): Sequential(
    (0): Linear(in_features=4, out_features=5, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
)
[device(type='cuda', index=0), device(type='cuda', index=1)]
before running the cycle
[tensor([[-0.1123,  0.3814,  1.1824]], device='cuda:0')]
====================
schedule for this step is
[(0, 0)]
inputting microbatch 0 into partition 0
before moving to cuda:0: tensor([[-0.1123,  0.3814,  1.1824]], device='cuda:0')
after: tensor([[-0.1123,  0.3814,  1.1824]], device='cuda:0')
********************
observing microbatch 0
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.1123,  0.3814,  1.1824]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 0
********************
receiving microbatch 0 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 0, tensor([[0.7132, 0.2830, 0.4619, 0.5144]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.7132, 0.2830, 0.4619, 0.5144]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.7132, 0.2830, 0.4619, 0.5144]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.7132, 0.2830, 0.4619, 0.5144]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)]
====================
schedule for this step is
[(0, 1)]
inputting microbatch 0 into partition 1
before moving to cuda:1: tensor([[0.7132, 0.2830, 0.4619, 0.5144]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0., 0., 0., 0.]], device='cuda:1', grad_fn=<ToCopyBackward0>)
********************
observing microbatch 0
current batch shape is torch.Size([1, 4])
batch is tensor([[0., 0., 0., 0.]], device='cuda:1', grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 0
********************
receiving microbatch 0 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 0, tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)]
finished running cycle
_____________________________ test_forward_0[1-16] _____________________________

batch_size = 16, split_size = 1

    @pytest.mark.a4_2_2
    @pytest.mark.parametrize("batch_size", [1, 16, 32, 64])
    @pytest.mark.parametrize("split_size", [1, 2, 4, 8, 16])
    # @pytest.mark.parametrize("batch_size", [1])
    # @pytest.mark.parametrize("split_size", [1, 2])
    def test_forward_0(batch_size, split_size):
        model = nn.Sequential(
            nn.Linear(3, 4).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
            nn.Linear(4, 5).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
        )
    
        x = torch.randn(batch_size, 3).to('cuda:0')
        y0 = model(x).to('cpu')
    
        # move the last two layer to another device
        model[-2] = model[-2].to('cuda:1')
        model[-1] = WithDevice(nn.Sigmoid(), 'cuda:1')
        pipe = Pipe(model, split_size=split_size)
        y1 = pipe(x).to('cpu')
>       assert torch.allclose(y0, y1)
E       assert False
E        +  where False = <built-in method allclose of type object at 0x153da78cb760>(tensor([[0.4428, 0.6159, 0.5037, 0.5056, 0.6173],\n        [0.4459, 0.6083, 0.4943, 0.5121, 0.6121],\n        [0.4767, 0.6409, 0.4685, 0.5126, 0.5796],\n        [0.4503, 0.6109, 0.4883, 0.5047, 0.6102],\n        [0.4620, 0.6345, 0.4867, 0.5076, 0.5972],\n        [0.4618, 0.6320, 0.4866, 0.5186, 0.5934],\n        [0.4513, 0.6011, 0.4818, 0.5204, 0.6041],\n        [0.4498, 0.6054, 0.4856, 0.5079, 0.6097],\n        [0.4733, 0.6359, 0.4689, 0.5003, 0.5877],\n        [0.4778, 0.6440, 0.4681, 0.5048, 0.5813],\n        [0.4716, 0.6468, 0.4813, 0.5117, 0.5862],\n        [0.4830, 0.6283, 0.4469, 0.4926, 0.5794],\n        [0.4633, 0.6414, 0.4901, 0.5097, 0.5953],\n        [0.4549, 0.6190, 0.4873, 0.5100, 0.6037],\n        [0.4718, 0.6322, 0.4696, 0.5093, 0.5860],\n        [0.4604, 0.6147, 0.4735, 0.4947, 0.6030]], grad_fn=<ToCopyBackward0>), tensor([[0.8520, 0.8520, 0.6225, 0.6225, 0.6225],\n        [0.8137, 0.8981, 0.6225, 0.6225, 0.6225],\n        [0.8143, 0.8963, 0.6225, 0.6225, 0.6225],\n        [0.8142, 0.8963, 0.6225, 0.6225, 0.6225],\n        [0.8142, 0.8963, 0.6225, 0.6225, 0.6225],\n        [0.8142, 0.8963, 0.6225, 0.6225, 0.6225],\n        [0.8142, 0.8963, 0.6225, 0.6225, 0.6225],\n        [0.8142, 0.8963, 0.6225, 0.6225, 0.6225],\n        [0.8142, 0.8963, 0.6225, 0.6225, 0.6225],\n        [0.8142, 0.8963, 0.6225, 0.6225, 0.6225],\n        [0.8142, 0.8963, 0.6225, 0.6225, 0.6225],\n        [0.8142, 0.8963, 0.6225, 0.6225, 0.6225],\n        [0.8142, 0.8963, 0.6225, 0.6225, 0.6225],\n        [0.8142, 0.8963, 0.6225, 0.6225, 0.6225],\n        [0.8142, 0.8963, 0.6225, 0.6225, 0.6225],\n        [0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], grad_fn=<ToCopyBackward0>))
E        +    where <built-in method allclose of type object at 0x153da78cb760> = torch.allclose

batch_size = 16
model      = Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
  (2): Linear(in_features=4, out_features=5, bias=True)
  (3): WithDevice(
    (_module): Sigmoid()
  )
)
pipe       = Pipe(
  (partitions): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=3, out_features=4, bias=True)
   ...inear(in_features=4, out_features=5, bias=True)
      (1): WithDevice(
        (_module): Sigmoid()
      )
    )
  )
)
split_size = 1
x          = tensor([[-1.7670, -1.4181,  0.2804],
        [-0.1052, -0.9746,  0.6316],
        [ 0.1143,  1.1685,  0.3764],
       ...-0.3503, -0.4953,  0.2739],
        [ 0.1900,  0.6186, -0.0598],
        [-0.6248, -0.8262, -1.7463]], device='cuda:0')
y0         = tensor([[0.4428, 0.6159, 0.5037, 0.5056, 0.6173],
        [0.4459, 0.6083, 0.4943, 0.5121, 0.6121],
        [0.4767, 0...[0.4718, 0.6322, 0.4696, 0.5093, 0.5860],
        [0.4604, 0.6147, 0.4735, 0.4947, 0.6030]], grad_fn=<ToCopyBackward0>)
y1         = tensor([[0.8520, 0.8520, 0.6225, 0.6225, 0.6225],
        [0.8137, 0.8981, 0.6225, 0.6225, 0.6225],
        [0.8143, 0...[0.8142, 0.8963, 0.6225, 0.6225, 0.6225],
        [0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], grad_fn=<ToCopyBackward0>)

tests/test_pipeline.py:83: AssertionError
----------------------------- Captured stdout call -----------------------------
in init
ModuleList(
  (0): Sequential(
    (0): Linear(in_features=3, out_features=4, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
  (1): Sequential(
    (0): Linear(in_features=4, out_features=5, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
)
[device(type='cuda', index=0), device(type='cuda', index=1)]
before running the cycle
[tensor([[-1.7670, -1.4181,  0.2804]], device='cuda:0'), tensor([[-0.1052, -0.9746,  0.6316]], device='cuda:0'), tensor([[0.1143, 1.1685, 0.3764]], device='cuda:0'), tensor([[-0.5207, -1.0308, -0.3600]], device='cuda:0'), tensor([[-1.3727, -0.1125,  0.1098]], device='cuda:0'), tensor([[-0.1686,  0.3748,  1.4383]], device='cuda:0'), tensor([[ 1.9318, -0.3229,  1.1138]], device='cuda:0'), tensor([[ 0.2490, -0.9419, -0.1221]], device='cuda:0'), tensor([[-0.8360,  0.3124, -1.0434]], device='cuda:0'), tensor([[-0.8391,  0.8704, -0.5152]], device='cuda:0'), tensor([[-1.4188,  0.7293,  0.4526]], device='cuda:0'), tensor([[ 0.5772,  0.6933, -2.4359]], device='cuda:0'), tensor([[-1.8648,  0.0898,  0.4881]], device='cuda:0'), tensor([[-0.3503, -0.4953,  0.2739]], device='cuda:0'), tensor([[ 0.1900,  0.6186, -0.0598]], device='cuda:0'), tensor([[-0.6248, -0.8262, -1.7463]], device='cuda:0')]
====================
schedule for this step is
[(0, 0)]
inputting microbatch 0 into partition 0
before moving to cuda:0: tensor([[-1.7670, -1.4181,  0.2804]], device='cuda:0')
after: tensor([[-1.7670, -1.4181,  0.2804]], device='cuda:0')
********************
observing microbatch 0
current batch shape is torch.Size([1, 3])
batch is tensor([[-1.7670, -1.4181,  0.2804]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 0
********************
receiving microbatch 0 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 0, tensor([[0.5625, 0.3080, 0.4049, 0.4674]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.5625, 0.3080, 0.4049, 0.4674]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5625, 0.3080, 0.4049, 0.4674]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.5625, 0.3080, 0.4049, 0.4674]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.1052, -0.9746,  0.6316]], device='cuda:0'), tensor([[0.1143, 1.1685, 0.3764]], device='cuda:0'), tensor([[-0.5207, -1.0308, -0.3600]], device='cuda:0'), tensor([[-1.3727, -0.1125,  0.1098]], device='cuda:0'), tensor([[-0.1686,  0.3748,  1.4383]], device='cuda:0'), tensor([[ 1.9318, -0.3229,  1.1138]], device='cuda:0'), tensor([[ 0.2490, -0.9419, -0.1221]], device='cuda:0'), tensor([[-0.8360,  0.3124, -1.0434]], device='cuda:0'), tensor([[-0.8391,  0.8704, -0.5152]], device='cuda:0'), tensor([[-1.4188,  0.7293,  0.4526]], device='cuda:0'), tensor([[ 0.5772,  0.6933, -2.4359]], device='cuda:0'), tensor([[-1.8648,  0.0898,  0.4881]], device='cuda:0'), tensor([[-0.3503, -0.4953,  0.2739]], device='cuda:0'), tensor([[ 0.1900,  0.6186, -0.0598]], device='cuda:0'), tensor([[-0.6248, -0.8262, -1.7463]], device='cuda:0')]
====================
schedule for this step is
[(1, 0), (0, 1)]
inputting microbatch 1 into partition 0
before moving to cuda:0: tensor([[-0.1052, -0.9746,  0.6316]], device='cuda:0')
after: tensor([[-0.1052, -0.9746,  0.6316]], device='cuda:0')
********************
observing microbatch 1
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.1052, -0.9746,  0.6316]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 1
********************
inputting microbatch 0 into partition 1
before moving to cuda:1: tensor([[0.5625, 0.3080, 0.4049, 0.4674]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.0000, 1.7500, 0.0000, 1.7500]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 0
current batch shape is torch.Size([1, 4])
batch is tensor([[0.0000, 1.7500, 0.0000, 1.7500]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 0
********************
receiving microbatch 1 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 1, tensor([[0.4895, 0.3404, 0.4731, 0.5237]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 1
result is tensor([[0.4895, 0.3404, 0.4731, 0.5237]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4895, 0.3404, 0.4731, 0.5237]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 0 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 0, tensor([[0.8520, 0.8520, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.8520, 0.8520, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.8520, 0.8520, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.8520, 0.8520, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4895, 0.3404, 0.4731, 0.5237]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[0.1143, 1.1685, 0.3764]], device='cuda:0'), tensor([[-0.5207, -1.0308, -0.3600]], device='cuda:0'), tensor([[-1.3727, -0.1125,  0.1098]], device='cuda:0'), tensor([[-0.1686,  0.3748,  1.4383]], device='cuda:0'), tensor([[ 1.9318, -0.3229,  1.1138]], device='cuda:0'), tensor([[ 0.2490, -0.9419, -0.1221]], device='cuda:0'), tensor([[-0.8360,  0.3124, -1.0434]], device='cuda:0'), tensor([[-0.8391,  0.8704, -0.5152]], device='cuda:0'), tensor([[-1.4188,  0.7293,  0.4526]], device='cuda:0'), tensor([[ 0.5772,  0.6933, -2.4359]], device='cuda:0'), tensor([[-1.8648,  0.0898,  0.4881]], device='cuda:0'), tensor([[-0.3503, -0.4953,  0.2739]], device='cuda:0'), tensor([[ 0.1900,  0.6186, -0.0598]], device='cuda:0'), tensor([[-0.6248, -0.8262, -1.7463]], device='cuda:0')]
====================
schedule for this step is
[(2, 0), (1, 1)]
inputting microbatch 2 into partition 0
before moving to cuda:0: tensor([[0.1143, 1.1685, 0.3764]], device='cuda:0')
after: tensor([[0.1143, 1.1685, 0.3764]], device='cuda:0')
********************
observing microbatch 2
current batch shape is torch.Size([1, 3])
batch is tensor([[0.1143, 1.1685, 0.3764]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 2
********************
inputting microbatch 1 into partition 1
before moving to cuda:1: tensor([[0.4895, 0.3404, 0.4731, 0.5237]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.8520, 0.8520, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 1
current batch shape is torch.Size([1, 4])
batch is tensor([[0.8520, 0.8520, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 1
********************
receiving microbatch 2 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 2, tensor([[0.3806, 0.4222, 0.7313, 0.2818]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 2
result is tensor([[0.3806, 0.4222, 0.7313, 0.2818]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3806, 0.4222, 0.7313, 0.2818]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 1 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 1, tensor([[0.8137, 0.8981, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 1
result is tensor([[0.8137, 0.8981, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.8137, 0.8981, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.8520, 0.8520, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8137, 0.8981, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3806, 0.4222, 0.7313, 0.2818]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.5207, -1.0308, -0.3600]], device='cuda:0'), tensor([[-1.3727, -0.1125,  0.1098]], device='cuda:0'), tensor([[-0.1686,  0.3748,  1.4383]], device='cuda:0'), tensor([[ 1.9318, -0.3229,  1.1138]], device='cuda:0'), tensor([[ 0.2490, -0.9419, -0.1221]], device='cuda:0'), tensor([[-0.8360,  0.3124, -1.0434]], device='cuda:0'), tensor([[-0.8391,  0.8704, -0.5152]], device='cuda:0'), tensor([[-1.4188,  0.7293,  0.4526]], device='cuda:0'), tensor([[ 0.5772,  0.6933, -2.4359]], device='cuda:0'), tensor([[-1.8648,  0.0898,  0.4881]], device='cuda:0'), tensor([[-0.3503, -0.4953,  0.2739]], device='cuda:0'), tensor([[ 0.1900,  0.6186, -0.0598]], device='cuda:0'), tensor([[-0.6248, -0.8262, -1.7463]], device='cuda:0')]
====================
schedule for this step is
[(3, 0), (2, 1)]
inputting microbatch 3 into partition 0
before moving to cuda:0: tensor([[-0.5207, -1.0308, -0.3600]], device='cuda:0')
after: tensor([[-0.5207, -1.0308, -0.3600]], device='cuda:0')
********************
observing microbatch 3
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.5207, -1.0308, -0.3600]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 3
********************
inputting microbatch 2 into partition 1
before moving to cuda:1: tensor([[0.3806, 0.4222, 0.7313, 0.2818]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.8137, 0.8981, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 2
current batch shape is torch.Size([1, 4])
batch is tensor([[0.8137, 0.8981, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 2
********************
receiving microbatch 3 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 3, tensor([[0.5485, 0.4294, 0.4908, 0.5216]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 3
result is tensor([[0.5485, 0.4294, 0.4908, 0.5216]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5485, 0.4294, 0.4908, 0.5216]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 2 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 2, tensor([[0.8143, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 2
result is tensor([[0.8143, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.8143, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.8520, 0.8520, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8137, 0.8981, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8143, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5485, 0.4294, 0.4908, 0.5216]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-1.3727, -0.1125,  0.1098]], device='cuda:0'), tensor([[-0.1686,  0.3748,  1.4383]], device='cuda:0'), tensor([[ 1.9318, -0.3229,  1.1138]], device='cuda:0'), tensor([[ 0.2490, -0.9419, -0.1221]], device='cuda:0'), tensor([[-0.8360,  0.3124, -1.0434]], device='cuda:0'), tensor([[-0.8391,  0.8704, -0.5152]], device='cuda:0'), tensor([[-1.4188,  0.7293,  0.4526]], device='cuda:0'), tensor([[ 0.5772,  0.6933, -2.4359]], device='cuda:0'), tensor([[-1.8648,  0.0898,  0.4881]], device='cuda:0'), tensor([[-0.3503, -0.4953,  0.2739]], device='cuda:0'), tensor([[ 0.1900,  0.6186, -0.0598]], device='cuda:0'), tensor([[-0.6248, -0.8262, -1.7463]], device='cuda:0')]
====================
schedule for this step is
[(4, 0), (3, 1)]
inputting microbatch 4 into partition 0
before moving to cuda:0: tensor([[-1.3727, -0.1125,  0.1098]], device='cuda:0')
after: tensor([[-1.3727, -0.1125,  0.1098]], device='cuda:0')
********************
observing microbatch 4
current batch shape is torch.Size([1, 3])
batch is tensor([[-1.3727, -0.1125,  0.1098]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 4
********************
inputting microbatch 3 into partition 1
before moving to cuda:1: tensor([[0.5485, 0.4294, 0.4908, 0.5216]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.8143, 0.8963, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 3
current batch shape is torch.Size([1, 4])
batch is tensor([[0.8143, 0.8963, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 3
********************
receiving microbatch 4 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 4, tensor([[0.4909, 0.3661, 0.5764, 0.3360]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 4
result is tensor([[0.4909, 0.3661, 0.5764, 0.3360]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4909, 0.3661, 0.5764, 0.3360]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 3 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 3, tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 3
result is tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.8520, 0.8520, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8137, 0.8981, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8143, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4909, 0.3661, 0.5764, 0.3360]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.1686,  0.3748,  1.4383]], device='cuda:0'), tensor([[ 1.9318, -0.3229,  1.1138]], device='cuda:0'), tensor([[ 0.2490, -0.9419, -0.1221]], device='cuda:0'), tensor([[-0.8360,  0.3124, -1.0434]], device='cuda:0'), tensor([[-0.8391,  0.8704, -0.5152]], device='cuda:0'), tensor([[-1.4188,  0.7293,  0.4526]], device='cuda:0'), tensor([[ 0.5772,  0.6933, -2.4359]], device='cuda:0'), tensor([[-1.8648,  0.0898,  0.4881]], device='cuda:0'), tensor([[-0.3503, -0.4953,  0.2739]], device='cuda:0'), tensor([[ 0.1900,  0.6186, -0.0598]], device='cuda:0'), tensor([[-0.6248, -0.8262, -1.7463]], device='cuda:0')]
====================
schedule for this step is
[(5, 0), (4, 1)]
inputting microbatch 5 into partition 0
before moving to cuda:0: tensor([[-0.1686,  0.3748,  1.4383]], device='cuda:0')
after: tensor([[-0.1686,  0.3748,  1.4383]], device='cuda:0')
********************
observing microbatch 5
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.1686,  0.3748,  1.4383]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 5
********************
inputting microbatch 4 into partition 1
before moving to cuda:1: tensor([[0.4909, 0.3661, 0.5764, 0.3360]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.8142, 0.8963, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 4
current batch shape is torch.Size([1, 4])
batch is tensor([[0.8142, 0.8963, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 4
********************
receiving microbatch 5 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 5, tensor([[0.3787, 0.2864, 0.6124, 0.3330]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 5
result is tensor([[0.3787, 0.2864, 0.6124, 0.3330]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3787, 0.2864, 0.6124, 0.3330]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 4 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 4, tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 4
result is tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.8520, 0.8520, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8137, 0.8981, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8143, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3787, 0.2864, 0.6124, 0.3330]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 1.9318, -0.3229,  1.1138]], device='cuda:0'), tensor([[ 0.2490, -0.9419, -0.1221]], device='cuda:0'), tensor([[-0.8360,  0.3124, -1.0434]], device='cuda:0'), tensor([[-0.8391,  0.8704, -0.5152]], device='cuda:0'), tensor([[-1.4188,  0.7293,  0.4526]], device='cuda:0'), tensor([[ 0.5772,  0.6933, -2.4359]], device='cuda:0'), tensor([[-1.8648,  0.0898,  0.4881]], device='cuda:0'), tensor([[-0.3503, -0.4953,  0.2739]], device='cuda:0'), tensor([[ 0.1900,  0.6186, -0.0598]], device='cuda:0'), tensor([[-0.6248, -0.8262, -1.7463]], device='cuda:0')]
====================
schedule for this step is
[(6, 0), (5, 1)]
inputting microbatch 6 into partition 0
before moving to cuda:0: tensor([[ 1.9318, -0.3229,  1.1138]], device='cuda:0')
after: tensor([[ 1.9318, -0.3229,  1.1138]], device='cuda:0')
********************
observing microbatch 6
current batch shape is torch.Size([1, 3])
batch is tensor([[ 1.9318, -0.3229,  1.1138]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 6
********************
inputting microbatch 5 into partition 1
before moving to cuda:1: tensor([[0.3787, 0.2864, 0.6124, 0.3330]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.8142, 0.8963, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 5
current batch shape is torch.Size([1, 4])
batch is tensor([[0.8142, 0.8963, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 5
********************
receiving microbatch 6 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 6, tensor([[0.3927, 0.3789, 0.5695, 0.5771]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 6
result is tensor([[0.3927, 0.3789, 0.5695, 0.5771]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3927, 0.3789, 0.5695, 0.5771]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 5 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 5, tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 5
result is tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.8520, 0.8520, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8137, 0.8981, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8143, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3927, 0.3789, 0.5695, 0.5771]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.2490, -0.9419, -0.1221]], device='cuda:0'), tensor([[-0.8360,  0.3124, -1.0434]], device='cuda:0'), tensor([[-0.8391,  0.8704, -0.5152]], device='cuda:0'), tensor([[-1.4188,  0.7293,  0.4526]], device='cuda:0'), tensor([[ 0.5772,  0.6933, -2.4359]], device='cuda:0'), tensor([[-1.8648,  0.0898,  0.4881]], device='cuda:0'), tensor([[-0.3503, -0.4953,  0.2739]], device='cuda:0'), tensor([[ 0.1900,  0.6186, -0.0598]], device='cuda:0'), tensor([[-0.6248, -0.8262, -1.7463]], device='cuda:0')]
====================
schedule for this step is
[(7, 0), (6, 1)]
inputting microbatch 7 into partition 0
before moving to cuda:0: tensor([[ 0.2490, -0.9419, -0.1221]], device='cuda:0')
after: tensor([[ 0.2490, -0.9419, -0.1221]], device='cuda:0')
********************
observing microbatch 7
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.2490, -0.9419, -0.1221]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 7
********************
inputting microbatch 6 into partition 1
before moving to cuda:1: tensor([[0.3927, 0.3789, 0.5695, 0.5771]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.8142, 0.8963, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 6
current batch shape is torch.Size([1, 4])
batch is tensor([[0.8142, 0.8963, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 6
********************
receiving microbatch 7 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 7, tensor([[0.5175, 0.4352, 0.5061, 0.5610]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 7
result is tensor([[0.5175, 0.4352, 0.5061, 0.5610]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5175, 0.4352, 0.5061, 0.5610]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 6 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 6, tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 6
result is tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.8520, 0.8520, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8137, 0.8981, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8143, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5175, 0.4352, 0.5061, 0.5610]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.8360,  0.3124, -1.0434]], device='cuda:0'), tensor([[-0.8391,  0.8704, -0.5152]], device='cuda:0'), tensor([[-1.4188,  0.7293,  0.4526]], device='cuda:0'), tensor([[ 0.5772,  0.6933, -2.4359]], device='cuda:0'), tensor([[-1.8648,  0.0898,  0.4881]], device='cuda:0'), tensor([[-0.3503, -0.4953,  0.2739]], device='cuda:0'), tensor([[ 0.1900,  0.6186, -0.0598]], device='cuda:0'), tensor([[-0.6248, -0.8262, -1.7463]], device='cuda:0')]
====================
schedule for this step is
[(8, 0), (7, 1)]
inputting microbatch 8 into partition 0
before moving to cuda:0: tensor([[-0.8360,  0.3124, -1.0434]], device='cuda:0')
after: tensor([[-0.8360,  0.3124, -1.0434]], device='cuda:0')
********************
observing microbatch 8
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.8360,  0.3124, -1.0434]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 8
********************
inputting microbatch 7 into partition 1
before moving to cuda:1: tensor([[0.5175, 0.4352, 0.5061, 0.5610]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.8142, 0.8963, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 7
current batch shape is torch.Size([1, 4])
batch is tensor([[0.8142, 0.8963, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 7
********************
receiving microbatch 8 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 8, tensor([[0.5131, 0.5245, 0.6672, 0.3434]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 8
result is tensor([[0.5131, 0.5245, 0.6672, 0.3434]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5131, 0.5245, 0.6672, 0.3434]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 7 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 7, tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 7
result is tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.8520, 0.8520, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8137, 0.8981, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8143, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5131, 0.5245, 0.6672, 0.3434]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.8391,  0.8704, -0.5152]], device='cuda:0'), tensor([[-1.4188,  0.7293,  0.4526]], device='cuda:0'), tensor([[ 0.5772,  0.6933, -2.4359]], device='cuda:0'), tensor([[-1.8648,  0.0898,  0.4881]], device='cuda:0'), tensor([[-0.3503, -0.4953,  0.2739]], device='cuda:0'), tensor([[ 0.1900,  0.6186, -0.0598]], device='cuda:0'), tensor([[-0.6248, -0.8262, -1.7463]], device='cuda:0')]
====================
schedule for this step is
[(9, 0), (8, 1)]
inputting microbatch 9 into partition 0
before moving to cuda:0: tensor([[-0.8391,  0.8704, -0.5152]], device='cuda:0')
after: tensor([[-0.8391,  0.8704, -0.5152]], device='cuda:0')
********************
observing microbatch 9
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.8391,  0.8704, -0.5152]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 9
********************
inputting microbatch 8 into partition 1
before moving to cuda:1: tensor([[0.5131, 0.5245, 0.6672, 0.3434]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.8142, 0.8963, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 8
current batch shape is torch.Size([1, 4])
batch is tensor([[0.8142, 0.8963, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 8
********************
receiving microbatch 9 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 9, tensor([[0.4565, 0.4775, 0.7122, 0.2721]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 9
result is tensor([[0.4565, 0.4775, 0.7122, 0.2721]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4565, 0.4775, 0.7122, 0.2721]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 8 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 8, tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 8
result is tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.8520, 0.8520, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8137, 0.8981, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8143, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4565, 0.4775, 0.7122, 0.2721]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-1.4188,  0.7293,  0.4526]], device='cuda:0'), tensor([[ 0.5772,  0.6933, -2.4359]], device='cuda:0'), tensor([[-1.8648,  0.0898,  0.4881]], device='cuda:0'), tensor([[-0.3503, -0.4953,  0.2739]], device='cuda:0'), tensor([[ 0.1900,  0.6186, -0.0598]], device='cuda:0'), tensor([[-0.6248, -0.8262, -1.7463]], device='cuda:0')]
====================
schedule for this step is
[(10, 0), (9, 1)]
inputting microbatch 10 into partition 0
before moving to cuda:0: tensor([[-1.4188,  0.7293,  0.4526]], device='cuda:0')
after: tensor([[-1.4188,  0.7293,  0.4526]], device='cuda:0')
********************
observing microbatch 10
current batch shape is torch.Size([1, 3])
batch is tensor([[-1.4188,  0.7293,  0.4526]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 10
********************
inputting microbatch 9 into partition 1
before moving to cuda:1: tensor([[0.4565, 0.4775, 0.7122, 0.2721]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.8142, 0.8963, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 9
current batch shape is torch.Size([1, 4])
batch is tensor([[0.8142, 0.8963, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 9
********************
receiving microbatch 10 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 10, tensor([[0.4287, 0.3466, 0.6637, 0.2383]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 10
result is tensor([[0.4287, 0.3466, 0.6637, 0.2383]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4287, 0.3466, 0.6637, 0.2383]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 9 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 9, tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 9
result is tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.8520, 0.8520, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8137, 0.8981, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8143, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4287, 0.3466, 0.6637, 0.2383]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.5772,  0.6933, -2.4359]], device='cuda:0'), tensor([[-1.8648,  0.0898,  0.4881]], device='cuda:0'), tensor([[-0.3503, -0.4953,  0.2739]], device='cuda:0'), tensor([[ 0.1900,  0.6186, -0.0598]], device='cuda:0'), tensor([[-0.6248, -0.8262, -1.7463]], device='cuda:0')]
====================
schedule for this step is
[(11, 0), (10, 1)]
inputting microbatch 11 into partition 0
before moving to cuda:0: tensor([[ 0.5772,  0.6933, -2.4359]], device='cuda:0')
after: tensor([[ 0.5772,  0.6933, -2.4359]], device='cuda:0')
********************
observing microbatch 11
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.5772,  0.6933, -2.4359]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 11
********************
inputting microbatch 10 into partition 1
before moving to cuda:1: tensor([[0.4287, 0.3466, 0.6637, 0.2383]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.8142, 0.8963, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 10
current batch shape is torch.Size([1, 4])
batch is tensor([[0.8142, 0.8963, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 10
********************
receiving microbatch 11 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 11, tensor([[0.5326, 0.7290, 0.7586, 0.4218]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 11
result is tensor([[0.5326, 0.7290, 0.7586, 0.4218]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5326, 0.7290, 0.7586, 0.4218]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 10 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 10, tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 10
result is tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.8520, 0.8520, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8137, 0.8981, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8143, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5326, 0.7290, 0.7586, 0.4218]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-1.8648,  0.0898,  0.4881]], device='cuda:0'), tensor([[-0.3503, -0.4953,  0.2739]], device='cuda:0'), tensor([[ 0.1900,  0.6186, -0.0598]], device='cuda:0'), tensor([[-0.6248, -0.8262, -1.7463]], device='cuda:0')]
====================
schedule for this step is
[(12, 0), (11, 1)]
inputting microbatch 12 into partition 0
before moving to cuda:0: tensor([[-1.8648,  0.0898,  0.4881]], device='cuda:0')
after: tensor([[-1.8648,  0.0898,  0.4881]], device='cuda:0')
********************
observing microbatch 12
current batch shape is torch.Size([1, 3])
batch is tensor([[-1.8648,  0.0898,  0.4881]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 12
********************
inputting microbatch 11 into partition 1
before moving to cuda:1: tensor([[0.5326, 0.7290, 0.7586, 0.4218]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.8142, 0.8963, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 11
current batch shape is torch.Size([1, 4])
batch is tensor([[0.8142, 0.8963, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 11
********************
receiving microbatch 12 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 12, tensor([[0.4706, 0.3143, 0.5820, 0.2764]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 12
result is tensor([[0.4706, 0.3143, 0.5820, 0.2764]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4706, 0.3143, 0.5820, 0.2764]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 11 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 11, tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 11
result is tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.8520, 0.8520, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8137, 0.8981, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8143, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4706, 0.3143, 0.5820, 0.2764]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.3503, -0.4953,  0.2739]], device='cuda:0'), tensor([[ 0.1900,  0.6186, -0.0598]], device='cuda:0'), tensor([[-0.6248, -0.8262, -1.7463]], device='cuda:0')]
====================
schedule for this step is
[(13, 0), (12, 1)]
inputting microbatch 13 into partition 0
before moving to cuda:0: tensor([[-0.3503, -0.4953,  0.2739]], device='cuda:0')
after: tensor([[-0.3503, -0.4953,  0.2739]], device='cuda:0')
********************
observing microbatch 13
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.3503, -0.4953,  0.2739]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 13
********************
inputting microbatch 12 into partition 1
before moving to cuda:1: tensor([[0.4706, 0.3143, 0.5820, 0.2764]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.8142, 0.8963, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 12
current batch shape is torch.Size([1, 4])
batch is tensor([[0.8142, 0.8963, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 12
********************
receiving microbatch 13 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 13, tensor([[0.4848, 0.3787, 0.5399, 0.4511]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 13
result is tensor([[0.4848, 0.3787, 0.5399, 0.4511]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4848, 0.3787, 0.5399, 0.4511]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 12 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 12, tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 12
result is tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.8520, 0.8520, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8137, 0.8981, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8143, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4848, 0.3787, 0.5399, 0.4511]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.1900,  0.6186, -0.0598]], device='cuda:0'), tensor([[-0.6248, -0.8262, -1.7463]], device='cuda:0')]
====================
schedule for this step is
[(14, 0), (13, 1)]
inputting microbatch 14 into partition 0
before moving to cuda:0: tensor([[ 0.1900,  0.6186, -0.0598]], device='cuda:0')
after: tensor([[ 0.1900,  0.6186, -0.0598]], device='cuda:0')
********************
observing microbatch 14
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.1900,  0.6186, -0.0598]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 14
********************
inputting microbatch 13 into partition 1
before moving to cuda:1: tensor([[0.4848, 0.3787, 0.5399, 0.4511]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.8142, 0.8963, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 13
current batch shape is torch.Size([1, 4])
batch is tensor([[0.8142, 0.8963, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 13
********************
receiving microbatch 14 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 14, tensor([[0.4292, 0.4613, 0.6872, 0.3566]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 14
result is tensor([[0.4292, 0.4613, 0.6872, 0.3566]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4292, 0.4613, 0.6872, 0.3566]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 13 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 13, tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 13
result is tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.8520, 0.8520, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8137, 0.8981, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8143, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4292, 0.4613, 0.6872, 0.3566]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.6248, -0.8262, -1.7463]], device='cuda:0')]
====================
schedule for this step is
[(15, 0), (14, 1)]
inputting microbatch 15 into partition 0
before moving to cuda:0: tensor([[-0.6248, -0.8262, -1.7463]], device='cuda:0')
after: tensor([[-0.6248, -0.8262, -1.7463]], device='cuda:0')
********************
observing microbatch 15
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.6248, -0.8262, -1.7463]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 15
********************
inputting microbatch 14 into partition 1
before moving to cuda:1: tensor([[0.4292, 0.4613, 0.6872, 0.3566]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.8142, 0.8963, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 14
current batch shape is torch.Size([1, 4])
batch is tensor([[0.8142, 0.8963, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 14
********************
receiving microbatch 15 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 15, tensor([[0.6050, 0.5855, 0.5576, 0.5166]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 15
result is tensor([[0.6050, 0.5855, 0.5576, 0.5166]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6050, 0.5855, 0.5576, 0.5166]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 14 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 14, tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 14
result is tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.8520, 0.8520, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8137, 0.8981, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8143, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6050, 0.5855, 0.5576, 0.5166]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)]
====================
schedule for this step is
[(15, 1)]
inputting microbatch 15 into partition 1
before moving to cuda:1: tensor([[0.6050, 0.5855, 0.5576, 0.5166]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.8142, 0.8963, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 15
current batch shape is torch.Size([1, 4])
batch is tensor([[0.8142, 0.8963, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 15
********************
receiving microbatch 15 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 15, tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 15
result is tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.8520, 0.8520, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8137, 0.8981, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8143, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8142, 0.8963, 0.6225, 0.6225, 0.6225]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)]
finished running cycle
_____________________________ test_forward_0[1-32] _____________________________

batch_size = 32, split_size = 1

    @pytest.mark.a4_2_2
    @pytest.mark.parametrize("batch_size", [1, 16, 32, 64])
    @pytest.mark.parametrize("split_size", [1, 2, 4, 8, 16])
    # @pytest.mark.parametrize("batch_size", [1])
    # @pytest.mark.parametrize("split_size", [1, 2])
    def test_forward_0(batch_size, split_size):
        model = nn.Sequential(
            nn.Linear(3, 4).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
            nn.Linear(4, 5).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
        )
    
        x = torch.randn(batch_size, 3).to('cuda:0')
        y0 = model(x).to('cpu')
    
        # move the last two layer to another device
        model[-2] = model[-2].to('cuda:1')
        model[-1] = WithDevice(nn.Sigmoid(), 'cuda:1')
        pipe = Pipe(model, split_size=split_size)
        y1 = pipe(x).to('cpu')
>       assert torch.allclose(y0, y1)
E       assert False
E        +  where False = <built-in method allclose of type object at 0x153da78cb760>(tensor([[0.5277, 0.5468, 0.6046, 0.5496, 0.3663],\n        [0.5356, 0.5349, 0.6119, 0.5678, 0.3501],\n        [0.5590, 0.5007, 0.6092, 0.5665, 0.3824],\n        [0.5672, 0.4946, 0.6008, 0.5707, 0.3893],\n        [0.5492, 0.5157, 0.6054, 0.5553, 0.3866],\n        [0.5242, 0.5544, 0.6002, 0.5509, 0.3604],\n        [0.5294, 0.5387, 0.6153, 0.5505, 0.3661],\n        [0.5584, 0.4993, 0.6126, 0.5645, 0.3840],\n        [0.5720, 0.4866, 0.6045, 0.5707, 0.3963],\n        [0.5464, 0.5152, 0.6171, 0.5662, 0.3653],\n        [0.5600, 0.5040, 0.6002, 0.5662, 0.3856],\n        [0.5396, 0.5333, 0.6018, 0.5616, 0.3652],\n        [0.5308, 0.5374, 0.6150, 0.5530, 0.3643],\n        [0.5425, 0.5165, 0.6237, 0.5619, 0.3654],\n        [0.5332, 0.5353, 0.6130, 0.5545, 0.3655],\n        [0.5374, 0.5252, 0.6184, 0.5507, 0.3759],\n        [0.5506, 0.5197, 0.5963, 0.5628, 0.3784],\n        [0.5429, 0.5208, 0.6183, 0.5698, 0.3558],\n        [0.5397, 0.5237, 0.6229, 0.5711, 0.3494],\n        [0.5236, 0.5417, 0.6245, 0.5516, 0.3553],\n        [0.5505, 0.5148, 0.6026, 0.5503, 0.3963],\n        [0.5457, 0.5171, 0.6134, 0.5595, 0.3745],\n        [0.5373, 0.5388, 0.5993, 0.5644, 0.3586],\n        [0.5488, 0.5070, 0.6225, 0.5580, 0.3796],\n        [0.5079, 0.5685, 0.6103, 0.5486, 0.3389],\n        [0.5543, 0.5061, 0.6103, 0.5609, 0.3843],\n        [0.5267, 0.5500, 0.6047, 0.5579, 0.3535],\n        [0.5432, 0.5069, 0.6368, 0.5562, 0.3728],\n        [0.5456, 0.5173, 0.6098, 0.5471, 0.3931],\n        [0.5596, 0.5105, 0.5894, 0.5607, 0.3952],\n        [0.5452, 0.5152, 0.6210, 0.5676, 0.3614],\n        [0.5516, 0.5101, 0.6099, 0.5598, 0.3824]], grad_fn=<ToCopyBackward0>), tensor([[0.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n        [0.0000, 0.9913, 0.9728, 0.9646, 0.9673],\n        [0.0000, 0.9906, 0.9706, 0.9621, 0.9651],\n        [0.0000, 0.9905, 0.9704, 0.9619, 0.9649],\n        [0.0000, 0.9905, 0.9704, 0.9619, 0.9649],\n        [0.0000, 0.9905, 0.9704, 0.9619, 0.9649],\n        [0.0000, 0.9905, 0.9704, 0.9619, 0.9649],\n        [0.0000, 0.9905, 0.9704, 0.9619, 0.9649],\n        [0.0000, 0.9905, 0.9704, 0.9619, 0.9649],\n        [0.0000, 0.9905, 0.9704, 0.9619, 0.9649],\n        [0.0000, 0.9905, 0.9704, 0.9619, 0.9649],\n        [0.0000, 0.9905, 0.9704, 0.9619, 0.9649],\n        [0.0000, 0.9905, 0.9704, 0.9619, 0.9649],\n        [0.0000, 0.9905, 0.9704, 0.9619, 0.9649],\n        [0.0000, 0.9905, 0.9704, 0.9619, 0.9649],\n        [0.0000, 0.9905, 0.9704, 0.9619, 0.9649],\n        [0.0000, 0.9905, 0.9704, 0.9619, 0.9649],\n        [0.0000, 0.9905, 0.9704, 0.9619, 0.9649],\n        [0.0000, 0.9905, 0.9704, 0.9619, 0.9649],\n        [0.0000, 0.9905, 0.9704, 0.9619, 0.9649],\n        [0.0000, 0.9905, 0.9704, 0.9619, 0.9649],\n        [0.0000, 0.9905, 0.9704, 0.9619, 0.9649],\n        [0.0000, 0.9905, 0.9704, 0.9619, 0.9649],\n        [0.0000, 0.9905, 0.9704, 0.9619, 0.9649],\n        [0.0000, 0.9905, 0.9704, 0.9619, 0.9649],\n        [0.0000, 0.9905, 0.9704, 0.9619, 0.9649],\n        [0.0000, 0.9905, 0.9704, 0.9619, 0.9649],\n        [0.0000, 0.9905, 0.9704, 0.9619, 0.9649],\n        [0.0000, 0.9905, 0.9704, 0.9619, 0.9649],\n        [0.0000, 0.9905, 0.9704, 0.9619, 0.9649],\n        [0.0000, 0.9905, 0.9704, 0.9619, 0.9649],\n        [0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], grad_fn=<ToCopyBackward0>))
E        +    where <built-in method allclose of type object at 0x153da78cb760> = torch.allclose

batch_size = 32
model      = Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
  (2): Linear(in_features=4, out_features=5, bias=True)
  (3): WithDevice(
    (_module): Sigmoid()
  )
)
pipe       = Pipe(
  (partitions): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=3, out_features=4, bias=True)
   ...inear(in_features=4, out_features=5, bias=True)
      (1): WithDevice(
        (_module): Sigmoid()
      )
    )
  )
)
split_size = 1
x          = tensor([[-1.2541, -1.4038, -0.9288],
        [-1.1673,  1.0275,  0.6123],
        [ 1.0809,  0.3632,  0.5154],
       ... 0.2664, -2.0704,  1.4860],
        [ 0.3418,  1.5683, -0.2014],
        [ 0.6095, -0.3728, -0.1772]], device='cuda:0')
y0         = tensor([[0.5277, 0.5468, 0.6046, 0.5496, 0.3663],
        [0.5356, 0.5349, 0.6119, 0.5678, 0.3501],
        [0.5590, 0...[0.5452, 0.5152, 0.6210, 0.5676, 0.3614],
        [0.5516, 0.5101, 0.6099, 0.5598, 0.3824]], grad_fn=<ToCopyBackward0>)
y1         = tensor([[0.0000, 1.0000, 1.0000, 1.0000, 1.0000],
        [0.0000, 0.9913, 0.9728, 0.9646, 0.9673],
        [0.0000, 0...[0.0000, 0.9905, 0.9704, 0.9619, 0.9649],
        [0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], grad_fn=<ToCopyBackward0>)

tests/test_pipeline.py:83: AssertionError
----------------------------- Captured stdout call -----------------------------
in init
ModuleList(
  (0): Sequential(
    (0): Linear(in_features=3, out_features=4, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
  (1): Sequential(
    (0): Linear(in_features=4, out_features=5, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
)
[device(type='cuda', index=0), device(type='cuda', index=1)]
before running the cycle
[tensor([[-1.2541, -1.4038, -0.9288]], device='cuda:0'), tensor([[-1.1673,  1.0275,  0.6123]], device='cuda:0'), tensor([[1.0809, 0.3632, 0.5154]], device='cuda:0'), tensor([[1.6104, 0.3614, 1.4949]], device='cuda:0'), tensor([[ 0.4139, -1.0822, -0.3696]], device='cuda:0'), tensor([[-1.8434, -1.5223, -0.5278]], device='cuda:0'), tensor([[-0.5481, -0.5040, -1.5791]], device='cuda:0'), tensor([[1.2148, 0.3034, 0.1279]], device='cuda:0'), tensor([[2.7157, 0.9389, 1.1483]], device='cuda:0'), tensor([[ 0.2827,  1.0558, -0.0760]], device='cuda:0'), tensor([[ 0.6669, -0.4911,  1.1908]], device='cuda:0'), tensor([[-0.8116, -0.3404,  0.3862]], device='cuda:0'), tensor([[-0.5300, -0.2377, -1.3405]], device='cuda:0'), tensor([[ 0.4126,  1.0101, -0.9078]], device='cuda:0'), tensor([[-0.4830, -0.2460, -1.0622]], device='cuda:0'), tensor([[ 0.2497, -0.4515, -1.7059]], device='cuda:0'), tensor([[-0.2070, -0.8569,  0.9374]], device='cuda:0'), tensor([[-0.1557,  1.6940,  0.2851]], device='cuda:0'), tensor([[-0.3276,  2.2576,  0.1918]], device='cuda:0'), tensor([[-0.6025,  0.4908, -2.1693]], device='cuda:0'), tensor([[ 0.6174, -1.8837, -0.6706]], device='cuda:0'), tensor([[ 0.3033, -0.0089, -0.4867]], device='cuda:0'), tensor([[-1.3237, -0.1580,  0.9035]], device='cuda:0'), tensor([[ 1.0219,  0.3149, -1.1579]], device='cuda:0'), tensor([[-2.8940, -0.6547, -1.5540]], device='cuda:0'), tensor([[ 0.8044, -0.3017, -0.0606]], device='cuda:0'), tensor([[-1.5536, -0.2126, -0.2513]], device='cuda:0'), tensor([[ 1.2939,  1.1673, -2.2498]], device='cuda:0'), tensor([[ 0.6879, -1.6218, -1.4965]], device='cuda:0'), tensor([[ 0.2664, -2.0704,  1.4860]], device='cuda:0'), tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0'), tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')]
====================
schedule for this step is
[(0, 0)]
inputting microbatch 0 into partition 0
before moving to cuda:0: tensor([[-1.2541, -1.4038, -0.9288]], device='cuda:0')
after: tensor([[-1.2541, -1.4038, -0.9288]], device='cuda:0')
********************
observing microbatch 0
current batch shape is torch.Size([1, 3])
batch is tensor([[-1.2541, -1.4038, -0.9288]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 0
********************
receiving microbatch 0 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 0, tensor([[0.4823, 0.5128, 0.2011, 0.6080]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.4823, 0.5128, 0.2011, 0.6080]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4823, 0.5128, 0.2011, 0.6080]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.4823, 0.5128, 0.2011, 0.6080]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-1.1673,  1.0275,  0.6123]], device='cuda:0'), tensor([[1.0809, 0.3632, 0.5154]], device='cuda:0'), tensor([[1.6104, 0.3614, 1.4949]], device='cuda:0'), tensor([[ 0.4139, -1.0822, -0.3696]], device='cuda:0'), tensor([[-1.8434, -1.5223, -0.5278]], device='cuda:0'), tensor([[-0.5481, -0.5040, -1.5791]], device='cuda:0'), tensor([[1.2148, 0.3034, 0.1279]], device='cuda:0'), tensor([[2.7157, 0.9389, 1.1483]], device='cuda:0'), tensor([[ 0.2827,  1.0558, -0.0760]], device='cuda:0'), tensor([[ 0.6669, -0.4911,  1.1908]], device='cuda:0'), tensor([[-0.8116, -0.3404,  0.3862]], device='cuda:0'), tensor([[-0.5300, -0.2377, -1.3405]], device='cuda:0'), tensor([[ 0.4126,  1.0101, -0.9078]], device='cuda:0'), tensor([[-0.4830, -0.2460, -1.0622]], device='cuda:0'), tensor([[ 0.2497, -0.4515, -1.7059]], device='cuda:0'), tensor([[-0.2070, -0.8569,  0.9374]], device='cuda:0'), tensor([[-0.1557,  1.6940,  0.2851]], device='cuda:0'), tensor([[-0.3276,  2.2576,  0.1918]], device='cuda:0'), tensor([[-0.6025,  0.4908, -2.1693]], device='cuda:0'), tensor([[ 0.6174, -1.8837, -0.6706]], device='cuda:0'), tensor([[ 0.3033, -0.0089, -0.4867]], device='cuda:0'), tensor([[-1.3237, -0.1580,  0.9035]], device='cuda:0'), tensor([[ 1.0219,  0.3149, -1.1579]], device='cuda:0'), tensor([[-2.8940, -0.6547, -1.5540]], device='cuda:0'), tensor([[ 0.8044, -0.3017, -0.0606]], device='cuda:0'), tensor([[-1.5536, -0.2126, -0.2513]], device='cuda:0'), tensor([[ 1.2939,  1.1673, -2.2498]], device='cuda:0'), tensor([[ 0.6879, -1.6218, -1.4965]], device='cuda:0'), tensor([[ 0.2664, -2.0704,  1.4860]], device='cuda:0'), tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0'), tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')]
====================
schedule for this step is
[(1, 0), (0, 1)]
inputting microbatch 1 into partition 0
before moving to cuda:0: tensor([[-1.1673,  1.0275,  0.6123]], device='cuda:0')
after: tensor([[-1.1673,  1.0275,  0.6123]], device='cuda:0')
********************
observing microbatch 1
current batch shape is torch.Size([1, 3])
batch is tensor([[-1.1673,  1.0275,  0.6123]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 1
********************
inputting microbatch 0 into partition 1
before moving to cuda:1: tensor([[0.4823, 0.5128, 0.2011, 0.6080]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[3.6893e+19, 1.8286e+00, 0.0000e+00, 1.8491e+00]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 0
current batch shape is torch.Size([1, 4])
batch is tensor([[3.6893e+19, 1.8286e+00, 0.0000e+00, 1.8491e+00]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 0
********************
receiving microbatch 1 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 1, tensor([[0.3918, 0.7827, 0.6630, 0.6215]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 1
result is tensor([[0.3918, 0.7827, 0.6630, 0.6215]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3918, 0.7827, 0.6630, 0.6215]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 0 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 0, tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>)
result shape is tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>), tensor([[0.3918, 0.7827, 0.6630, 0.6215]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[1.0809, 0.3632, 0.5154]], device='cuda:0'), tensor([[1.6104, 0.3614, 1.4949]], device='cuda:0'), tensor([[ 0.4139, -1.0822, -0.3696]], device='cuda:0'), tensor([[-1.8434, -1.5223, -0.5278]], device='cuda:0'), tensor([[-0.5481, -0.5040, -1.5791]], device='cuda:0'), tensor([[1.2148, 0.3034, 0.1279]], device='cuda:0'), tensor([[2.7157, 0.9389, 1.1483]], device='cuda:0'), tensor([[ 0.2827,  1.0558, -0.0760]], device='cuda:0'), tensor([[ 0.6669, -0.4911,  1.1908]], device='cuda:0'), tensor([[-0.8116, -0.3404,  0.3862]], device='cuda:0'), tensor([[-0.5300, -0.2377, -1.3405]], device='cuda:0'), tensor([[ 0.4126,  1.0101, -0.9078]], device='cuda:0'), tensor([[-0.4830, -0.2460, -1.0622]], device='cuda:0'), tensor([[ 0.2497, -0.4515, -1.7059]], device='cuda:0'), tensor([[-0.2070, -0.8569,  0.9374]], device='cuda:0'), tensor([[-0.1557,  1.6940,  0.2851]], device='cuda:0'), tensor([[-0.3276,  2.2576,  0.1918]], device='cuda:0'), tensor([[-0.6025,  0.4908, -2.1693]], device='cuda:0'), tensor([[ 0.6174, -1.8837, -0.6706]], device='cuda:0'), tensor([[ 0.3033, -0.0089, -0.4867]], device='cuda:0'), tensor([[-1.3237, -0.1580,  0.9035]], device='cuda:0'), tensor([[ 1.0219,  0.3149, -1.1579]], device='cuda:0'), tensor([[-2.8940, -0.6547, -1.5540]], device='cuda:0'), tensor([[ 0.8044, -0.3017, -0.0606]], device='cuda:0'), tensor([[-1.5536, -0.2126, -0.2513]], device='cuda:0'), tensor([[ 1.2939,  1.1673, -2.2498]], device='cuda:0'), tensor([[ 0.6879, -1.6218, -1.4965]], device='cuda:0'), tensor([[ 0.2664, -2.0704,  1.4860]], device='cuda:0'), tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0'), tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')]
====================
schedule for this step is
[(2, 0), (1, 1)]
inputting microbatch 2 into partition 0
before moving to cuda:0: tensor([[1.0809, 0.3632, 0.5154]], device='cuda:0')
after: tensor([[1.0809, 0.3632, 0.5154]], device='cuda:0')
********************
observing microbatch 2
current batch shape is torch.Size([1, 3])
batch is tensor([[1.0809, 0.3632, 0.5154]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 2
********************
inputting microbatch 1 into partition 1
before moving to cuda:1: tensor([[0.3918, 0.7827, 0.6630, 0.6215]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[1., 1., 1., 1.]], device='cuda:1', grad_fn=<ToCopyBackward0>)
********************
observing microbatch 1
current batch shape is torch.Size([1, 4])
batch is tensor([[1., 1., 1., 1.]], device='cuda:1', grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 1
********************
receiving microbatch 2 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 2, tensor([[0.2844, 0.6021, 0.8143, 0.2818]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 2
result is tensor([[0.2844, 0.6021, 0.8143, 0.2818]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.2844, 0.6021, 0.8143, 0.2818]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 1 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 1, tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 1
result is tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.2844, 0.6021, 0.8143, 0.2818]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[1.6104, 0.3614, 1.4949]], device='cuda:0'), tensor([[ 0.4139, -1.0822, -0.3696]], device='cuda:0'), tensor([[-1.8434, -1.5223, -0.5278]], device='cuda:0'), tensor([[-0.5481, -0.5040, -1.5791]], device='cuda:0'), tensor([[1.2148, 0.3034, 0.1279]], device='cuda:0'), tensor([[2.7157, 0.9389, 1.1483]], device='cuda:0'), tensor([[ 0.2827,  1.0558, -0.0760]], device='cuda:0'), tensor([[ 0.6669, -0.4911,  1.1908]], device='cuda:0'), tensor([[-0.8116, -0.3404,  0.3862]], device='cuda:0'), tensor([[-0.5300, -0.2377, -1.3405]], device='cuda:0'), tensor([[ 0.4126,  1.0101, -0.9078]], device='cuda:0'), tensor([[-0.4830, -0.2460, -1.0622]], device='cuda:0'), tensor([[ 0.2497, -0.4515, -1.7059]], device='cuda:0'), tensor([[-0.2070, -0.8569,  0.9374]], device='cuda:0'), tensor([[-0.1557,  1.6940,  0.2851]], device='cuda:0'), tensor([[-0.3276,  2.2576,  0.1918]], device='cuda:0'), tensor([[-0.6025,  0.4908, -2.1693]], device='cuda:0'), tensor([[ 0.6174, -1.8837, -0.6706]], device='cuda:0'), tensor([[ 0.3033, -0.0089, -0.4867]], device='cuda:0'), tensor([[-1.3237, -0.1580,  0.9035]], device='cuda:0'), tensor([[ 1.0219,  0.3149, -1.1579]], device='cuda:0'), tensor([[-2.8940, -0.6547, -1.5540]], device='cuda:0'), tensor([[ 0.8044, -0.3017, -0.0606]], device='cuda:0'), tensor([[-1.5536, -0.2126, -0.2513]], device='cuda:0'), tensor([[ 1.2939,  1.1673, -2.2498]], device='cuda:0'), tensor([[ 0.6879, -1.6218, -1.4965]], device='cuda:0'), tensor([[ 0.2664, -2.0704,  1.4860]], device='cuda:0'), tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0'), tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')]
====================
schedule for this step is
[(3, 0), (2, 1)]
inputting microbatch 3 into partition 0
before moving to cuda:0: tensor([[1.6104, 0.3614, 1.4949]], device='cuda:0')
after: tensor([[1.6104, 0.3614, 1.4949]], device='cuda:0')
********************
observing microbatch 3
current batch shape is torch.Size([1, 3])
batch is tensor([[1.6104, 0.3614, 1.4949]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 3
********************
inputting microbatch 2 into partition 1
before moving to cuda:1: tensor([[0.2844, 0.6021, 0.8143, 0.2818]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 2
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 2
********************
receiving microbatch 3 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 3, tensor([[0.2213, 0.6319, 0.9056, 0.1694]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 3
result is tensor([[0.2213, 0.6319, 0.9056, 0.1694]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.2213, 0.6319, 0.9056, 0.1694]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 2 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 2, tensor([[0.0000, 0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 2
result is tensor([[0.0000, 0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.2213, 0.6319, 0.9056, 0.1694]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.4139, -1.0822, -0.3696]], device='cuda:0'), tensor([[-1.8434, -1.5223, -0.5278]], device='cuda:0'), tensor([[-0.5481, -0.5040, -1.5791]], device='cuda:0'), tensor([[1.2148, 0.3034, 0.1279]], device='cuda:0'), tensor([[2.7157, 0.9389, 1.1483]], device='cuda:0'), tensor([[ 0.2827,  1.0558, -0.0760]], device='cuda:0'), tensor([[ 0.6669, -0.4911,  1.1908]], device='cuda:0'), tensor([[-0.8116, -0.3404,  0.3862]], device='cuda:0'), tensor([[-0.5300, -0.2377, -1.3405]], device='cuda:0'), tensor([[ 0.4126,  1.0101, -0.9078]], device='cuda:0'), tensor([[-0.4830, -0.2460, -1.0622]], device='cuda:0'), tensor([[ 0.2497, -0.4515, -1.7059]], device='cuda:0'), tensor([[-0.2070, -0.8569,  0.9374]], device='cuda:0'), tensor([[-0.1557,  1.6940,  0.2851]], device='cuda:0'), tensor([[-0.3276,  2.2576,  0.1918]], device='cuda:0'), tensor([[-0.6025,  0.4908, -2.1693]], device='cuda:0'), tensor([[ 0.6174, -1.8837, -0.6706]], device='cuda:0'), tensor([[ 0.3033, -0.0089, -0.4867]], device='cuda:0'), tensor([[-1.3237, -0.1580,  0.9035]], device='cuda:0'), tensor([[ 1.0219,  0.3149, -1.1579]], device='cuda:0'), tensor([[-2.8940, -0.6547, -1.5540]], device='cuda:0'), tensor([[ 0.8044, -0.3017, -0.0606]], device='cuda:0'), tensor([[-1.5536, -0.2126, -0.2513]], device='cuda:0'), tensor([[ 1.2939,  1.1673, -2.2498]], device='cuda:0'), tensor([[ 0.6879, -1.6218, -1.4965]], device='cuda:0'), tensor([[ 0.2664, -2.0704,  1.4860]], device='cuda:0'), tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0'), tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')]
====================
schedule for this step is
[(4, 0), (3, 1)]
inputting microbatch 4 into partition 0
before moving to cuda:0: tensor([[ 0.4139, -1.0822, -0.3696]], device='cuda:0')
after: tensor([[ 0.4139, -1.0822, -0.3696]], device='cuda:0')
********************
observing microbatch 4
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.4139, -1.0822, -0.3696]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 4
********************
inputting microbatch 3 into partition 1
before moving to cuda:1: tensor([[0.2213, 0.6319, 0.9056, 0.1694]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 3
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 3
********************
receiving microbatch 4 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 4, tensor([[0.3609, 0.4651, 0.4870, 0.3478]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 4
result is tensor([[0.3609, 0.4651, 0.4870, 0.3478]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3609, 0.4651, 0.4870, 0.3478]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 3 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 3, tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 3
result is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3609, 0.4651, 0.4870, 0.3478]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-1.8434, -1.5223, -0.5278]], device='cuda:0'), tensor([[-0.5481, -0.5040, -1.5791]], device='cuda:0'), tensor([[1.2148, 0.3034, 0.1279]], device='cuda:0'), tensor([[2.7157, 0.9389, 1.1483]], device='cuda:0'), tensor([[ 0.2827,  1.0558, -0.0760]], device='cuda:0'), tensor([[ 0.6669, -0.4911,  1.1908]], device='cuda:0'), tensor([[-0.8116, -0.3404,  0.3862]], device='cuda:0'), tensor([[-0.5300, -0.2377, -1.3405]], device='cuda:0'), tensor([[ 0.4126,  1.0101, -0.9078]], device='cuda:0'), tensor([[-0.4830, -0.2460, -1.0622]], device='cuda:0'), tensor([[ 0.2497, -0.4515, -1.7059]], device='cuda:0'), tensor([[-0.2070, -0.8569,  0.9374]], device='cuda:0'), tensor([[-0.1557,  1.6940,  0.2851]], device='cuda:0'), tensor([[-0.3276,  2.2576,  0.1918]], device='cuda:0'), tensor([[-0.6025,  0.4908, -2.1693]], device='cuda:0'), tensor([[ 0.6174, -1.8837, -0.6706]], device='cuda:0'), tensor([[ 0.3033, -0.0089, -0.4867]], device='cuda:0'), tensor([[-1.3237, -0.1580,  0.9035]], device='cuda:0'), tensor([[ 1.0219,  0.3149, -1.1579]], device='cuda:0'), tensor([[-2.8940, -0.6547, -1.5540]], device='cuda:0'), tensor([[ 0.8044, -0.3017, -0.0606]], device='cuda:0'), tensor([[-1.5536, -0.2126, -0.2513]], device='cuda:0'), tensor([[ 1.2939,  1.1673, -2.2498]], device='cuda:0'), tensor([[ 0.6879, -1.6218, -1.4965]], device='cuda:0'), tensor([[ 0.2664, -2.0704,  1.4860]], device='cuda:0'), tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0'), tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')]
====================
schedule for this step is
[(5, 0), (4, 1)]
inputting microbatch 5 into partition 0
before moving to cuda:0: tensor([[-1.8434, -1.5223, -0.5278]], device='cuda:0')
after: tensor([[-1.8434, -1.5223, -0.5278]], device='cuda:0')
********************
observing microbatch 5
current batch shape is torch.Size([1, 3])
batch is tensor([[-1.8434, -1.5223, -0.5278]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 5
********************
inputting microbatch 4 into partition 1
before moving to cuda:1: tensor([[0.3609, 0.4651, 0.4870, 0.3478]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 4
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 4
********************
receiving microbatch 5 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 5, tensor([[0.4935, 0.5703, 0.1761, 0.6457]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 5
result is tensor([[0.4935, 0.5703, 0.1761, 0.6457]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4935, 0.5703, 0.1761, 0.6457]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 4 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 4, tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 4
result is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4935, 0.5703, 0.1761, 0.6457]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.5481, -0.5040, -1.5791]], device='cuda:0'), tensor([[1.2148, 0.3034, 0.1279]], device='cuda:0'), tensor([[2.7157, 0.9389, 1.1483]], device='cuda:0'), tensor([[ 0.2827,  1.0558, -0.0760]], device='cuda:0'), tensor([[ 0.6669, -0.4911,  1.1908]], device='cuda:0'), tensor([[-0.8116, -0.3404,  0.3862]], device='cuda:0'), tensor([[-0.5300, -0.2377, -1.3405]], device='cuda:0'), tensor([[ 0.4126,  1.0101, -0.9078]], device='cuda:0'), tensor([[-0.4830, -0.2460, -1.0622]], device='cuda:0'), tensor([[ 0.2497, -0.4515, -1.7059]], device='cuda:0'), tensor([[-0.2070, -0.8569,  0.9374]], device='cuda:0'), tensor([[-0.1557,  1.6940,  0.2851]], device='cuda:0'), tensor([[-0.3276,  2.2576,  0.1918]], device='cuda:0'), tensor([[-0.6025,  0.4908, -2.1693]], device='cuda:0'), tensor([[ 0.6174, -1.8837, -0.6706]], device='cuda:0'), tensor([[ 0.3033, -0.0089, -0.4867]], device='cuda:0'), tensor([[-1.3237, -0.1580,  0.9035]], device='cuda:0'), tensor([[ 1.0219,  0.3149, -1.1579]], device='cuda:0'), tensor([[-2.8940, -0.6547, -1.5540]], device='cuda:0'), tensor([[ 0.8044, -0.3017, -0.0606]], device='cuda:0'), tensor([[-1.5536, -0.2126, -0.2513]], device='cuda:0'), tensor([[ 1.2939,  1.1673, -2.2498]], device='cuda:0'), tensor([[ 0.6879, -1.6218, -1.4965]], device='cuda:0'), tensor([[ 0.2664, -2.0704,  1.4860]], device='cuda:0'), tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0'), tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')]
====================
schedule for this step is
[(6, 0), (5, 1)]
inputting microbatch 6 into partition 0
before moving to cuda:0: tensor([[-0.5481, -0.5040, -1.5791]], device='cuda:0')
after: tensor([[-0.5481, -0.5040, -1.5791]], device='cuda:0')
********************
observing microbatch 6
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.5481, -0.5040, -1.5791]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 6
********************
inputting microbatch 5 into partition 1
before moving to cuda:1: tensor([[0.4935, 0.5703, 0.1761, 0.6457]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 5
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 5
********************
receiving microbatch 6 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 6, tensor([[0.4774, 0.4961, 0.2908, 0.6251]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 6
result is tensor([[0.4774, 0.4961, 0.2908, 0.6251]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4774, 0.4961, 0.2908, 0.6251]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 5 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 5, tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 5
result is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4774, 0.4961, 0.2908, 0.6251]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[1.2148, 0.3034, 0.1279]], device='cuda:0'), tensor([[2.7157, 0.9389, 1.1483]], device='cuda:0'), tensor([[ 0.2827,  1.0558, -0.0760]], device='cuda:0'), tensor([[ 0.6669, -0.4911,  1.1908]], device='cuda:0'), tensor([[-0.8116, -0.3404,  0.3862]], device='cuda:0'), tensor([[-0.5300, -0.2377, -1.3405]], device='cuda:0'), tensor([[ 0.4126,  1.0101, -0.9078]], device='cuda:0'), tensor([[-0.4830, -0.2460, -1.0622]], device='cuda:0'), tensor([[ 0.2497, -0.4515, -1.7059]], device='cuda:0'), tensor([[-0.2070, -0.8569,  0.9374]], device='cuda:0'), tensor([[-0.1557,  1.6940,  0.2851]], device='cuda:0'), tensor([[-0.3276,  2.2576,  0.1918]], device='cuda:0'), tensor([[-0.6025,  0.4908, -2.1693]], device='cuda:0'), tensor([[ 0.6174, -1.8837, -0.6706]], device='cuda:0'), tensor([[ 0.3033, -0.0089, -0.4867]], device='cuda:0'), tensor([[-1.3237, -0.1580,  0.9035]], device='cuda:0'), tensor([[ 1.0219,  0.3149, -1.1579]], device='cuda:0'), tensor([[-2.8940, -0.6547, -1.5540]], device='cuda:0'), tensor([[ 0.8044, -0.3017, -0.0606]], device='cuda:0'), tensor([[-1.5536, -0.2126, -0.2513]], device='cuda:0'), tensor([[ 1.2939,  1.1673, -2.2498]], device='cuda:0'), tensor([[ 0.6879, -1.6218, -1.4965]], device='cuda:0'), tensor([[ 0.2664, -2.0704,  1.4860]], device='cuda:0'), tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0'), tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')]
====================
schedule for this step is
[(7, 0), (6, 1)]
inputting microbatch 7 into partition 0
before moving to cuda:0: tensor([[1.2148, 0.3034, 0.1279]], device='cuda:0')
after: tensor([[1.2148, 0.3034, 0.1279]], device='cuda:0')
********************
observing microbatch 7
current batch shape is torch.Size([1, 3])
batch is tensor([[1.2148, 0.3034, 0.1279]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 7
********************
inputting microbatch 6 into partition 1
before moving to cuda:1: tensor([[0.4774, 0.4961, 0.2908, 0.6251]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 6
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 6
********************
receiving microbatch 7 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 7, tensor([[0.2958, 0.5622, 0.7891, 0.2912]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 7
result is tensor([[0.2958, 0.5622, 0.7891, 0.2912]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.2958, 0.5622, 0.7891, 0.2912]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 6 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 6, tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 6
result is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.2958, 0.5622, 0.7891, 0.2912]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[2.7157, 0.9389, 1.1483]], device='cuda:0'), tensor([[ 0.2827,  1.0558, -0.0760]], device='cuda:0'), tensor([[ 0.6669, -0.4911,  1.1908]], device='cuda:0'), tensor([[-0.8116, -0.3404,  0.3862]], device='cuda:0'), tensor([[-0.5300, -0.2377, -1.3405]], device='cuda:0'), tensor([[ 0.4126,  1.0101, -0.9078]], device='cuda:0'), tensor([[-0.4830, -0.2460, -1.0622]], device='cuda:0'), tensor([[ 0.2497, -0.4515, -1.7059]], device='cuda:0'), tensor([[-0.2070, -0.8569,  0.9374]], device='cuda:0'), tensor([[-0.1557,  1.6940,  0.2851]], device='cuda:0'), tensor([[-0.3276,  2.2576,  0.1918]], device='cuda:0'), tensor([[-0.6025,  0.4908, -2.1693]], device='cuda:0'), tensor([[ 0.6174, -1.8837, -0.6706]], device='cuda:0'), tensor([[ 0.3033, -0.0089, -0.4867]], device='cuda:0'), tensor([[-1.3237, -0.1580,  0.9035]], device='cuda:0'), tensor([[ 1.0219,  0.3149, -1.1579]], device='cuda:0'), tensor([[-2.8940, -0.6547, -1.5540]], device='cuda:0'), tensor([[ 0.8044, -0.3017, -0.0606]], device='cuda:0'), tensor([[-1.5536, -0.2126, -0.2513]], device='cuda:0'), tensor([[ 1.2939,  1.1673, -2.2498]], device='cuda:0'), tensor([[ 0.6879, -1.6218, -1.4965]], device='cuda:0'), tensor([[ 0.2664, -2.0704,  1.4860]], device='cuda:0'), tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0'), tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')]
====================
schedule for this step is
[(8, 0), (7, 1)]
inputting microbatch 8 into partition 0
before moving to cuda:0: tensor([[2.7157, 0.9389, 1.1483]], device='cuda:0')
after: tensor([[2.7157, 0.9389, 1.1483]], device='cuda:0')
********************
observing microbatch 8
current batch shape is torch.Size([1, 3])
batch is tensor([[2.7157, 0.9389, 1.1483]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 8
********************
inputting microbatch 7 into partition 1
before moving to cuda:1: tensor([[0.2958, 0.5622, 0.7891, 0.2912]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 7
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 7
********************
receiving microbatch 8 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 8, tensor([[0.1926, 0.5839, 0.9504, 0.1255]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 8
result is tensor([[0.1926, 0.5839, 0.9504, 0.1255]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.1926, 0.5839, 0.9504, 0.1255]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 7 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 7, tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 7
result is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.1926, 0.5839, 0.9504, 0.1255]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.2827,  1.0558, -0.0760]], device='cuda:0'), tensor([[ 0.6669, -0.4911,  1.1908]], device='cuda:0'), tensor([[-0.8116, -0.3404,  0.3862]], device='cuda:0'), tensor([[-0.5300, -0.2377, -1.3405]], device='cuda:0'), tensor([[ 0.4126,  1.0101, -0.9078]], device='cuda:0'), tensor([[-0.4830, -0.2460, -1.0622]], device='cuda:0'), tensor([[ 0.2497, -0.4515, -1.7059]], device='cuda:0'), tensor([[-0.2070, -0.8569,  0.9374]], device='cuda:0'), tensor([[-0.1557,  1.6940,  0.2851]], device='cuda:0'), tensor([[-0.3276,  2.2576,  0.1918]], device='cuda:0'), tensor([[-0.6025,  0.4908, -2.1693]], device='cuda:0'), tensor([[ 0.6174, -1.8837, -0.6706]], device='cuda:0'), tensor([[ 0.3033, -0.0089, -0.4867]], device='cuda:0'), tensor([[-1.3237, -0.1580,  0.9035]], device='cuda:0'), tensor([[ 1.0219,  0.3149, -1.1579]], device='cuda:0'), tensor([[-2.8940, -0.6547, -1.5540]], device='cuda:0'), tensor([[ 0.8044, -0.3017, -0.0606]], device='cuda:0'), tensor([[-1.5536, -0.2126, -0.2513]], device='cuda:0'), tensor([[ 1.2939,  1.1673, -2.2498]], device='cuda:0'), tensor([[ 0.6879, -1.6218, -1.4965]], device='cuda:0'), tensor([[ 0.2664, -2.0704,  1.4860]], device='cuda:0'), tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0'), tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')]
====================
schedule for this step is
[(9, 0), (8, 1)]
inputting microbatch 9 into partition 0
before moving to cuda:0: tensor([[ 0.2827,  1.0558, -0.0760]], device='cuda:0')
after: tensor([[ 0.2827,  1.0558, -0.0760]], device='cuda:0')
********************
observing microbatch 9
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.2827,  1.0558, -0.0760]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 9
********************
inputting microbatch 8 into partition 1
before moving to cuda:1: tensor([[0.1926, 0.5839, 0.9504, 0.1255]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 8
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 8
********************
receiving microbatch 9 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 9, tensor([[0.3509, 0.6691, 0.7504, 0.4801]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 9
result is tensor([[0.3509, 0.6691, 0.7504, 0.4801]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3509, 0.6691, 0.7504, 0.4801]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 8 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 8, tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 8
result is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3509, 0.6691, 0.7504, 0.4801]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.6669, -0.4911,  1.1908]], device='cuda:0'), tensor([[-0.8116, -0.3404,  0.3862]], device='cuda:0'), tensor([[-0.5300, -0.2377, -1.3405]], device='cuda:0'), tensor([[ 0.4126,  1.0101, -0.9078]], device='cuda:0'), tensor([[-0.4830, -0.2460, -1.0622]], device='cuda:0'), tensor([[ 0.2497, -0.4515, -1.7059]], device='cuda:0'), tensor([[-0.2070, -0.8569,  0.9374]], device='cuda:0'), tensor([[-0.1557,  1.6940,  0.2851]], device='cuda:0'), tensor([[-0.3276,  2.2576,  0.1918]], device='cuda:0'), tensor([[-0.6025,  0.4908, -2.1693]], device='cuda:0'), tensor([[ 0.6174, -1.8837, -0.6706]], device='cuda:0'), tensor([[ 0.3033, -0.0089, -0.4867]], device='cuda:0'), tensor([[-1.3237, -0.1580,  0.9035]], device='cuda:0'), tensor([[ 1.0219,  0.3149, -1.1579]], device='cuda:0'), tensor([[-2.8940, -0.6547, -1.5540]], device='cuda:0'), tensor([[ 0.8044, -0.3017, -0.0606]], device='cuda:0'), tensor([[-1.5536, -0.2126, -0.2513]], device='cuda:0'), tensor([[ 1.2939,  1.1673, -2.2498]], device='cuda:0'), tensor([[ 0.6879, -1.6218, -1.4965]], device='cuda:0'), tensor([[ 0.2664, -2.0704,  1.4860]], device='cuda:0'), tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0'), tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')]
====================
schedule for this step is
[(10, 0), (9, 1)]
inputting microbatch 10 into partition 0
before moving to cuda:0: tensor([[ 0.6669, -0.4911,  1.1908]], device='cuda:0')
after: tensor([[ 0.6669, -0.4911,  1.1908]], device='cuda:0')
********************
observing microbatch 10
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.6669, -0.4911,  1.1908]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 10
********************
inputting microbatch 9 into partition 1
before moving to cuda:1: tensor([[0.3509, 0.6691, 0.7504, 0.4801]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 9
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 9
********************
receiving microbatch 10 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 10, tensor([[0.2739, 0.6045, 0.7650, 0.2365]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 10
result is tensor([[0.2739, 0.6045, 0.7650, 0.2365]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.2739, 0.6045, 0.7650, 0.2365]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 9 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 9, tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 9
result is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.2739, 0.6045, 0.7650, 0.2365]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.8116, -0.3404,  0.3862]], device='cuda:0'), tensor([[-0.5300, -0.2377, -1.3405]], device='cuda:0'), tensor([[ 0.4126,  1.0101, -0.9078]], device='cuda:0'), tensor([[-0.4830, -0.2460, -1.0622]], device='cuda:0'), tensor([[ 0.2497, -0.4515, -1.7059]], device='cuda:0'), tensor([[-0.2070, -0.8569,  0.9374]], device='cuda:0'), tensor([[-0.1557,  1.6940,  0.2851]], device='cuda:0'), tensor([[-0.3276,  2.2576,  0.1918]], device='cuda:0'), tensor([[-0.6025,  0.4908, -2.1693]], device='cuda:0'), tensor([[ 0.6174, -1.8837, -0.6706]], device='cuda:0'), tensor([[ 0.3033, -0.0089, -0.4867]], device='cuda:0'), tensor([[-1.3237, -0.1580,  0.9035]], device='cuda:0'), tensor([[ 1.0219,  0.3149, -1.1579]], device='cuda:0'), tensor([[-2.8940, -0.6547, -1.5540]], device='cuda:0'), tensor([[ 0.8044, -0.3017, -0.0606]], device='cuda:0'), tensor([[-1.5536, -0.2126, -0.2513]], device='cuda:0'), tensor([[ 1.2939,  1.1673, -2.2498]], device='cuda:0'), tensor([[ 0.6879, -1.6218, -1.4965]], device='cuda:0'), tensor([[ 0.2664, -2.0704,  1.4860]], device='cuda:0'), tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0'), tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')]
====================
schedule for this step is
[(11, 0), (10, 1)]
inputting microbatch 11 into partition 0
before moving to cuda:0: tensor([[-0.8116, -0.3404,  0.3862]], device='cuda:0')
after: tensor([[-0.8116, -0.3404,  0.3862]], device='cuda:0')
********************
observing microbatch 11
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.8116, -0.3404,  0.3862]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 11
********************
inputting microbatch 10 into partition 1
before moving to cuda:1: tensor([[0.2739, 0.6045, 0.7650, 0.2365]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 10
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 10
********************
receiving microbatch 11 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 11, tensor([[0.3859, 0.6584, 0.5157, 0.5007]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 11
result is tensor([[0.3859, 0.6584, 0.5157, 0.5007]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3859, 0.6584, 0.5157, 0.5007]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 10 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 10, tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 10
result is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3859, 0.6584, 0.5157, 0.5007]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.5300, -0.2377, -1.3405]], device='cuda:0'), tensor([[ 0.4126,  1.0101, -0.9078]], device='cuda:0'), tensor([[-0.4830, -0.2460, -1.0622]], device='cuda:0'), tensor([[ 0.2497, -0.4515, -1.7059]], device='cuda:0'), tensor([[-0.2070, -0.8569,  0.9374]], device='cuda:0'), tensor([[-0.1557,  1.6940,  0.2851]], device='cuda:0'), tensor([[-0.3276,  2.2576,  0.1918]], device='cuda:0'), tensor([[-0.6025,  0.4908, -2.1693]], device='cuda:0'), tensor([[ 0.6174, -1.8837, -0.6706]], device='cuda:0'), tensor([[ 0.3033, -0.0089, -0.4867]], device='cuda:0'), tensor([[-1.3237, -0.1580,  0.9035]], device='cuda:0'), tensor([[ 1.0219,  0.3149, -1.1579]], device='cuda:0'), tensor([[-2.8940, -0.6547, -1.5540]], device='cuda:0'), tensor([[ 0.8044, -0.3017, -0.0606]], device='cuda:0'), tensor([[-1.5536, -0.2126, -0.2513]], device='cuda:0'), tensor([[ 1.2939,  1.1673, -2.2498]], device='cuda:0'), tensor([[ 0.6879, -1.6218, -1.4965]], device='cuda:0'), tensor([[ 0.2664, -2.0704,  1.4860]], device='cuda:0'), tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0'), tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')]
====================
schedule for this step is
[(12, 0), (11, 1)]
inputting microbatch 12 into partition 0
before moving to cuda:0: tensor([[-0.5300, -0.2377, -1.3405]], device='cuda:0')
after: tensor([[-0.5300, -0.2377, -1.3405]], device='cuda:0')
********************
observing microbatch 12
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.5300, -0.2377, -1.3405]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 12
********************
inputting microbatch 11 into partition 1
before moving to cuda:1: tensor([[0.3859, 0.6584, 0.5157, 0.5007]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 11
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 11
********************
receiving microbatch 12 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 12, tensor([[0.4630, 0.5340, 0.3484, 0.6196]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 12
result is tensor([[0.4630, 0.5340, 0.3484, 0.6196]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4630, 0.5340, 0.3484, 0.6196]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 11 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 11, tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 11
result is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4630, 0.5340, 0.3484, 0.6196]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.4126,  1.0101, -0.9078]], device='cuda:0'), tensor([[-0.4830, -0.2460, -1.0622]], device='cuda:0'), tensor([[ 0.2497, -0.4515, -1.7059]], device='cuda:0'), tensor([[-0.2070, -0.8569,  0.9374]], device='cuda:0'), tensor([[-0.1557,  1.6940,  0.2851]], device='cuda:0'), tensor([[-0.3276,  2.2576,  0.1918]], device='cuda:0'), tensor([[-0.6025,  0.4908, -2.1693]], device='cuda:0'), tensor([[ 0.6174, -1.8837, -0.6706]], device='cuda:0'), tensor([[ 0.3033, -0.0089, -0.4867]], device='cuda:0'), tensor([[-1.3237, -0.1580,  0.9035]], device='cuda:0'), tensor([[ 1.0219,  0.3149, -1.1579]], device='cuda:0'), tensor([[-2.8940, -0.6547, -1.5540]], device='cuda:0'), tensor([[ 0.8044, -0.3017, -0.0606]], device='cuda:0'), tensor([[-1.5536, -0.2126, -0.2513]], device='cuda:0'), tensor([[ 1.2939,  1.1673, -2.2498]], device='cuda:0'), tensor([[ 0.6879, -1.6218, -1.4965]], device='cuda:0'), tensor([[ 0.2664, -2.0704,  1.4860]], device='cuda:0'), tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0'), tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')]
====================
schedule for this step is
[(13, 0), (12, 1)]
inputting microbatch 13 into partition 0
before moving to cuda:0: tensor([[ 0.4126,  1.0101, -0.9078]], device='cuda:0')
after: tensor([[ 0.4126,  1.0101, -0.9078]], device='cuda:0')
********************
observing microbatch 13
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.4126,  1.0101, -0.9078]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 13
********************
inputting microbatch 12 into partition 1
before moving to cuda:1: tensor([[0.4630, 0.5340, 0.3484, 0.6196]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 12
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 12
********************
receiving microbatch 13 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 13, tensor([[0.3867, 0.6041, 0.6722, 0.5329]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 13
result is tensor([[0.3867, 0.6041, 0.6722, 0.5329]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3867, 0.6041, 0.6722, 0.5329]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 12 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 12, tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 12
result is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3867, 0.6041, 0.6722, 0.5329]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.4830, -0.2460, -1.0622]], device='cuda:0'), tensor([[ 0.2497, -0.4515, -1.7059]], device='cuda:0'), tensor([[-0.2070, -0.8569,  0.9374]], device='cuda:0'), tensor([[-0.1557,  1.6940,  0.2851]], device='cuda:0'), tensor([[-0.3276,  2.2576,  0.1918]], device='cuda:0'), tensor([[-0.6025,  0.4908, -2.1693]], device='cuda:0'), tensor([[ 0.6174, -1.8837, -0.6706]], device='cuda:0'), tensor([[ 0.3033, -0.0089, -0.4867]], device='cuda:0'), tensor([[-1.3237, -0.1580,  0.9035]], device='cuda:0'), tensor([[ 1.0219,  0.3149, -1.1579]], device='cuda:0'), tensor([[-2.8940, -0.6547, -1.5540]], device='cuda:0'), tensor([[ 0.8044, -0.3017, -0.0606]], device='cuda:0'), tensor([[-1.5536, -0.2126, -0.2513]], device='cuda:0'), tensor([[ 1.2939,  1.1673, -2.2498]], device='cuda:0'), tensor([[ 0.6879, -1.6218, -1.4965]], device='cuda:0'), tensor([[ 0.2664, -2.0704,  1.4860]], device='cuda:0'), tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0'), tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')]
====================
schedule for this step is
[(14, 0), (13, 1)]
inputting microbatch 14 into partition 0
before moving to cuda:0: tensor([[-0.4830, -0.2460, -1.0622]], device='cuda:0')
after: tensor([[-0.4830, -0.2460, -1.0622]], device='cuda:0')
********************
observing microbatch 14
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.4830, -0.2460, -1.0622]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 14
********************
inputting microbatch 13 into partition 1
before moving to cuda:1: tensor([[0.3867, 0.6041, 0.6722, 0.5329]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 13
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 13
********************
receiving microbatch 14 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 14, tensor([[0.4452, 0.5492, 0.3865, 0.5888]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 14
result is tensor([[0.4452, 0.5492, 0.3865, 0.5888]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4452, 0.5492, 0.3865, 0.5888]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 13 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 13, tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 13
result is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4452, 0.5492, 0.3865, 0.5888]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.2497, -0.4515, -1.7059]], device='cuda:0'), tensor([[-0.2070, -0.8569,  0.9374]], device='cuda:0'), tensor([[-0.1557,  1.6940,  0.2851]], device='cuda:0'), tensor([[-0.3276,  2.2576,  0.1918]], device='cuda:0'), tensor([[-0.6025,  0.4908, -2.1693]], device='cuda:0'), tensor([[ 0.6174, -1.8837, -0.6706]], device='cuda:0'), tensor([[ 0.3033, -0.0089, -0.4867]], device='cuda:0'), tensor([[-1.3237, -0.1580,  0.9035]], device='cuda:0'), tensor([[ 1.0219,  0.3149, -1.1579]], device='cuda:0'), tensor([[-2.8940, -0.6547, -1.5540]], device='cuda:0'), tensor([[ 0.8044, -0.3017, -0.0606]], device='cuda:0'), tensor([[-1.5536, -0.2126, -0.2513]], device='cuda:0'), tensor([[ 1.2939,  1.1673, -2.2498]], device='cuda:0'), tensor([[ 0.6879, -1.6218, -1.4965]], device='cuda:0'), tensor([[ 0.2664, -2.0704,  1.4860]], device='cuda:0'), tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0'), tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')]
====================
schedule for this step is
[(15, 0), (14, 1)]
inputting microbatch 15 into partition 0
before moving to cuda:0: tensor([[ 0.2497, -0.4515, -1.7059]], device='cuda:0')
after: tensor([[ 0.2497, -0.4515, -1.7059]], device='cuda:0')
********************
observing microbatch 15
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.2497, -0.4515, -1.7059]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 15
********************
inputting microbatch 14 into partition 1
before moving to cuda:1: tensor([[0.4452, 0.5492, 0.3865, 0.5888]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 14
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 14
********************
receiving microbatch 15 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 15, tensor([[0.4396, 0.4376, 0.3751, 0.5286]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 15
result is tensor([[0.4396, 0.4376, 0.3751, 0.5286]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4396, 0.4376, 0.3751, 0.5286]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 14 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 14, tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 14
result is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4396, 0.4376, 0.3751, 0.5286]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.2070, -0.8569,  0.9374]], device='cuda:0'), tensor([[-0.1557,  1.6940,  0.2851]], device='cuda:0'), tensor([[-0.3276,  2.2576,  0.1918]], device='cuda:0'), tensor([[-0.6025,  0.4908, -2.1693]], device='cuda:0'), tensor([[ 0.6174, -1.8837, -0.6706]], device='cuda:0'), tensor([[ 0.3033, -0.0089, -0.4867]], device='cuda:0'), tensor([[-1.3237, -0.1580,  0.9035]], device='cuda:0'), tensor([[ 1.0219,  0.3149, -1.1579]], device='cuda:0'), tensor([[-2.8940, -0.6547, -1.5540]], device='cuda:0'), tensor([[ 0.8044, -0.3017, -0.0606]], device='cuda:0'), tensor([[-1.5536, -0.2126, -0.2513]], device='cuda:0'), tensor([[ 1.2939,  1.1673, -2.2498]], device='cuda:0'), tensor([[ 0.6879, -1.6218, -1.4965]], device='cuda:0'), tensor([[ 0.2664, -2.0704,  1.4860]], device='cuda:0'), tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0'), tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')]
====================
schedule for this step is
[(16, 0), (15, 1)]
inputting microbatch 16 into partition 0
before moving to cuda:0: tensor([[-0.2070, -0.8569,  0.9374]], device='cuda:0')
after: tensor([[-0.2070, -0.8569,  0.9374]], device='cuda:0')
********************
observing microbatch 16
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.2070, -0.8569,  0.9374]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 16
********************
inputting microbatch 15 into partition 1
before moving to cuda:1: tensor([[0.4396, 0.4376, 0.3751, 0.5286]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 15
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 15
********************
receiving microbatch 16 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 16, tensor([[0.3273, 0.6150, 0.6002, 0.3362]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 16
result is tensor([[0.3273, 0.6150, 0.6002, 0.3362]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3273, 0.6150, 0.6002, 0.3362]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 15 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 15, tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 15
result is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3273, 0.6150, 0.6002, 0.3362]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.1557,  1.6940,  0.2851]], device='cuda:0'), tensor([[-0.3276,  2.2576,  0.1918]], device='cuda:0'), tensor([[-0.6025,  0.4908, -2.1693]], device='cuda:0'), tensor([[ 0.6174, -1.8837, -0.6706]], device='cuda:0'), tensor([[ 0.3033, -0.0089, -0.4867]], device='cuda:0'), tensor([[-1.3237, -0.1580,  0.9035]], device='cuda:0'), tensor([[ 1.0219,  0.3149, -1.1579]], device='cuda:0'), tensor([[-2.8940, -0.6547, -1.5540]], device='cuda:0'), tensor([[ 0.8044, -0.3017, -0.0606]], device='cuda:0'), tensor([[-1.5536, -0.2126, -0.2513]], device='cuda:0'), tensor([[ 1.2939,  1.1673, -2.2498]], device='cuda:0'), tensor([[ 0.6879, -1.6218, -1.4965]], device='cuda:0'), tensor([[ 0.2664, -2.0704,  1.4860]], device='cuda:0'), tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0'), tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')]
====================
schedule for this step is
[(17, 0), (16, 1)]
inputting microbatch 17 into partition 0
before moving to cuda:0: tensor([[-0.1557,  1.6940,  0.2851]], device='cuda:0')
after: tensor([[-0.1557,  1.6940,  0.2851]], device='cuda:0')
********************
observing microbatch 17
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.1557,  1.6940,  0.2851]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 17
********************
inputting microbatch 16 into partition 1
before moving to cuda:1: tensor([[0.3273, 0.6150, 0.6002, 0.3362]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 16
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 16
********************
receiving microbatch 17 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 17, tensor([[0.3547, 0.7579, 0.7982, 0.5535]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 17
result is tensor([[0.3547, 0.7579, 0.7982, 0.5535]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3547, 0.7579, 0.7982, 0.5535]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 16 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 16, tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 16
result is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3547, 0.7579, 0.7982, 0.5535]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.3276,  2.2576,  0.1918]], device='cuda:0'), tensor([[-0.6025,  0.4908, -2.1693]], device='cuda:0'), tensor([[ 0.6174, -1.8837, -0.6706]], device='cuda:0'), tensor([[ 0.3033, -0.0089, -0.4867]], device='cuda:0'), tensor([[-1.3237, -0.1580,  0.9035]], device='cuda:0'), tensor([[ 1.0219,  0.3149, -1.1579]], device='cuda:0'), tensor([[-2.8940, -0.6547, -1.5540]], device='cuda:0'), tensor([[ 0.8044, -0.3017, -0.0606]], device='cuda:0'), tensor([[-1.5536, -0.2126, -0.2513]], device='cuda:0'), tensor([[ 1.2939,  1.1673, -2.2498]], device='cuda:0'), tensor([[ 0.6879, -1.6218, -1.4965]], device='cuda:0'), tensor([[ 0.2664, -2.0704,  1.4860]], device='cuda:0'), tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0'), tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')]
====================
schedule for this step is
[(18, 0), (17, 1)]
inputting microbatch 18 into partition 0
before moving to cuda:0: tensor([[-0.3276,  2.2576,  0.1918]], device='cuda:0')
after: tensor([[-0.3276,  2.2576,  0.1918]], device='cuda:0')
********************
observing microbatch 18
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.3276,  2.2576,  0.1918]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 18
********************
inputting microbatch 17 into partition 1
before moving to cuda:1: tensor([[0.3547, 0.7579, 0.7982, 0.5535]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 17
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 17
********************
receiving microbatch 18 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 18, tensor([[0.3679, 0.7947, 0.8203, 0.6223]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 18
result is tensor([[0.3679, 0.7947, 0.8203, 0.6223]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3679, 0.7947, 0.8203, 0.6223]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 17 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 17, tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 17
result is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3679, 0.7947, 0.8203, 0.6223]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.6025,  0.4908, -2.1693]], device='cuda:0'), tensor([[ 0.6174, -1.8837, -0.6706]], device='cuda:0'), tensor([[ 0.3033, -0.0089, -0.4867]], device='cuda:0'), tensor([[-1.3237, -0.1580,  0.9035]], device='cuda:0'), tensor([[ 1.0219,  0.3149, -1.1579]], device='cuda:0'), tensor([[-2.8940, -0.6547, -1.5540]], device='cuda:0'), tensor([[ 0.8044, -0.3017, -0.0606]], device='cuda:0'), tensor([[-1.5536, -0.2126, -0.2513]], device='cuda:0'), tensor([[ 1.2939,  1.1673, -2.2498]], device='cuda:0'), tensor([[ 0.6879, -1.6218, -1.4965]], device='cuda:0'), tensor([[ 0.2664, -2.0704,  1.4860]], device='cuda:0'), tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0'), tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')]
====================
schedule for this step is
[(19, 0), (18, 1)]
inputting microbatch 19 into partition 0
before moving to cuda:0: tensor([[-0.6025,  0.4908, -2.1693]], device='cuda:0')
after: tensor([[-0.6025,  0.4908, -2.1693]], device='cuda:0')
********************
observing microbatch 19
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.6025,  0.4908, -2.1693]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 19
********************
inputting microbatch 18 into partition 1
before moving to cuda:1: tensor([[0.3679, 0.7947, 0.8203, 0.6223]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 18
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 18
********************
receiving microbatch 19 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 19, tensor([[0.5120, 0.5434, 0.3261, 0.7356]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 19
result is tensor([[0.5120, 0.5434, 0.3261, 0.7356]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5120, 0.5434, 0.3261, 0.7356]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 18 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 18, tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 18
result is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5120, 0.5434, 0.3261, 0.7356]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.6174, -1.8837, -0.6706]], device='cuda:0'), tensor([[ 0.3033, -0.0089, -0.4867]], device='cuda:0'), tensor([[-1.3237, -0.1580,  0.9035]], device='cuda:0'), tensor([[ 1.0219,  0.3149, -1.1579]], device='cuda:0'), tensor([[-2.8940, -0.6547, -1.5540]], device='cuda:0'), tensor([[ 0.8044, -0.3017, -0.0606]], device='cuda:0'), tensor([[-1.5536, -0.2126, -0.2513]], device='cuda:0'), tensor([[ 1.2939,  1.1673, -2.2498]], device='cuda:0'), tensor([[ 0.6879, -1.6218, -1.4965]], device='cuda:0'), tensor([[ 0.2664, -2.0704,  1.4860]], device='cuda:0'), tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0'), tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')]
====================
schedule for this step is
[(20, 0), (19, 1)]
inputting microbatch 20 into partition 0
before moving to cuda:0: tensor([[ 0.6174, -1.8837, -0.6706]], device='cuda:0')
after: tensor([[ 0.6174, -1.8837, -0.6706]], device='cuda:0')
********************
observing microbatch 20
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.6174, -1.8837, -0.6706]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 20
********************
inputting microbatch 19 into partition 1
before moving to cuda:1: tensor([[0.5120, 0.5434, 0.3261, 0.7356]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 19
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 19
********************
receiving microbatch 20 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 20, tensor([[0.3663, 0.3655, 0.3773, 0.2985]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 20
result is tensor([[0.3663, 0.3655, 0.3773, 0.2985]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3663, 0.3655, 0.3773, 0.2985]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 19 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 19, tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 19
result is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3663, 0.3655, 0.3773, 0.2985]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.3033, -0.0089, -0.4867]], device='cuda:0'), tensor([[-1.3237, -0.1580,  0.9035]], device='cuda:0'), tensor([[ 1.0219,  0.3149, -1.1579]], device='cuda:0'), tensor([[-2.8940, -0.6547, -1.5540]], device='cuda:0'), tensor([[ 0.8044, -0.3017, -0.0606]], device='cuda:0'), tensor([[-1.5536, -0.2126, -0.2513]], device='cuda:0'), tensor([[ 1.2939,  1.1673, -2.2498]], device='cuda:0'), tensor([[ 0.6879, -1.6218, -1.4965]], device='cuda:0'), tensor([[ 0.2664, -2.0704,  1.4860]], device='cuda:0'), tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0'), tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')]
====================
schedule for this step is
[(21, 0), (20, 1)]
inputting microbatch 21 into partition 0
before moving to cuda:0: tensor([[ 0.3033, -0.0089, -0.4867]], device='cuda:0')
after: tensor([[ 0.3033, -0.0089, -0.4867]], device='cuda:0')
********************
observing microbatch 21
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.3033, -0.0089, -0.4867]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 21
********************
inputting microbatch 20 into partition 1
before moving to cuda:1: tensor([[0.3663, 0.3655, 0.3773, 0.2985]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 20
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 20
********************
receiving microbatch 21 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 21, tensor([[0.3717, 0.5555, 0.5907, 0.4421]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 21
result is tensor([[0.3717, 0.5555, 0.5907, 0.4421]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3717, 0.5555, 0.5907, 0.4421]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 20 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 20, tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 20
result is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3717, 0.5555, 0.5907, 0.4421]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-1.3237, -0.1580,  0.9035]], device='cuda:0'), tensor([[ 1.0219,  0.3149, -1.1579]], device='cuda:0'), tensor([[-2.8940, -0.6547, -1.5540]], device='cuda:0'), tensor([[ 0.8044, -0.3017, -0.0606]], device='cuda:0'), tensor([[-1.5536, -0.2126, -0.2513]], device='cuda:0'), tensor([[ 1.2939,  1.1673, -2.2498]], device='cuda:0'), tensor([[ 0.6879, -1.6218, -1.4965]], device='cuda:0'), tensor([[ 0.2664, -2.0704,  1.4860]], device='cuda:0'), tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0'), tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')]
====================
schedule for this step is
[(22, 0), (21, 1)]
inputting microbatch 22 into partition 0
before moving to cuda:0: tensor([[-1.3237, -0.1580,  0.9035]], device='cuda:0')
after: tensor([[-1.3237, -0.1580,  0.9035]], device='cuda:0')
********************
observing microbatch 22
current batch shape is torch.Size([1, 3])
batch is tensor([[-1.3237, -0.1580,  0.9035]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 22
********************
inputting microbatch 21 into partition 1
before moving to cuda:1: tensor([[0.3717, 0.5555, 0.5907, 0.4421]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 21
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 21
********************
receiving microbatch 22 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 22, tensor([[0.3861, 0.7314, 0.5375, 0.5399]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 22
result is tensor([[0.3861, 0.7314, 0.5375, 0.5399]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3861, 0.7314, 0.5375, 0.5399]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 21 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 21, tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 21
result is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3861, 0.7314, 0.5375, 0.5399]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 1.0219,  0.3149, -1.1579]], device='cuda:0'), tensor([[-2.8940, -0.6547, -1.5540]], device='cuda:0'), tensor([[ 0.8044, -0.3017, -0.0606]], device='cuda:0'), tensor([[-1.5536, -0.2126, -0.2513]], device='cuda:0'), tensor([[ 1.2939,  1.1673, -2.2498]], device='cuda:0'), tensor([[ 0.6879, -1.6218, -1.4965]], device='cuda:0'), tensor([[ 0.2664, -2.0704,  1.4860]], device='cuda:0'), tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0'), tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')]
====================
schedule for this step is
[(23, 0), (22, 1)]
inputting microbatch 23 into partition 0
before moving to cuda:0: tensor([[ 1.0219,  0.3149, -1.1579]], device='cuda:0')
after: tensor([[ 1.0219,  0.3149, -1.1579]], device='cuda:0')
********************
observing microbatch 23
current batch shape is torch.Size([1, 3])
batch is tensor([[ 1.0219,  0.3149, -1.1579]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 23
********************
inputting microbatch 22 into partition 1
before moving to cuda:1: tensor([[0.3861, 0.7314, 0.5375, 0.5399]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 22
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 22
********************
receiving microbatch 23 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 23, tensor([[0.3680, 0.4877, 0.6370, 0.4217]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 23
result is tensor([[0.3680, 0.4877, 0.6370, 0.4217]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3680, 0.4877, 0.6370, 0.4217]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 22 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 22, tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 22
result is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3680, 0.4877, 0.6370, 0.4217]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-2.8940, -0.6547, -1.5540]], device='cuda:0'), tensor([[ 0.8044, -0.3017, -0.0606]], device='cuda:0'), tensor([[-1.5536, -0.2126, -0.2513]], device='cuda:0'), tensor([[ 1.2939,  1.1673, -2.2498]], device='cuda:0'), tensor([[ 0.6879, -1.6218, -1.4965]], device='cuda:0'), tensor([[ 0.2664, -2.0704,  1.4860]], device='cuda:0'), tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0'), tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')]
====================
schedule for this step is
[(24, 0), (23, 1)]
inputting microbatch 24 into partition 0
before moving to cuda:0: tensor([[-2.8940, -0.6547, -1.5540]], device='cuda:0')
after: tensor([[-2.8940, -0.6547, -1.5540]], device='cuda:0')
********************
observing microbatch 24
current batch shape is torch.Size([1, 3])
batch is tensor([[-2.8940, -0.6547, -1.5540]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 24
********************
inputting microbatch 23 into partition 1
before moving to cuda:1: tensor([[0.3680, 0.4877, 0.6370, 0.4217]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 23
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 23
********************
receiving microbatch 24 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 24, tensor([[0.6068, 0.6413, 0.1007, 0.8586]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 24
result is tensor([[0.6068, 0.6413, 0.1007, 0.8586]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6068, 0.6413, 0.1007, 0.8586]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 23 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 23, tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 23
result is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6068, 0.6413, 0.1007, 0.8586]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.8044, -0.3017, -0.0606]], device='cuda:0'), tensor([[-1.5536, -0.2126, -0.2513]], device='cuda:0'), tensor([[ 1.2939,  1.1673, -2.2498]], device='cuda:0'), tensor([[ 0.6879, -1.6218, -1.4965]], device='cuda:0'), tensor([[ 0.2664, -2.0704,  1.4860]], device='cuda:0'), tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0'), tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')]
====================
schedule for this step is
[(25, 0), (24, 1)]
inputting microbatch 25 into partition 0
before moving to cuda:0: tensor([[ 0.8044, -0.3017, -0.0606]], device='cuda:0')
after: tensor([[ 0.8044, -0.3017, -0.0606]], device='cuda:0')
********************
observing microbatch 25
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.8044, -0.3017, -0.0606]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 25
********************
inputting microbatch 24 into partition 1
before moving to cuda:1: tensor([[0.6068, 0.6413, 0.1007, 0.8586]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 24
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 24
********************
receiving microbatch 25 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 25, tensor([[0.3250, 0.5260, 0.6690, 0.3205]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 25
result is tensor([[0.3250, 0.5260, 0.6690, 0.3205]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3250, 0.5260, 0.6690, 0.3205]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 24 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 24, tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 24
result is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3250, 0.5260, 0.6690, 0.3205]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-1.5536, -0.2126, -0.2513]], device='cuda:0'), tensor([[ 1.2939,  1.1673, -2.2498]], device='cuda:0'), tensor([[ 0.6879, -1.6218, -1.4965]], device='cuda:0'), tensor([[ 0.2664, -2.0704,  1.4860]], device='cuda:0'), tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0'), tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')]
====================
schedule for this step is
[(26, 0), (25, 1)]
inputting microbatch 26 into partition 0
before moving to cuda:0: tensor([[-1.5536, -0.2126, -0.2513]], device='cuda:0')
after: tensor([[-1.5536, -0.2126, -0.2513]], device='cuda:0')
********************
observing microbatch 26
current batch shape is torch.Size([1, 3])
batch is tensor([[-1.5536, -0.2126, -0.2513]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 26
********************
inputting microbatch 25 into partition 1
before moving to cuda:1: tensor([[0.3250, 0.5260, 0.6690, 0.3205]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 25
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 25
********************
receiving microbatch 26 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 26, tensor([[0.4607, 0.6739, 0.3561, 0.6660]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 26
result is tensor([[0.4607, 0.6739, 0.3561, 0.6660]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4607, 0.6739, 0.3561, 0.6660]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 25 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 25, tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 25
result is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4607, 0.6739, 0.3561, 0.6660]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 1.2939,  1.1673, -2.2498]], device='cuda:0'), tensor([[ 0.6879, -1.6218, -1.4965]], device='cuda:0'), tensor([[ 0.2664, -2.0704,  1.4860]], device='cuda:0'), tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0'), tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')]
====================
schedule for this step is
[(27, 0), (26, 1)]
inputting microbatch 27 into partition 0
before moving to cuda:0: tensor([[ 1.2939,  1.1673, -2.2498]], device='cuda:0')
after: tensor([[ 1.2939,  1.1673, -2.2498]], device='cuda:0')
********************
observing microbatch 27
current batch shape is torch.Size([1, 3])
batch is tensor([[ 1.2939,  1.1673, -2.2498]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 27
********************
inputting microbatch 26 into partition 1
before moving to cuda:1: tensor([[0.4607, 0.6739, 0.3561, 0.6660]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 26
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 26
********************
receiving microbatch 27 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 27, tensor([[0.4098, 0.4660, 0.6389, 0.5376]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 27
result is tensor([[0.4098, 0.4660, 0.6389, 0.5376]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4098, 0.4660, 0.6389, 0.5376]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 26 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 26, tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 26
result is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4098, 0.4660, 0.6389, 0.5376]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.6879, -1.6218, -1.4965]], device='cuda:0'), tensor([[ 0.2664, -2.0704,  1.4860]], device='cuda:0'), tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0'), tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')]
====================
schedule for this step is
[(28, 0), (27, 1)]
inputting microbatch 28 into partition 0
before moving to cuda:0: tensor([[ 0.6879, -1.6218, -1.4965]], device='cuda:0')
after: tensor([[ 0.6879, -1.6218, -1.4965]], device='cuda:0')
********************
observing microbatch 28
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.6879, -1.6218, -1.4965]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 28
********************
inputting microbatch 27 into partition 1
before moving to cuda:1: tensor([[0.4098, 0.4660, 0.6389, 0.5376]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 27
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 27
********************
receiving microbatch 28 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 28, tensor([[0.4054, 0.3295, 0.3191, 0.3710]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 28
result is tensor([[0.4054, 0.3295, 0.3191, 0.3710]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4054, 0.3295, 0.3191, 0.3710]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 27 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 27, tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 27
result is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4054, 0.3295, 0.3191, 0.3710]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.2664, -2.0704,  1.4860]], device='cuda:0'), tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0'), tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')]
====================
schedule for this step is
[(29, 0), (28, 1)]
inputting microbatch 29 into partition 0
before moving to cuda:0: tensor([[ 0.2664, -2.0704,  1.4860]], device='cuda:0')
after: tensor([[ 0.2664, -2.0704,  1.4860]], device='cuda:0')
********************
observing microbatch 29
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.2664, -2.0704,  1.4860]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 29
********************
inputting microbatch 28 into partition 1
before moving to cuda:1: tensor([[0.4054, 0.3295, 0.3191, 0.3710]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 28
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 28
********************
receiving microbatch 29 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 29, tensor([[0.2802, 0.5195, 0.5818, 0.1860]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 29
result is tensor([[0.2802, 0.5195, 0.5818, 0.1860]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.2802, 0.5195, 0.5818, 0.1860]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 28 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 28, tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 28
result is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.2802, 0.5195, 0.5818, 0.1860]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0'), tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')]
====================
schedule for this step is
[(30, 0), (29, 1)]
inputting microbatch 30 into partition 0
before moving to cuda:0: tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0')
after: tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0')
********************
observing microbatch 30
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.3418,  1.5683, -0.2014]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 30
********************
inputting microbatch 29 into partition 1
before moving to cuda:1: tensor([[0.2802, 0.5195, 0.5818, 0.1860]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 29
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 29
********************
receiving microbatch 30 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 30, tensor([[0.3537, 0.6958, 0.7898, 0.5176]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 30
result is tensor([[0.3537, 0.6958, 0.7898, 0.5176]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3537, 0.6958, 0.7898, 0.5176]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 29 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 29, tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 29
result is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3537, 0.6958, 0.7898, 0.5176]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')]
====================
schedule for this step is
[(31, 0), (30, 1)]
inputting microbatch 31 into partition 0
before moving to cuda:0: tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')
after: tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')
********************
observing microbatch 31
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.6095, -0.3728, -0.1772]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 31
********************
inputting microbatch 30 into partition 1
before moving to cuda:1: tensor([[0.3537, 0.6958, 0.7898, 0.5176]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 30
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 30
********************
receiving microbatch 31 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 31, tensor([[0.3405, 0.5252, 0.6237, 0.3504]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 31
result is tensor([[0.3405, 0.5252, 0.6237, 0.3504]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3405, 0.5252, 0.6237, 0.3504]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 30 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 30, tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 30
result is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3405, 0.5252, 0.6237, 0.3504]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)]
====================
schedule for this step is
[(31, 1)]
inputting microbatch 31 into partition 1
before moving to cuda:1: tensor([[0.3405, 0.5252, 0.6237, 0.3504]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 31
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 31
********************
receiving microbatch 31 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 31, tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 31
result is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0., 1., 1., 1., 1.]], device='cuda:1', grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9913, 0.9728, 0.9646, 0.9673]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9906, 0.9706, 0.9621, 0.9651]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9905, 0.9704, 0.9619, 0.9649]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)]
finished running cycle
_____________________________ test_forward_0[1-64] _____________________________

batch_size = 64, split_size = 1

    @pytest.mark.a4_2_2
    @pytest.mark.parametrize("batch_size", [1, 16, 32, 64])
    @pytest.mark.parametrize("split_size", [1, 2, 4, 8, 16])
    # @pytest.mark.parametrize("batch_size", [1])
    # @pytest.mark.parametrize("split_size", [1, 2])
    def test_forward_0(batch_size, split_size):
        model = nn.Sequential(
            nn.Linear(3, 4).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
            nn.Linear(4, 5).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
        )
    
        x = torch.randn(batch_size, 3).to('cuda:0')
        y0 = model(x).to('cpu')
    
        # move the last two layer to another device
        model[-2] = model[-2].to('cuda:1')
        model[-1] = WithDevice(nn.Sigmoid(), 'cuda:1')
        pipe = Pipe(model, split_size=split_size)
        y1 = pipe(x).to('cpu')
>       assert torch.allclose(y0, y1)
E       assert False
E        +  where False = <built-in method allclose of type object at 0x153da78cb760>(tensor([[0.4609, 0.3877, 0.5372, 0.4288, 0.3853],\n        [0.4346, 0.4851, 0.5899, 0.4952, 0.4425],\n        [0.4438, 0.4729, 0.5953, 0.4841, 0.4253],\n        [0.4434, 0.4593, 0.5778, 0.4777, 0.4256],\n        [0.4246, 0.4826, 0.5728, 0.4968, 0.4557],\n        [0.4158, 0.4238, 0.5103, 0.4625, 0.4505],\n        [0.4500, 0.3812, 0.5276, 0.4243, 0.3907],\n        [0.4544, 0.3877, 0.5313, 0.4296, 0.3915],\n        [0.4241, 0.4455, 0.5358, 0.4758, 0.4494],\n        [0.4265, 0.4465, 0.5388, 0.4762, 0.4473],\n        [0.4506, 0.4337, 0.5696, 0.4591, 0.4077],\n        [0.4553, 0.4252, 0.5532, 0.4565, 0.4075],\n        [0.4546, 0.4237, 0.5670, 0.4516, 0.3997],\n        [0.4572, 0.4234, 0.5748, 0.4494, 0.3938],\n        [0.4277, 0.4583, 0.5533, 0.4825, 0.4475],\n        [0.4431, 0.4393, 0.5524, 0.4679, 0.4253],\n        [0.4359, 0.4800, 0.5785, 0.4940, 0.4442],\n        [0.4419, 0.4088, 0.5345, 0.4463, 0.4128],\n        [0.4373, 0.4698, 0.5828, 0.4847, 0.4340],\n        [0.4327, 0.4474, 0.5502, 0.4746, 0.4384],\n        [0.4388, 0.4962, 0.5969, 0.5027, 0.4440],\n        [0.4464, 0.4231, 0.5462, 0.4561, 0.4149],\n        [0.4275, 0.4577, 0.5477, 0.4833, 0.4500],\n        [0.4308, 0.4167, 0.5187, 0.4559, 0.4327]...689, 0.4547, 0.4011],\n        [0.4456, 0.3851, 0.5169, 0.4295, 0.4009],\n        [0.4476, 0.4511, 0.5632, 0.4749, 0.4250],\n        [0.4400, 0.4078, 0.5329, 0.4457, 0.4139],\n        [0.4255, 0.4498, 0.5416, 0.4781, 0.4487],\n        [0.4408, 0.4367, 0.5491, 0.4664, 0.4265],\n        [0.4459, 0.4037, 0.5364, 0.4417, 0.4058],\n        [0.4556, 0.4251, 0.5669, 0.4528, 0.4001],\n        [0.4286, 0.4293, 0.5312, 0.4637, 0.4373],\n        [0.4218, 0.4489, 0.5392, 0.4777, 0.4515],\n        [0.4400, 0.4484, 0.5604, 0.4732, 0.4294],\n        [0.4512, 0.4224, 0.5566, 0.4531, 0.4062],\n        [0.4734, 0.3992, 0.5645, 0.4329, 0.3732],\n        [0.4377, 0.4332, 0.5447, 0.4644, 0.4281],\n        [0.4334, 0.4263, 0.5338, 0.4609, 0.4311],\n        [0.4514, 0.4224, 0.5513, 0.4544, 0.4088],\n        [0.4263, 0.4399, 0.5292, 0.4729, 0.4474],\n        [0.4392, 0.4550, 0.5589, 0.4790, 0.4353],\n        [0.4479, 0.4570, 0.5882, 0.4731, 0.4155],\n        [0.4313, 0.4646, 0.5510, 0.4882, 0.4506],\n        [0.4375, 0.4352, 0.5362, 0.4682, 0.4341],\n        [0.4429, 0.4289, 0.5496, 0.4599, 0.4194],\n        [0.4357, 0.4832, 0.5820, 0.4957, 0.4446],\n        [0.4300, 0.4398, 0.5467, 0.4690, 0.4365]], grad_fn=<ToCopyBackward0>), tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311],\n        [0.9648, 0.9941, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311]...311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311],\n        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], grad_fn=<ToCopyBackward0>))
E        +    where <built-in method allclose of type object at 0x153da78cb760> = torch.allclose

batch_size = 64
model      = Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
  (2): Linear(in_features=4, out_features=5, bias=True)
  (3): WithDevice(
    (_module): Sigmoid()
  )
)
pipe       = Pipe(
  (partitions): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=3, out_features=4, bias=True)
   ...inear(in_features=4, out_features=5, bias=True)
      (1): WithDevice(
        (_module): Sigmoid()
      )
    )
  )
)
split_size = 1
x          = tensor([[-1.4029,  0.0162,  1.6835],
        [ 0.7325, -1.0311, -1.6610],
        [ 0.3561, -1.6771, -1.2184],
       ... 0.0377, -0.1169,  0.3499],
        [-0.2439,  0.0276, -1.9983],
        [ 1.4419, -0.3411,  0.3442]], device='cuda:0')
y0         = tensor([[0.4609, 0.3877, 0.5372, 0.4288, 0.3853],
        [0.4346, 0.4851, 0.5899, 0.4952, 0.4425],
        [0.4438, 0...[0.4357, 0.4832, 0.5820, 0.4957, 0.4446],
        [0.4300, 0.4398, 0.5467, 0.4690, 0.4365]], grad_fn=<ToCopyBackward0>)
y1         = tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311],
        [0.9648, 0.9941, 0.7311, 0.7311, 0.7311],
        [0.9654, 0...[0.9654, 0.9942, 0.7311, 0.7311, 0.7311],
        [0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], grad_fn=<ToCopyBackward0>)

tests/test_pipeline.py:83: AssertionError
----------------------------- Captured stdout call -----------------------------
in init
ModuleList(
  (0): Sequential(
    (0): Linear(in_features=3, out_features=4, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
  (1): Sequential(
    (0): Linear(in_features=4, out_features=5, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
)
[device(type='cuda', index=0), device(type='cuda', index=1)]
before running the cycle
[tensor([[-1.4029,  0.0162,  1.6835]], device='cuda:0'), tensor([[ 0.7325, -1.0311, -1.6610]], device='cuda:0'), tensor([[ 0.3561, -1.6771, -1.2184]], device='cuda:0'), tensor([[-0.0821, -0.7277, -0.8305]], device='cuda:0'), tensor([[ 1.2477, -0.0230, -1.7589]], device='cuda:0'), tensor([[1.5081, 1.7815, 0.8988]], device='cuda:0'), tensor([[ 0.3307, -0.5491,  2.5208]], device='cuda:0'), tensor([[-0.7972,  0.1108,  1.8350]], device='cuda:0'), tensor([[ 0.9680,  0.9619, -0.2934]], device='cuda:0'), tensor([[ 0.7002,  0.9288, -0.3785]], device='cuda:0'), tensor([[-0.0906, -1.0878,  0.1841]], device='cuda:0'), tensor([[-1.9266,  0.6633, -0.1954]], device='cuda:0'), tensor([[-0.2679, -1.1845,  0.5339]], device='cuda:0'), tensor([[-0.0554, -1.8176,  0.6832]], device='cuda:0'), tensor([[ 0.8967,  0.3229, -0.7313]], device='cuda:0'), tensor([[-0.6632,  0.4736, -0.3262]], device='cuda:0'), tensor([[-0.3157,  0.1757, -1.8963]], device='cuda:0'), tensor([[ 0.4078, -0.0678,  1.2859]], device='cuda:0'), tensor([[ 0.8052, -1.1757, -0.9411]], device='cuda:0'), tensor([[ 0.5656,  0.2175, -0.2839]], device='cuda:0'), tensor([[-0.7292, -0.2653, -2.7252]], device='cuda:0'), tensor([[-0.5452,  0.2312,  0.3703]], device='cuda:0'), tensor([[ 0.4026,  0.9891, -0.9871]], device='cuda:0'), tensor([[-0.1987,  1.7354,  0.7749]], device='cuda:0'), tensor([[-1.0050, -0.6191, -0.0147]], device='cuda:0'), tensor([[-0.0749,  1.0228, -0.6044]], device='cuda:0'), tensor([[-0.8288, -1.9254, -0.2606]], device='cuda:0'), tensor([[-0.3670, -1.2722,  1.2708]], device='cuda:0'), tensor([[ 0.8890,  0.8967, -0.6183]], device='cuda:0'), tensor([[0.3361, 0.0314, 0.6872]], device='cuda:0'), tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0'), tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0'), tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0'), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(0, 0)]
inputting microbatch 0 into partition 0
before moving to cuda:0: tensor([[-1.4029,  0.0162,  1.6835]], device='cuda:0')
after: tensor([[-1.4029,  0.0162,  1.6835]], device='cuda:0')
********************
observing microbatch 0
current batch shape is torch.Size([1, 3])
batch is tensor([[-1.4029,  0.0162,  1.6835]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 0
********************
receiving microbatch 0 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 0, tensor([[0.7971, 0.4270, 0.7577, 0.2275]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.7971, 0.4270, 0.7577, 0.2275]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.7971, 0.4270, 0.7577, 0.2275]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.7971, 0.4270, 0.7577, 0.2275]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.7325, -1.0311, -1.6610]], device='cuda:0'), tensor([[ 0.3561, -1.6771, -1.2184]], device='cuda:0'), tensor([[-0.0821, -0.7277, -0.8305]], device='cuda:0'), tensor([[ 1.2477, -0.0230, -1.7589]], device='cuda:0'), tensor([[1.5081, 1.7815, 0.8988]], device='cuda:0'), tensor([[ 0.3307, -0.5491,  2.5208]], device='cuda:0'), tensor([[-0.7972,  0.1108,  1.8350]], device='cuda:0'), tensor([[ 0.9680,  0.9619, -0.2934]], device='cuda:0'), tensor([[ 0.7002,  0.9288, -0.3785]], device='cuda:0'), tensor([[-0.0906, -1.0878,  0.1841]], device='cuda:0'), tensor([[-1.9266,  0.6633, -0.1954]], device='cuda:0'), tensor([[-0.2679, -1.1845,  0.5339]], device='cuda:0'), tensor([[-0.0554, -1.8176,  0.6832]], device='cuda:0'), tensor([[ 0.8967,  0.3229, -0.7313]], device='cuda:0'), tensor([[-0.6632,  0.4736, -0.3262]], device='cuda:0'), tensor([[-0.3157,  0.1757, -1.8963]], device='cuda:0'), tensor([[ 0.4078, -0.0678,  1.2859]], device='cuda:0'), tensor([[ 0.8052, -1.1757, -0.9411]], device='cuda:0'), tensor([[ 0.5656,  0.2175, -0.2839]], device='cuda:0'), tensor([[-0.7292, -0.2653, -2.7252]], device='cuda:0'), tensor([[-0.5452,  0.2312,  0.3703]], device='cuda:0'), tensor([[ 0.4026,  0.9891, -0.9871]], device='cuda:0'), tensor([[-0.1987,  1.7354,  0.7749]], device='cuda:0'), tensor([[-1.0050, -0.6191, -0.0147]], device='cuda:0'), tensor([[-0.0749,  1.0228, -0.6044]], device='cuda:0'), tensor([[-0.8288, -1.9254, -0.2606]], device='cuda:0'), tensor([[-0.3670, -1.2722,  1.2708]], device='cuda:0'), tensor([[ 0.8890,  0.8967, -0.6183]], device='cuda:0'), tensor([[0.3361, 0.0314, 0.6872]], device='cuda:0'), tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0'), tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0'), tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0'), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(1, 0), (0, 1)]
inputting microbatch 1 into partition 0
before moving to cuda:0: tensor([[ 0.7325, -1.0311, -1.6610]], device='cuda:0')
after: tensor([[ 0.7325, -1.0311, -1.6610]], device='cuda:0')
********************
observing microbatch 1
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.7325, -1.0311, -1.6610]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 1
********************
inputting microbatch 0 into partition 1
before moving to cuda:1: tensor([[0.7971, 0.4270, 0.7577, 0.2275]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9619, 0.9649, 0.0000, 0.9905]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 0
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9619, 0.9649, 0.0000, 0.9905]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 0
********************
receiving microbatch 1 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 1, tensor([[0.2858, 0.2997, 0.2499, 0.7205]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 1
result is tensor([[0.2858, 0.2997, 0.2499, 0.7205]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.2858, 0.2997, 0.2499, 0.7205]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 0 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 0, tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.2858, 0.2997, 0.2499, 0.7205]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.3561, -1.6771, -1.2184]], device='cuda:0'), tensor([[-0.0821, -0.7277, -0.8305]], device='cuda:0'), tensor([[ 1.2477, -0.0230, -1.7589]], device='cuda:0'), tensor([[1.5081, 1.7815, 0.8988]], device='cuda:0'), tensor([[ 0.3307, -0.5491,  2.5208]], device='cuda:0'), tensor([[-0.7972,  0.1108,  1.8350]], device='cuda:0'), tensor([[ 0.9680,  0.9619, -0.2934]], device='cuda:0'), tensor([[ 0.7002,  0.9288, -0.3785]], device='cuda:0'), tensor([[-0.0906, -1.0878,  0.1841]], device='cuda:0'), tensor([[-1.9266,  0.6633, -0.1954]], device='cuda:0'), tensor([[-0.2679, -1.1845,  0.5339]], device='cuda:0'), tensor([[-0.0554, -1.8176,  0.6832]], device='cuda:0'), tensor([[ 0.8967,  0.3229, -0.7313]], device='cuda:0'), tensor([[-0.6632,  0.4736, -0.3262]], device='cuda:0'), tensor([[-0.3157,  0.1757, -1.8963]], device='cuda:0'), tensor([[ 0.4078, -0.0678,  1.2859]], device='cuda:0'), tensor([[ 0.8052, -1.1757, -0.9411]], device='cuda:0'), tensor([[ 0.5656,  0.2175, -0.2839]], device='cuda:0'), tensor([[-0.7292, -0.2653, -2.7252]], device='cuda:0'), tensor([[-0.5452,  0.2312,  0.3703]], device='cuda:0'), tensor([[ 0.4026,  0.9891, -0.9871]], device='cuda:0'), tensor([[-0.1987,  1.7354,  0.7749]], device='cuda:0'), tensor([[-1.0050, -0.6191, -0.0147]], device='cuda:0'), tensor([[-0.0749,  1.0228, -0.6044]], device='cuda:0'), tensor([[-0.8288, -1.9254, -0.2606]], device='cuda:0'), tensor([[-0.3670, -1.2722,  1.2708]], device='cuda:0'), tensor([[ 0.8890,  0.8967, -0.6183]], device='cuda:0'), tensor([[0.3361, 0.0314, 0.6872]], device='cuda:0'), tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0'), tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0'), tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0'), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(2, 0), (1, 1)]
inputting microbatch 2 into partition 0
before moving to cuda:0: tensor([[ 0.3561, -1.6771, -1.2184]], device='cuda:0')
after: tensor([[ 0.3561, -1.6771, -1.2184]], device='cuda:0')
********************
observing microbatch 2
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.3561, -1.6771, -1.2184]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 2
********************
inputting microbatch 1 into partition 1
before moving to cuda:1: tensor([[0.2858, 0.2997, 0.2499, 0.7205]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9451, 0.9962, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 1
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9451, 0.9962, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 1
********************
receiving microbatch 2 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 2, tensor([[0.2616, 0.2507, 0.4039, 0.7502]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 2
result is tensor([[0.2616, 0.2507, 0.4039, 0.7502]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.2616, 0.2507, 0.4039, 0.7502]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 1 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 1, tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 1
result is tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.2616, 0.2507, 0.4039, 0.7502]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.0821, -0.7277, -0.8305]], device='cuda:0'), tensor([[ 1.2477, -0.0230, -1.7589]], device='cuda:0'), tensor([[1.5081, 1.7815, 0.8988]], device='cuda:0'), tensor([[ 0.3307, -0.5491,  2.5208]], device='cuda:0'), tensor([[-0.7972,  0.1108,  1.8350]], device='cuda:0'), tensor([[ 0.9680,  0.9619, -0.2934]], device='cuda:0'), tensor([[ 0.7002,  0.9288, -0.3785]], device='cuda:0'), tensor([[-0.0906, -1.0878,  0.1841]], device='cuda:0'), tensor([[-1.9266,  0.6633, -0.1954]], device='cuda:0'), tensor([[-0.2679, -1.1845,  0.5339]], device='cuda:0'), tensor([[-0.0554, -1.8176,  0.6832]], device='cuda:0'), tensor([[ 0.8967,  0.3229, -0.7313]], device='cuda:0'), tensor([[-0.6632,  0.4736, -0.3262]], device='cuda:0'), tensor([[-0.3157,  0.1757, -1.8963]], device='cuda:0'), tensor([[ 0.4078, -0.0678,  1.2859]], device='cuda:0'), tensor([[ 0.8052, -1.1757, -0.9411]], device='cuda:0'), tensor([[ 0.5656,  0.2175, -0.2839]], device='cuda:0'), tensor([[-0.7292, -0.2653, -2.7252]], device='cuda:0'), tensor([[-0.5452,  0.2312,  0.3703]], device='cuda:0'), tensor([[ 0.4026,  0.9891, -0.9871]], device='cuda:0'), tensor([[-0.1987,  1.7354,  0.7749]], device='cuda:0'), tensor([[-1.0050, -0.6191, -0.0147]], device='cuda:0'), tensor([[-0.0749,  1.0228, -0.6044]], device='cuda:0'), tensor([[-0.8288, -1.9254, -0.2606]], device='cuda:0'), tensor([[-0.3670, -1.2722,  1.2708]], device='cuda:0'), tensor([[ 0.8890,  0.8967, -0.6183]], device='cuda:0'), tensor([[0.3361, 0.0314, 0.6872]], device='cuda:0'), tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0'), tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0'), tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0'), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(3, 0), (2, 1)]
inputting microbatch 3 into partition 0
before moving to cuda:0: tensor([[-0.0821, -0.7277, -0.8305]], device='cuda:0')
after: tensor([[-0.0821, -0.7277, -0.8305]], device='cuda:0')
********************
observing microbatch 3
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.0821, -0.7277, -0.8305]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 3
********************
inputting microbatch 2 into partition 1
before moving to cuda:1: tensor([[0.2616, 0.2507, 0.4039, 0.7502]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9648, 0.9941, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 2
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9648, 0.9941, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 2
********************
receiving microbatch 3 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 3, tensor([[0.4129, 0.3144, 0.3968, 0.5966]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 3
result is tensor([[0.4129, 0.3144, 0.3968, 0.5966]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4129, 0.3144, 0.3968, 0.5966]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 2 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 2, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 2
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4129, 0.3144, 0.3968, 0.5966]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 1.2477, -0.0230, -1.7589]], device='cuda:0'), tensor([[1.5081, 1.7815, 0.8988]], device='cuda:0'), tensor([[ 0.3307, -0.5491,  2.5208]], device='cuda:0'), tensor([[-0.7972,  0.1108,  1.8350]], device='cuda:0'), tensor([[ 0.9680,  0.9619, -0.2934]], device='cuda:0'), tensor([[ 0.7002,  0.9288, -0.3785]], device='cuda:0'), tensor([[-0.0906, -1.0878,  0.1841]], device='cuda:0'), tensor([[-1.9266,  0.6633, -0.1954]], device='cuda:0'), tensor([[-0.2679, -1.1845,  0.5339]], device='cuda:0'), tensor([[-0.0554, -1.8176,  0.6832]], device='cuda:0'), tensor([[ 0.8967,  0.3229, -0.7313]], device='cuda:0'), tensor([[-0.6632,  0.4736, -0.3262]], device='cuda:0'), tensor([[-0.3157,  0.1757, -1.8963]], device='cuda:0'), tensor([[ 0.4078, -0.0678,  1.2859]], device='cuda:0'), tensor([[ 0.8052, -1.1757, -0.9411]], device='cuda:0'), tensor([[ 0.5656,  0.2175, -0.2839]], device='cuda:0'), tensor([[-0.7292, -0.2653, -2.7252]], device='cuda:0'), tensor([[-0.5452,  0.2312,  0.3703]], device='cuda:0'), tensor([[ 0.4026,  0.9891, -0.9871]], device='cuda:0'), tensor([[-0.1987,  1.7354,  0.7749]], device='cuda:0'), tensor([[-1.0050, -0.6191, -0.0147]], device='cuda:0'), tensor([[-0.0749,  1.0228, -0.6044]], device='cuda:0'), tensor([[-0.8288, -1.9254, -0.2606]], device='cuda:0'), tensor([[-0.3670, -1.2722,  1.2708]], device='cuda:0'), tensor([[ 0.8890,  0.8967, -0.6183]], device='cuda:0'), tensor([[0.3361, 0.0314, 0.6872]], device='cuda:0'), tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0'), tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0'), tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0'), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(4, 0), (3, 1)]
inputting microbatch 4 into partition 0
before moving to cuda:0: tensor([[ 1.2477, -0.0230, -1.7589]], device='cuda:0')
after: tensor([[ 1.2477, -0.0230, -1.7589]], device='cuda:0')
********************
observing microbatch 4
current batch shape is torch.Size([1, 3])
batch is tensor([[ 1.2477, -0.0230, -1.7589]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 4
********************
inputting microbatch 3 into partition 1
before moving to cuda:1: tensor([[0.4129, 0.3144, 0.3968, 0.5966]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 3
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 3
********************
receiving microbatch 4 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 4, tensor([[0.4036, 0.4149, 0.1323, 0.6035]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 4
result is tensor([[0.4036, 0.4149, 0.1323, 0.6035]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4036, 0.4149, 0.1323, 0.6035]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 3 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 3, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 3
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4036, 0.4149, 0.1323, 0.6035]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[1.5081, 1.7815, 0.8988]], device='cuda:0'), tensor([[ 0.3307, -0.5491,  2.5208]], device='cuda:0'), tensor([[-0.7972,  0.1108,  1.8350]], device='cuda:0'), tensor([[ 0.9680,  0.9619, -0.2934]], device='cuda:0'), tensor([[ 0.7002,  0.9288, -0.3785]], device='cuda:0'), tensor([[-0.0906, -1.0878,  0.1841]], device='cuda:0'), tensor([[-1.9266,  0.6633, -0.1954]], device='cuda:0'), tensor([[-0.2679, -1.1845,  0.5339]], device='cuda:0'), tensor([[-0.0554, -1.8176,  0.6832]], device='cuda:0'), tensor([[ 0.8967,  0.3229, -0.7313]], device='cuda:0'), tensor([[-0.6632,  0.4736, -0.3262]], device='cuda:0'), tensor([[-0.3157,  0.1757, -1.8963]], device='cuda:0'), tensor([[ 0.4078, -0.0678,  1.2859]], device='cuda:0'), tensor([[ 0.8052, -1.1757, -0.9411]], device='cuda:0'), tensor([[ 0.5656,  0.2175, -0.2839]], device='cuda:0'), tensor([[-0.7292, -0.2653, -2.7252]], device='cuda:0'), tensor([[-0.5452,  0.2312,  0.3703]], device='cuda:0'), tensor([[ 0.4026,  0.9891, -0.9871]], device='cuda:0'), tensor([[-0.1987,  1.7354,  0.7749]], device='cuda:0'), tensor([[-1.0050, -0.6191, -0.0147]], device='cuda:0'), tensor([[-0.0749,  1.0228, -0.6044]], device='cuda:0'), tensor([[-0.8288, -1.9254, -0.2606]], device='cuda:0'), tensor([[-0.3670, -1.2722,  1.2708]], device='cuda:0'), tensor([[ 0.8890,  0.8967, -0.6183]], device='cuda:0'), tensor([[0.3361, 0.0314, 0.6872]], device='cuda:0'), tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0'), tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0'), tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0'), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(5, 0), (4, 1)]
inputting microbatch 5 into partition 0
before moving to cuda:0: tensor([[1.5081, 1.7815, 0.8988]], device='cuda:0')
after: tensor([[1.5081, 1.7815, 0.8988]], device='cuda:0')
********************
observing microbatch 5
current batch shape is torch.Size([1, 3])
batch is tensor([[1.5081, 1.7815, 0.8988]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 5
********************
inputting microbatch 4 into partition 1
before moving to cuda:1: tensor([[0.4036, 0.4149, 0.1323, 0.6035]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 4
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 4
********************
receiving microbatch 5 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 5, tensor([[0.8928, 0.7393, 0.1747, 0.1308]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 5
result is tensor([[0.8928, 0.7393, 0.1747, 0.1308]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.8928, 0.7393, 0.1747, 0.1308]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 4 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 4, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 4
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8928, 0.7393, 0.1747, 0.1308]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.3307, -0.5491,  2.5208]], device='cuda:0'), tensor([[-0.7972,  0.1108,  1.8350]], device='cuda:0'), tensor([[ 0.9680,  0.9619, -0.2934]], device='cuda:0'), tensor([[ 0.7002,  0.9288, -0.3785]], device='cuda:0'), tensor([[-0.0906, -1.0878,  0.1841]], device='cuda:0'), tensor([[-1.9266,  0.6633, -0.1954]], device='cuda:0'), tensor([[-0.2679, -1.1845,  0.5339]], device='cuda:0'), tensor([[-0.0554, -1.8176,  0.6832]], device='cuda:0'), tensor([[ 0.8967,  0.3229, -0.7313]], device='cuda:0'), tensor([[-0.6632,  0.4736, -0.3262]], device='cuda:0'), tensor([[-0.3157,  0.1757, -1.8963]], device='cuda:0'), tensor([[ 0.4078, -0.0678,  1.2859]], device='cuda:0'), tensor([[ 0.8052, -1.1757, -0.9411]], device='cuda:0'), tensor([[ 0.5656,  0.2175, -0.2839]], device='cuda:0'), tensor([[-0.7292, -0.2653, -2.7252]], device='cuda:0'), tensor([[-0.5452,  0.2312,  0.3703]], device='cuda:0'), tensor([[ 0.4026,  0.9891, -0.9871]], device='cuda:0'), tensor([[-0.1987,  1.7354,  0.7749]], device='cuda:0'), tensor([[-1.0050, -0.6191, -0.0147]], device='cuda:0'), tensor([[-0.0749,  1.0228, -0.6044]], device='cuda:0'), tensor([[-0.8288, -1.9254, -0.2606]], device='cuda:0'), tensor([[-0.3670, -1.2722,  1.2708]], device='cuda:0'), tensor([[ 0.8890,  0.8967, -0.6183]], device='cuda:0'), tensor([[0.3361, 0.0314, 0.6872]], device='cuda:0'), tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0'), tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0'), tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0'), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(6, 0), (5, 1)]
inputting microbatch 6 into partition 0
before moving to cuda:0: tensor([[ 0.3307, -0.5491,  2.5208]], device='cuda:0')
after: tensor([[ 0.3307, -0.5491,  2.5208]], device='cuda:0')
********************
observing microbatch 6
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.3307, -0.5491,  2.5208]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 6
********************
inputting microbatch 5 into partition 1
before moving to cuda:1: tensor([[0.8928, 0.7393, 0.1747, 0.1308]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 5
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 5
********************
receiving microbatch 6 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 6, tensor([[0.8469, 0.5532, 0.7172, 0.2041]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 6
result is tensor([[0.8469, 0.5532, 0.7172, 0.2041]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.8469, 0.5532, 0.7172, 0.2041]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 5 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 5, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 5
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8469, 0.5532, 0.7172, 0.2041]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.7972,  0.1108,  1.8350]], device='cuda:0'), tensor([[ 0.9680,  0.9619, -0.2934]], device='cuda:0'), tensor([[ 0.7002,  0.9288, -0.3785]], device='cuda:0'), tensor([[-0.0906, -1.0878,  0.1841]], device='cuda:0'), tensor([[-1.9266,  0.6633, -0.1954]], device='cuda:0'), tensor([[-0.2679, -1.1845,  0.5339]], device='cuda:0'), tensor([[-0.0554, -1.8176,  0.6832]], device='cuda:0'), tensor([[ 0.8967,  0.3229, -0.7313]], device='cuda:0'), tensor([[-0.6632,  0.4736, -0.3262]], device='cuda:0'), tensor([[-0.3157,  0.1757, -1.8963]], device='cuda:0'), tensor([[ 0.4078, -0.0678,  1.2859]], device='cuda:0'), tensor([[ 0.8052, -1.1757, -0.9411]], device='cuda:0'), tensor([[ 0.5656,  0.2175, -0.2839]], device='cuda:0'), tensor([[-0.7292, -0.2653, -2.7252]], device='cuda:0'), tensor([[-0.5452,  0.2312,  0.3703]], device='cuda:0'), tensor([[ 0.4026,  0.9891, -0.9871]], device='cuda:0'), tensor([[-0.1987,  1.7354,  0.7749]], device='cuda:0'), tensor([[-1.0050, -0.6191, -0.0147]], device='cuda:0'), tensor([[-0.0749,  1.0228, -0.6044]], device='cuda:0'), tensor([[-0.8288, -1.9254, -0.2606]], device='cuda:0'), tensor([[-0.3670, -1.2722,  1.2708]], device='cuda:0'), tensor([[ 0.8890,  0.8967, -0.6183]], device='cuda:0'), tensor([[0.3361, 0.0314, 0.6872]], device='cuda:0'), tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0'), tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0'), tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0'), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(7, 0), (6, 1)]
inputting microbatch 7 into partition 0
before moving to cuda:0: tensor([[-0.7972,  0.1108,  1.8350]], device='cuda:0')
after: tensor([[-0.7972,  0.1108,  1.8350]], device='cuda:0')
********************
observing microbatch 7
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.7972,  0.1108,  1.8350]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 7
********************
inputting microbatch 6 into partition 1
before moving to cuda:1: tensor([[0.8469, 0.5532, 0.7172, 0.2041]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 6
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 6
********************
receiving microbatch 7 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 7, tensor([[0.8265, 0.4895, 0.7029, 0.2035]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 7
result is tensor([[0.8265, 0.4895, 0.7029, 0.2035]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.8265, 0.4895, 0.7029, 0.2035]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 6 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 6, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 6
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8265, 0.4895, 0.7029, 0.2035]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.9680,  0.9619, -0.2934]], device='cuda:0'), tensor([[ 0.7002,  0.9288, -0.3785]], device='cuda:0'), tensor([[-0.0906, -1.0878,  0.1841]], device='cuda:0'), tensor([[-1.9266,  0.6633, -0.1954]], device='cuda:0'), tensor([[-0.2679, -1.1845,  0.5339]], device='cuda:0'), tensor([[-0.0554, -1.8176,  0.6832]], device='cuda:0'), tensor([[ 0.8967,  0.3229, -0.7313]], device='cuda:0'), tensor([[-0.6632,  0.4736, -0.3262]], device='cuda:0'), tensor([[-0.3157,  0.1757, -1.8963]], device='cuda:0'), tensor([[ 0.4078, -0.0678,  1.2859]], device='cuda:0'), tensor([[ 0.8052, -1.1757, -0.9411]], device='cuda:0'), tensor([[ 0.5656,  0.2175, -0.2839]], device='cuda:0'), tensor([[-0.7292, -0.2653, -2.7252]], device='cuda:0'), tensor([[-0.5452,  0.2312,  0.3703]], device='cuda:0'), tensor([[ 0.4026,  0.9891, -0.9871]], device='cuda:0'), tensor([[-0.1987,  1.7354,  0.7749]], device='cuda:0'), tensor([[-1.0050, -0.6191, -0.0147]], device='cuda:0'), tensor([[-0.0749,  1.0228, -0.6044]], device='cuda:0'), tensor([[-0.8288, -1.9254, -0.2606]], device='cuda:0'), tensor([[-0.3670, -1.2722,  1.2708]], device='cuda:0'), tensor([[ 0.8890,  0.8967, -0.6183]], device='cuda:0'), tensor([[0.3361, 0.0314, 0.6872]], device='cuda:0'), tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0'), tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0'), tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0'), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(8, 0), (7, 1)]
inputting microbatch 8 into partition 0
before moving to cuda:0: tensor([[ 0.9680,  0.9619, -0.2934]], device='cuda:0')
after: tensor([[ 0.9680,  0.9619, -0.2934]], device='cuda:0')
********************
observing microbatch 8
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.9680,  0.9619, -0.2934]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 8
********************
inputting microbatch 7 into partition 1
before moving to cuda:1: tensor([[0.8265, 0.4895, 0.7029, 0.2035]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 7
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 7
********************
receiving microbatch 8 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 8, tensor([[0.7209, 0.5721, 0.1856, 0.3016]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 8
result is tensor([[0.7209, 0.5721, 0.1856, 0.3016]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.7209, 0.5721, 0.1856, 0.3016]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 7 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 7, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 7
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.7209, 0.5721, 0.1856, 0.3016]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.7002,  0.9288, -0.3785]], device='cuda:0'), tensor([[-0.0906, -1.0878,  0.1841]], device='cuda:0'), tensor([[-1.9266,  0.6633, -0.1954]], device='cuda:0'), tensor([[-0.2679, -1.1845,  0.5339]], device='cuda:0'), tensor([[-0.0554, -1.8176,  0.6832]], device='cuda:0'), tensor([[ 0.8967,  0.3229, -0.7313]], device='cuda:0'), tensor([[-0.6632,  0.4736, -0.3262]], device='cuda:0'), tensor([[-0.3157,  0.1757, -1.8963]], device='cuda:0'), tensor([[ 0.4078, -0.0678,  1.2859]], device='cuda:0'), tensor([[ 0.8052, -1.1757, -0.9411]], device='cuda:0'), tensor([[ 0.5656,  0.2175, -0.2839]], device='cuda:0'), tensor([[-0.7292, -0.2653, -2.7252]], device='cuda:0'), tensor([[-0.5452,  0.2312,  0.3703]], device='cuda:0'), tensor([[ 0.4026,  0.9891, -0.9871]], device='cuda:0'), tensor([[-0.1987,  1.7354,  0.7749]], device='cuda:0'), tensor([[-1.0050, -0.6191, -0.0147]], device='cuda:0'), tensor([[-0.0749,  1.0228, -0.6044]], device='cuda:0'), tensor([[-0.8288, -1.9254, -0.2606]], device='cuda:0'), tensor([[-0.3670, -1.2722,  1.2708]], device='cuda:0'), tensor([[ 0.8890,  0.8967, -0.6183]], device='cuda:0'), tensor([[0.3361, 0.0314, 0.6872]], device='cuda:0'), tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0'), tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0'), tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0'), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(9, 0), (8, 1)]
inputting microbatch 9 into partition 0
before moving to cuda:0: tensor([[ 0.7002,  0.9288, -0.3785]], device='cuda:0')
after: tensor([[ 0.7002,  0.9288, -0.3785]], device='cuda:0')
********************
observing microbatch 9
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.7002,  0.9288, -0.3785]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 9
********************
inputting microbatch 8 into partition 1
before moving to cuda:1: tensor([[0.7209, 0.5721, 0.1856, 0.3016]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 8
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 8
********************
receiving microbatch 9 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 9, tensor([[0.7022, 0.5444, 0.2028, 0.3158]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 9
result is tensor([[0.7022, 0.5444, 0.2028, 0.3158]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.7022, 0.5444, 0.2028, 0.3158]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 8 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 8, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 8
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.7022, 0.5444, 0.2028, 0.3158]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.0906, -1.0878,  0.1841]], device='cuda:0'), tensor([[-1.9266,  0.6633, -0.1954]], device='cuda:0'), tensor([[-0.2679, -1.1845,  0.5339]], device='cuda:0'), tensor([[-0.0554, -1.8176,  0.6832]], device='cuda:0'), tensor([[ 0.8967,  0.3229, -0.7313]], device='cuda:0'), tensor([[-0.6632,  0.4736, -0.3262]], device='cuda:0'), tensor([[-0.3157,  0.1757, -1.8963]], device='cuda:0'), tensor([[ 0.4078, -0.0678,  1.2859]], device='cuda:0'), tensor([[ 0.8052, -1.1757, -0.9411]], device='cuda:0'), tensor([[ 0.5656,  0.2175, -0.2839]], device='cuda:0'), tensor([[-0.7292, -0.2653, -2.7252]], device='cuda:0'), tensor([[-0.5452,  0.2312,  0.3703]], device='cuda:0'), tensor([[ 0.4026,  0.9891, -0.9871]], device='cuda:0'), tensor([[-0.1987,  1.7354,  0.7749]], device='cuda:0'), tensor([[-1.0050, -0.6191, -0.0147]], device='cuda:0'), tensor([[-0.0749,  1.0228, -0.6044]], device='cuda:0'), tensor([[-0.8288, -1.9254, -0.2606]], device='cuda:0'), tensor([[-0.3670, -1.2722,  1.2708]], device='cuda:0'), tensor([[ 0.8890,  0.8967, -0.6183]], device='cuda:0'), tensor([[0.3361, 0.0314, 0.6872]], device='cuda:0'), tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0'), tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0'), tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0'), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(10, 0), (9, 1)]
inputting microbatch 10 into partition 0
before moving to cuda:0: tensor([[-0.0906, -1.0878,  0.1841]], device='cuda:0')
after: tensor([[-0.0906, -1.0878,  0.1841]], device='cuda:0')
********************
observing microbatch 10
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.0906, -1.0878,  0.1841]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 10
********************
inputting microbatch 9 into partition 1
before moving to cuda:1: tensor([[0.7022, 0.5444, 0.2028, 0.3158]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 9
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 9
********************
receiving microbatch 10 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 10, tensor([[0.5102, 0.3379, 0.5624, 0.5238]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 10
result is tensor([[0.5102, 0.3379, 0.5624, 0.5238]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5102, 0.3379, 0.5624, 0.5238]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 9 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 9, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 9
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5102, 0.3379, 0.5624, 0.5238]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-1.9266,  0.6633, -0.1954]], device='cuda:0'), tensor([[-0.2679, -1.1845,  0.5339]], device='cuda:0'), tensor([[-0.0554, -1.8176,  0.6832]], device='cuda:0'), tensor([[ 0.8967,  0.3229, -0.7313]], device='cuda:0'), tensor([[-0.6632,  0.4736, -0.3262]], device='cuda:0'), tensor([[-0.3157,  0.1757, -1.8963]], device='cuda:0'), tensor([[ 0.4078, -0.0678,  1.2859]], device='cuda:0'), tensor([[ 0.8052, -1.1757, -0.9411]], device='cuda:0'), tensor([[ 0.5656,  0.2175, -0.2839]], device='cuda:0'), tensor([[-0.7292, -0.2653, -2.7252]], device='cuda:0'), tensor([[-0.5452,  0.2312,  0.3703]], device='cuda:0'), tensor([[ 0.4026,  0.9891, -0.9871]], device='cuda:0'), tensor([[-0.1987,  1.7354,  0.7749]], device='cuda:0'), tensor([[-1.0050, -0.6191, -0.0147]], device='cuda:0'), tensor([[-0.0749,  1.0228, -0.6044]], device='cuda:0'), tensor([[-0.8288, -1.9254, -0.2606]], device='cuda:0'), tensor([[-0.3670, -1.2722,  1.2708]], device='cuda:0'), tensor([[ 0.8890,  0.8967, -0.6183]], device='cuda:0'), tensor([[0.3361, 0.0314, 0.6872]], device='cuda:0'), tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0'), tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0'), tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0'), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(11, 0), (10, 1)]
inputting microbatch 11 into partition 0
before moving to cuda:0: tensor([[-1.9266,  0.6633, -0.1954]], device='cuda:0')
after: tensor([[-1.9266,  0.6633, -0.1954]], device='cuda:0')
********************
observing microbatch 11
current batch shape is torch.Size([1, 3])
batch is tensor([[-1.9266,  0.6633, -0.1954]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 11
********************
inputting microbatch 10 into partition 1
before moving to cuda:1: tensor([[0.5102, 0.3379, 0.5624, 0.5238]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 10
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 10
********************
receiving microbatch 11 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 11, tensor([[0.6412, 0.3406, 0.5458, 0.3431]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 11
result is tensor([[0.6412, 0.3406, 0.5458, 0.3431]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6412, 0.3406, 0.5458, 0.3431]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 10 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 10, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 10
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6412, 0.3406, 0.5458, 0.3431]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.2679, -1.1845,  0.5339]], device='cuda:0'), tensor([[-0.0554, -1.8176,  0.6832]], device='cuda:0'), tensor([[ 0.8967,  0.3229, -0.7313]], device='cuda:0'), tensor([[-0.6632,  0.4736, -0.3262]], device='cuda:0'), tensor([[-0.3157,  0.1757, -1.8963]], device='cuda:0'), tensor([[ 0.4078, -0.0678,  1.2859]], device='cuda:0'), tensor([[ 0.8052, -1.1757, -0.9411]], device='cuda:0'), tensor([[ 0.5656,  0.2175, -0.2839]], device='cuda:0'), tensor([[-0.7292, -0.2653, -2.7252]], device='cuda:0'), tensor([[-0.5452,  0.2312,  0.3703]], device='cuda:0'), tensor([[ 0.4026,  0.9891, -0.9871]], device='cuda:0'), tensor([[-0.1987,  1.7354,  0.7749]], device='cuda:0'), tensor([[-1.0050, -0.6191, -0.0147]], device='cuda:0'), tensor([[-0.0749,  1.0228, -0.6044]], device='cuda:0'), tensor([[-0.8288, -1.9254, -0.2606]], device='cuda:0'), tensor([[-0.3670, -1.2722,  1.2708]], device='cuda:0'), tensor([[ 0.8890,  0.8967, -0.6183]], device='cuda:0'), tensor([[0.3361, 0.0314, 0.6872]], device='cuda:0'), tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0'), tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0'), tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0'), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(12, 0), (11, 1)]
inputting microbatch 12 into partition 0
before moving to cuda:0: tensor([[-0.2679, -1.1845,  0.5339]], device='cuda:0')
after: tensor([[-0.2679, -1.1845,  0.5339]], device='cuda:0')
********************
observing microbatch 12
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.2679, -1.1845,  0.5339]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 12
********************
inputting microbatch 11 into partition 1
before moving to cuda:1: tensor([[0.6412, 0.3406, 0.5458, 0.3431]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 11
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 11
********************
receiving microbatch 12 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 12, tensor([[0.5436, 0.3368, 0.6360, 0.4955]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 12
result is tensor([[0.5436, 0.3368, 0.6360, 0.4955]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5436, 0.3368, 0.6360, 0.4955]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 11 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 11, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 11
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5436, 0.3368, 0.6360, 0.4955]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.0554, -1.8176,  0.6832]], device='cuda:0'), tensor([[ 0.8967,  0.3229, -0.7313]], device='cuda:0'), tensor([[-0.6632,  0.4736, -0.3262]], device='cuda:0'), tensor([[-0.3157,  0.1757, -1.8963]], device='cuda:0'), tensor([[ 0.4078, -0.0678,  1.2859]], device='cuda:0'), tensor([[ 0.8052, -1.1757, -0.9411]], device='cuda:0'), tensor([[ 0.5656,  0.2175, -0.2839]], device='cuda:0'), tensor([[-0.7292, -0.2653, -2.7252]], device='cuda:0'), tensor([[-0.5452,  0.2312,  0.3703]], device='cuda:0'), tensor([[ 0.4026,  0.9891, -0.9871]], device='cuda:0'), tensor([[-0.1987,  1.7354,  0.7749]], device='cuda:0'), tensor([[-1.0050, -0.6191, -0.0147]], device='cuda:0'), tensor([[-0.0749,  1.0228, -0.6044]], device='cuda:0'), tensor([[-0.8288, -1.9254, -0.2606]], device='cuda:0'), tensor([[-0.3670, -1.2722,  1.2708]], device='cuda:0'), tensor([[ 0.8890,  0.8967, -0.6183]], device='cuda:0'), tensor([[0.3361, 0.0314, 0.6872]], device='cuda:0'), tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0'), tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0'), tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0'), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(13, 0), (12, 1)]
inputting microbatch 13 into partition 0
before moving to cuda:0: tensor([[-0.0554, -1.8176,  0.6832]], device='cuda:0')
after: tensor([[-0.0554, -1.8176,  0.6832]], device='cuda:0')
********************
observing microbatch 13
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.0554, -1.8176,  0.6832]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 13
********************
inputting microbatch 12 into partition 1
before moving to cuda:1: tensor([[0.5436, 0.3368, 0.6360, 0.4955]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 12
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 12
********************
receiving microbatch 13 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 13, tensor([[0.4863, 0.3071, 0.6939, 0.5638]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 13
result is tensor([[0.4863, 0.3071, 0.6939, 0.5638]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4863, 0.3071, 0.6939, 0.5638]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 12 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 12, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 12
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4863, 0.3071, 0.6939, 0.5638]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.8967,  0.3229, -0.7313]], device='cuda:0'), tensor([[-0.6632,  0.4736, -0.3262]], device='cuda:0'), tensor([[-0.3157,  0.1757, -1.8963]], device='cuda:0'), tensor([[ 0.4078, -0.0678,  1.2859]], device='cuda:0'), tensor([[ 0.8052, -1.1757, -0.9411]], device='cuda:0'), tensor([[ 0.5656,  0.2175, -0.2839]], device='cuda:0'), tensor([[-0.7292, -0.2653, -2.7252]], device='cuda:0'), tensor([[-0.5452,  0.2312,  0.3703]], device='cuda:0'), tensor([[ 0.4026,  0.9891, -0.9871]], device='cuda:0'), tensor([[-0.1987,  1.7354,  0.7749]], device='cuda:0'), tensor([[-1.0050, -0.6191, -0.0147]], device='cuda:0'), tensor([[-0.0749,  1.0228, -0.6044]], device='cuda:0'), tensor([[-0.8288, -1.9254, -0.2606]], device='cuda:0'), tensor([[-0.3670, -1.2722,  1.2708]], device='cuda:0'), tensor([[ 0.8890,  0.8967, -0.6183]], device='cuda:0'), tensor([[0.3361, 0.0314, 0.6872]], device='cuda:0'), tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0'), tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0'), tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0'), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(14, 0), (13, 1)]
inputting microbatch 14 into partition 0
before moving to cuda:0: tensor([[ 0.8967,  0.3229, -0.7313]], device='cuda:0')
after: tensor([[ 0.8967,  0.3229, -0.7313]], device='cuda:0')
********************
observing microbatch 14
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.8967,  0.3229, -0.7313]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 14
********************
inputting microbatch 13 into partition 1
before moving to cuda:1: tensor([[0.4863, 0.3071, 0.6939, 0.5638]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 13
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 13
********************
receiving microbatch 14 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 14, tensor([[0.5871, 0.4817, 0.2044, 0.4336]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 14
result is tensor([[0.5871, 0.4817, 0.2044, 0.4336]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5871, 0.4817, 0.2044, 0.4336]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 13 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 13, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 13
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5871, 0.4817, 0.2044, 0.4336]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.6632,  0.4736, -0.3262]], device='cuda:0'), tensor([[-0.3157,  0.1757, -1.8963]], device='cuda:0'), tensor([[ 0.4078, -0.0678,  1.2859]], device='cuda:0'), tensor([[ 0.8052, -1.1757, -0.9411]], device='cuda:0'), tensor([[ 0.5656,  0.2175, -0.2839]], device='cuda:0'), tensor([[-0.7292, -0.2653, -2.7252]], device='cuda:0'), tensor([[-0.5452,  0.2312,  0.3703]], device='cuda:0'), tensor([[ 0.4026,  0.9891, -0.9871]], device='cuda:0'), tensor([[-0.1987,  1.7354,  0.7749]], device='cuda:0'), tensor([[-1.0050, -0.6191, -0.0147]], device='cuda:0'), tensor([[-0.0749,  1.0228, -0.6044]], device='cuda:0'), tensor([[-0.8288, -1.9254, -0.2606]], device='cuda:0'), tensor([[-0.3670, -1.2722,  1.2708]], device='cuda:0'), tensor([[ 0.8890,  0.8967, -0.6183]], device='cuda:0'), tensor([[0.3361, 0.0314, 0.6872]], device='cuda:0'), tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0'), tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0'), tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0'), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(15, 0), (14, 1)]
inputting microbatch 15 into partition 0
before moving to cuda:0: tensor([[-0.6632,  0.4736, -0.3262]], device='cuda:0')
after: tensor([[-0.6632,  0.4736, -0.3262]], device='cuda:0')
********************
observing microbatch 15
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.6632,  0.4736, -0.3262]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 15
********************
inputting microbatch 14 into partition 1
before moving to cuda:1: tensor([[0.5871, 0.4817, 0.2044, 0.4336]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 14
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 14
********************
receiving microbatch 15 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 15, tensor([[0.6274, 0.4048, 0.3928, 0.3750]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 15
result is tensor([[0.6274, 0.4048, 0.3928, 0.3750]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6274, 0.4048, 0.3928, 0.3750]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 14 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 14, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 14
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6274, 0.4048, 0.3928, 0.3750]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.3157,  0.1757, -1.8963]], device='cuda:0'), tensor([[ 0.4078, -0.0678,  1.2859]], device='cuda:0'), tensor([[ 0.8052, -1.1757, -0.9411]], device='cuda:0'), tensor([[ 0.5656,  0.2175, -0.2839]], device='cuda:0'), tensor([[-0.7292, -0.2653, -2.7252]], device='cuda:0'), tensor([[-0.5452,  0.2312,  0.3703]], device='cuda:0'), tensor([[ 0.4026,  0.9891, -0.9871]], device='cuda:0'), tensor([[-0.1987,  1.7354,  0.7749]], device='cuda:0'), tensor([[-1.0050, -0.6191, -0.0147]], device='cuda:0'), tensor([[-0.0749,  1.0228, -0.6044]], device='cuda:0'), tensor([[-0.8288, -1.9254, -0.2606]], device='cuda:0'), tensor([[-0.3670, -1.2722,  1.2708]], device='cuda:0'), tensor([[ 0.8890,  0.8967, -0.6183]], device='cuda:0'), tensor([[0.3361, 0.0314, 0.6872]], device='cuda:0'), tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0'), tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0'), tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0'), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(16, 0), (15, 1)]
inputting microbatch 16 into partition 0
before moving to cuda:0: tensor([[-0.3157,  0.1757, -1.8963]], device='cuda:0')
after: tensor([[-0.3157,  0.1757, -1.8963]], device='cuda:0')
********************
observing microbatch 16
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.3157,  0.1757, -1.8963]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 16
********************
inputting microbatch 15 into partition 1
before moving to cuda:1: tensor([[0.6274, 0.4048, 0.3928, 0.3750]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 15
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 15
********************
receiving microbatch 16 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 16, tensor([[0.3755, 0.3167, 0.2238, 0.6025]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 16
result is tensor([[0.3755, 0.3167, 0.2238, 0.6025]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3755, 0.3167, 0.2238, 0.6025]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 15 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 15, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 15
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3755, 0.3167, 0.2238, 0.6025]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.4078, -0.0678,  1.2859]], device='cuda:0'), tensor([[ 0.8052, -1.1757, -0.9411]], device='cuda:0'), tensor([[ 0.5656,  0.2175, -0.2839]], device='cuda:0'), tensor([[-0.7292, -0.2653, -2.7252]], device='cuda:0'), tensor([[-0.5452,  0.2312,  0.3703]], device='cuda:0'), tensor([[ 0.4026,  0.9891, -0.9871]], device='cuda:0'), tensor([[-0.1987,  1.7354,  0.7749]], device='cuda:0'), tensor([[-1.0050, -0.6191, -0.0147]], device='cuda:0'), tensor([[-0.0749,  1.0228, -0.6044]], device='cuda:0'), tensor([[-0.8288, -1.9254, -0.2606]], device='cuda:0'), tensor([[-0.3670, -1.2722,  1.2708]], device='cuda:0'), tensor([[ 0.8890,  0.8967, -0.6183]], device='cuda:0'), tensor([[0.3361, 0.0314, 0.6872]], device='cuda:0'), tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0'), tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0'), tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0'), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(17, 0), (16, 1)]
inputting microbatch 17 into partition 0
before moving to cuda:0: tensor([[ 0.4078, -0.0678,  1.2859]], device='cuda:0')
after: tensor([[ 0.4078, -0.0678,  1.2859]], device='cuda:0')
********************
observing microbatch 17
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.4078, -0.0678,  1.2859]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 17
********************
inputting microbatch 16 into partition 1
before moving to cuda:1: tensor([[0.3755, 0.3167, 0.2238, 0.6025]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 16
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 16
********************
receiving microbatch 17 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 17, tensor([[0.7793, 0.5298, 0.5153, 0.2639]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 17
result is tensor([[0.7793, 0.5298, 0.5153, 0.2639]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.7793, 0.5298, 0.5153, 0.2639]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 16 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 16, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 16
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.7793, 0.5298, 0.5153, 0.2639]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.8052, -1.1757, -0.9411]], device='cuda:0'), tensor([[ 0.5656,  0.2175, -0.2839]], device='cuda:0'), tensor([[-0.7292, -0.2653, -2.7252]], device='cuda:0'), tensor([[-0.5452,  0.2312,  0.3703]], device='cuda:0'), tensor([[ 0.4026,  0.9891, -0.9871]], device='cuda:0'), tensor([[-0.1987,  1.7354,  0.7749]], device='cuda:0'), tensor([[-1.0050, -0.6191, -0.0147]], device='cuda:0'), tensor([[-0.0749,  1.0228, -0.6044]], device='cuda:0'), tensor([[-0.8288, -1.9254, -0.2606]], device='cuda:0'), tensor([[-0.3670, -1.2722,  1.2708]], device='cuda:0'), tensor([[ 0.8890,  0.8967, -0.6183]], device='cuda:0'), tensor([[0.3361, 0.0314, 0.6872]], device='cuda:0'), tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0'), tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0'), tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0'), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(18, 0), (17, 1)]
inputting microbatch 18 into partition 0
before moving to cuda:0: tensor([[ 0.8052, -1.1757, -0.9411]], device='cuda:0')
after: tensor([[ 0.8052, -1.1757, -0.9411]], device='cuda:0')
********************
observing microbatch 18
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.8052, -1.1757, -0.9411]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 18
********************
inputting microbatch 17 into partition 1
before moving to cuda:1: tensor([[0.7793, 0.5298, 0.5153, 0.2639]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 17
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 17
********************
receiving microbatch 18 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 18, tensor([[0.3610, 0.3301, 0.3282, 0.6622]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 18
result is tensor([[0.3610, 0.3301, 0.3282, 0.6622]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3610, 0.3301, 0.3282, 0.6622]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 17 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 17, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 17
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3610, 0.3301, 0.3282, 0.6622]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.5656,  0.2175, -0.2839]], device='cuda:0'), tensor([[-0.7292, -0.2653, -2.7252]], device='cuda:0'), tensor([[-0.5452,  0.2312,  0.3703]], device='cuda:0'), tensor([[ 0.4026,  0.9891, -0.9871]], device='cuda:0'), tensor([[-0.1987,  1.7354,  0.7749]], device='cuda:0'), tensor([[-1.0050, -0.6191, -0.0147]], device='cuda:0'), tensor([[-0.0749,  1.0228, -0.6044]], device='cuda:0'), tensor([[-0.8288, -1.9254, -0.2606]], device='cuda:0'), tensor([[-0.3670, -1.2722,  1.2708]], device='cuda:0'), tensor([[ 0.8890,  0.8967, -0.6183]], device='cuda:0'), tensor([[0.3361, 0.0314, 0.6872]], device='cuda:0'), tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0'), tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0'), tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0'), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(19, 0), (18, 1)]
inputting microbatch 19 into partition 0
before moving to cuda:0: tensor([[ 0.5656,  0.2175, -0.2839]], device='cuda:0')
after: tensor([[ 0.5656,  0.2175, -0.2839]], device='cuda:0')
********************
observing microbatch 19
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.5656,  0.2175, -0.2839]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 19
********************
inputting microbatch 18 into partition 1
before moving to cuda:1: tensor([[0.3610, 0.3301, 0.3282, 0.6622]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 18
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 18
********************
receiving microbatch 19 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 19, tensor([[0.6279, 0.4740, 0.2846, 0.3966]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 19
result is tensor([[0.6279, 0.4740, 0.2846, 0.3966]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6279, 0.4740, 0.2846, 0.3966]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 18 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 18, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 18
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6279, 0.4740, 0.2846, 0.3966]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.7292, -0.2653, -2.7252]], device='cuda:0'), tensor([[-0.5452,  0.2312,  0.3703]], device='cuda:0'), tensor([[ 0.4026,  0.9891, -0.9871]], device='cuda:0'), tensor([[-0.1987,  1.7354,  0.7749]], device='cuda:0'), tensor([[-1.0050, -0.6191, -0.0147]], device='cuda:0'), tensor([[-0.0749,  1.0228, -0.6044]], device='cuda:0'), tensor([[-0.8288, -1.9254, -0.2606]], device='cuda:0'), tensor([[-0.3670, -1.2722,  1.2708]], device='cuda:0'), tensor([[ 0.8890,  0.8967, -0.6183]], device='cuda:0'), tensor([[0.3361, 0.0314, 0.6872]], device='cuda:0'), tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0'), tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0'), tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0'), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(20, 0), (19, 1)]
inputting microbatch 20 into partition 0
before moving to cuda:0: tensor([[-0.7292, -0.2653, -2.7252]], device='cuda:0')
after: tensor([[-0.7292, -0.2653, -2.7252]], device='cuda:0')
********************
observing microbatch 20
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.7292, -0.2653, -2.7252]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 20
********************
inputting microbatch 19 into partition 1
before moving to cuda:1: tensor([[0.6279, 0.4740, 0.2846, 0.3966]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 19
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 19
********************
receiving microbatch 20 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 20, tensor([[0.2214, 0.2218, 0.2256, 0.7461]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 20
result is tensor([[0.2214, 0.2218, 0.2256, 0.7461]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.2214, 0.2218, 0.2256, 0.7461]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 19 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 19, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 19
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.2214, 0.2218, 0.2256, 0.7461]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.5452,  0.2312,  0.3703]], device='cuda:0'), tensor([[ 0.4026,  0.9891, -0.9871]], device='cuda:0'), tensor([[-0.1987,  1.7354,  0.7749]], device='cuda:0'), tensor([[-1.0050, -0.6191, -0.0147]], device='cuda:0'), tensor([[-0.0749,  1.0228, -0.6044]], device='cuda:0'), tensor([[-0.8288, -1.9254, -0.2606]], device='cuda:0'), tensor([[-0.3670, -1.2722,  1.2708]], device='cuda:0'), tensor([[ 0.8890,  0.8967, -0.6183]], device='cuda:0'), tensor([[0.3361, 0.0314, 0.6872]], device='cuda:0'), tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0'), tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0'), tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0'), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(21, 0), (20, 1)]
inputting microbatch 21 into partition 0
before moving to cuda:0: tensor([[-0.5452,  0.2312,  0.3703]], device='cuda:0')
after: tensor([[-0.5452,  0.2312,  0.3703]], device='cuda:0')
********************
observing microbatch 21
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.5452,  0.2312,  0.3703]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 21
********************
inputting microbatch 20 into partition 1
before moving to cuda:1: tensor([[0.2214, 0.2218, 0.2256, 0.7461]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 20
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 20
********************
receiving microbatch 21 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 21, tensor([[0.6910, 0.4321, 0.4897, 0.3274]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 21
result is tensor([[0.6910, 0.4321, 0.4897, 0.3274]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6910, 0.4321, 0.4897, 0.3274]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 20 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 20, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 20
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6910, 0.4321, 0.4897, 0.3274]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.4026,  0.9891, -0.9871]], device='cuda:0'), tensor([[-0.1987,  1.7354,  0.7749]], device='cuda:0'), tensor([[-1.0050, -0.6191, -0.0147]], device='cuda:0'), tensor([[-0.0749,  1.0228, -0.6044]], device='cuda:0'), tensor([[-0.8288, -1.9254, -0.2606]], device='cuda:0'), tensor([[-0.3670, -1.2722,  1.2708]], device='cuda:0'), tensor([[ 0.8890,  0.8967, -0.6183]], device='cuda:0'), tensor([[0.3361, 0.0314, 0.6872]], device='cuda:0'), tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0'), tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0'), tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0'), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(22, 0), (21, 1)]
inputting microbatch 22 into partition 0
before moving to cuda:0: tensor([[ 0.4026,  0.9891, -0.9871]], device='cuda:0')
after: tensor([[ 0.4026,  0.9891, -0.9871]], device='cuda:0')
********************
observing microbatch 22
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.4026,  0.9891, -0.9871]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 22
********************
inputting microbatch 21 into partition 1
before moving to cuda:1: tensor([[0.6910, 0.4321, 0.4897, 0.3274]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 21
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 21
********************
receiving microbatch 22 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 22, tensor([[0.6252, 0.4914, 0.1760, 0.3773]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 22
result is tensor([[0.6252, 0.4914, 0.1760, 0.3773]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6252, 0.4914, 0.1760, 0.3773]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 21 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 21, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 21
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6252, 0.4914, 0.1760, 0.3773]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.1987,  1.7354,  0.7749]], device='cuda:0'), tensor([[-1.0050, -0.6191, -0.0147]], device='cuda:0'), tensor([[-0.0749,  1.0228, -0.6044]], device='cuda:0'), tensor([[-0.8288, -1.9254, -0.2606]], device='cuda:0'), tensor([[-0.3670, -1.2722,  1.2708]], device='cuda:0'), tensor([[ 0.8890,  0.8967, -0.6183]], device='cuda:0'), tensor([[0.3361, 0.0314, 0.6872]], device='cuda:0'), tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0'), tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0'), tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0'), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(23, 0), (22, 1)]
inputting microbatch 23 into partition 0
before moving to cuda:0: tensor([[-0.1987,  1.7354,  0.7749]], device='cuda:0')
after: tensor([[-0.1987,  1.7354,  0.7749]], device='cuda:0')
********************
observing microbatch 23
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.1987,  1.7354,  0.7749]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 23
********************
inputting microbatch 22 into partition 1
before moving to cuda:1: tensor([[0.6252, 0.4914, 0.1760, 0.3773]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 22
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 22
********************
receiving microbatch 23 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 23, tensor([[0.8660, 0.6192, 0.3270, 0.1461]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 23
result is tensor([[0.8660, 0.6192, 0.3270, 0.1461]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.8660, 0.6192, 0.3270, 0.1461]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 22 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 22, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 22
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8660, 0.6192, 0.3270, 0.1461]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-1.0050, -0.6191, -0.0147]], device='cuda:0'), tensor([[-0.0749,  1.0228, -0.6044]], device='cuda:0'), tensor([[-0.8288, -1.9254, -0.2606]], device='cuda:0'), tensor([[-0.3670, -1.2722,  1.2708]], device='cuda:0'), tensor([[ 0.8890,  0.8967, -0.6183]], device='cuda:0'), tensor([[0.3361, 0.0314, 0.6872]], device='cuda:0'), tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0'), tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0'), tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0'), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(24, 0), (23, 1)]
inputting microbatch 24 into partition 0
before moving to cuda:0: tensor([[-1.0050, -0.6191, -0.0147]], device='cuda:0')
after: tensor([[-1.0050, -0.6191, -0.0147]], device='cuda:0')
********************
observing microbatch 24
current batch shape is torch.Size([1, 3])
batch is tensor([[-1.0050, -0.6191, -0.0147]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 24
********************
inputting microbatch 23 into partition 1
before moving to cuda:1: tensor([[0.8660, 0.6192, 0.3270, 0.1461]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 23
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 23
********************
receiving microbatch 24 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 24, tensor([[0.5226, 0.3060, 0.5989, 0.4885]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 24
result is tensor([[0.5226, 0.3060, 0.5989, 0.4885]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5226, 0.3060, 0.5989, 0.4885]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 23 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 23, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 23
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5226, 0.3060, 0.5989, 0.4885]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.0749,  1.0228, -0.6044]], device='cuda:0'), tensor([[-0.8288, -1.9254, -0.2606]], device='cuda:0'), tensor([[-0.3670, -1.2722,  1.2708]], device='cuda:0'), tensor([[ 0.8890,  0.8967, -0.6183]], device='cuda:0'), tensor([[0.3361, 0.0314, 0.6872]], device='cuda:0'), tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0'), tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0'), tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0'), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(25, 0), (24, 1)]
inputting microbatch 25 into partition 0
before moving to cuda:0: tensor([[-0.0749,  1.0228, -0.6044]], device='cuda:0')
after: tensor([[-0.0749,  1.0228, -0.6044]], device='cuda:0')
********************
observing microbatch 25
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.0749,  1.0228, -0.6044]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 25
********************
inputting microbatch 24 into partition 1
before moving to cuda:1: tensor([[0.5226, 0.3060, 0.5989, 0.4885]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 24
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 24
********************
receiving microbatch 25 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 25, tensor([[0.6695, 0.4819, 0.2445, 0.3328]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 25
result is tensor([[0.6695, 0.4819, 0.2445, 0.3328]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6695, 0.4819, 0.2445, 0.3328]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 24 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 24, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 24
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6695, 0.4819, 0.2445, 0.3328]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.8288, -1.9254, -0.2606]], device='cuda:0'), tensor([[-0.3670, -1.2722,  1.2708]], device='cuda:0'), tensor([[ 0.8890,  0.8967, -0.6183]], device='cuda:0'), tensor([[0.3361, 0.0314, 0.6872]], device='cuda:0'), tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0'), tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0'), tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0'), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(26, 0), (25, 1)]
inputting microbatch 26 into partition 0
before moving to cuda:0: tensor([[-0.8288, -1.9254, -0.2606]], device='cuda:0')
after: tensor([[-0.8288, -1.9254, -0.2606]], device='cuda:0')
********************
observing microbatch 26
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.8288, -1.9254, -0.2606]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 26
********************
inputting microbatch 25 into partition 1
before moving to cuda:1: tensor([[0.6695, 0.4819, 0.2445, 0.3328]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 25
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 25
********************
receiving microbatch 26 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 26, tensor([[0.3259, 0.2124, 0.6900, 0.6896]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 26
result is tensor([[0.3259, 0.2124, 0.6900, 0.6896]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3259, 0.2124, 0.6900, 0.6896]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 25 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 25, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 25
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3259, 0.2124, 0.6900, 0.6896]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.3670, -1.2722,  1.2708]], device='cuda:0'), tensor([[ 0.8890,  0.8967, -0.6183]], device='cuda:0'), tensor([[0.3361, 0.0314, 0.6872]], device='cuda:0'), tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0'), tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0'), tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0'), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(27, 0), (26, 1)]
inputting microbatch 27 into partition 0
before moving to cuda:0: tensor([[-0.3670, -1.2722,  1.2708]], device='cuda:0')
after: tensor([[-0.3670, -1.2722,  1.2708]], device='cuda:0')
********************
observing microbatch 27
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.3670, -1.2722,  1.2708]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 27
********************
inputting microbatch 26 into partition 1
before moving to cuda:1: tensor([[0.3259, 0.2124, 0.6900, 0.6896]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 26
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 26
********************
receiving microbatch 27 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 27, tensor([[0.6327, 0.3629, 0.7332, 0.4188]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 27
result is tensor([[0.6327, 0.3629, 0.7332, 0.4188]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6327, 0.3629, 0.7332, 0.4188]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 26 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 26, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 26
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6327, 0.3629, 0.7332, 0.4188]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.8890,  0.8967, -0.6183]], device='cuda:0'), tensor([[0.3361, 0.0314, 0.6872]], device='cuda:0'), tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0'), tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0'), tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0'), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(28, 0), (27, 1)]
inputting microbatch 28 into partition 0
before moving to cuda:0: tensor([[ 0.8890,  0.8967, -0.6183]], device='cuda:0')
after: tensor([[ 0.8890,  0.8967, -0.6183]], device='cuda:0')
********************
observing microbatch 28
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.8890,  0.8967, -0.6183]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 28
********************
inputting microbatch 27 into partition 1
before moving to cuda:1: tensor([[0.6327, 0.3629, 0.7332, 0.4188]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 27
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 27
********************
receiving microbatch 28 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 28, tensor([[0.6727, 0.5411, 0.1726, 0.3447]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 28
result is tensor([[0.6727, 0.5411, 0.1726, 0.3447]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6727, 0.5411, 0.1726, 0.3447]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 27 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 27, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 27
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6727, 0.5411, 0.1726, 0.3447]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[0.3361, 0.0314, 0.6872]], device='cuda:0'), tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0'), tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0'), tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0'), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(29, 0), (28, 1)]
inputting microbatch 29 into partition 0
before moving to cuda:0: tensor([[0.3361, 0.0314, 0.6872]], device='cuda:0')
after: tensor([[0.3361, 0.0314, 0.6872]], device='cuda:0')
********************
observing microbatch 29
current batch shape is torch.Size([1, 3])
batch is tensor([[0.3361, 0.0314, 0.6872]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 29
********************
inputting microbatch 28 into partition 1
before moving to cuda:1: tensor([[0.6727, 0.5411, 0.1726, 0.3447]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 28
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 28
********************
receiving microbatch 29 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 29, tensor([[0.7237, 0.4979, 0.4398, 0.3131]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 29
result is tensor([[0.7237, 0.4979, 0.4398, 0.3131]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.7237, 0.4979, 0.4398, 0.3131]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 28 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 28, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 28
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.7237, 0.4979, 0.4398, 0.3131]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0'), tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0'), tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0'), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(30, 0), (29, 1)]
inputting microbatch 30 into partition 0
before moving to cuda:0: tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0')
after: tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0')
********************
observing microbatch 30
current batch shape is torch.Size([1, 3])
batch is tensor([[ 2.0157,  0.5908, -0.2263]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 30
********************
inputting microbatch 29 into partition 1
before moving to cuda:1: tensor([[0.7237, 0.4979, 0.4398, 0.3131]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 29
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 29
********************
receiving microbatch 30 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 30, tensor([[0.7084, 0.6180, 0.1410, 0.3336]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 30
result is tensor([[0.7084, 0.6180, 0.1410, 0.3336]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.7084, 0.6180, 0.1410, 0.3336]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 29 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 29, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 29
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.7084, 0.6180, 0.1410, 0.3336]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0'), tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0'), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(31, 0), (30, 1)]
inputting microbatch 31 into partition 0
before moving to cuda:0: tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0')
after: tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0')
********************
observing microbatch 31
current batch shape is torch.Size([1, 3])
batch is tensor([[-1.5400,  1.0130,  0.1117]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 31
********************
inputting microbatch 30 into partition 1
before moving to cuda:1: tensor([[0.7084, 0.6180, 0.1410, 0.3336]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 30
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 30
********************
receiving microbatch 31 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 31, tensor([[0.7268, 0.4155, 0.4935, 0.2681]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 31
result is tensor([[0.7268, 0.4155, 0.4935, 0.2681]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.7268, 0.4155, 0.4935, 0.2681]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 30 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 30, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 30
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.7268, 0.4155, 0.4935, 0.2681]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0'), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(32, 0), (31, 1)]
inputting microbatch 32 into partition 0
before moving to cuda:0: tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0')
after: tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0')
********************
observing microbatch 32
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.1112, -0.1205,  1.3927]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 32
********************
inputting microbatch 31 into partition 1
before moving to cuda:1: tensor([[0.7268, 0.4155, 0.4935, 0.2681]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 31
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 31
********************
receiving microbatch 32 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 32, tensor([[0.7804, 0.5093, 0.5718, 0.2609]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 32
result is tensor([[0.7804, 0.5093, 0.5718, 0.2609]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.7804, 0.5093, 0.5718, 0.2609]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 31 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 31, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 31
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.7804, 0.5093, 0.5718, 0.2609]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0'), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(33, 0), (32, 1)]
inputting microbatch 33 into partition 0
before moving to cuda:0: tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0')
after: tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0')
********************
observing microbatch 33
current batch shape is torch.Size([1, 3])
batch is tensor([[0.0411, 1.2360, 0.4194]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 33
********************
inputting microbatch 32 into partition 1
before moving to cuda:1: tensor([[0.7804, 0.5093, 0.5718, 0.2609]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 32
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 32
********************
receiving microbatch 33 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 33, tensor([[0.8052, 0.5712, 0.3132, 0.2110]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 33
result is tensor([[0.8052, 0.5712, 0.3132, 0.2110]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.8052, 0.5712, 0.3132, 0.2110]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 32 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 32, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 32
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8052, 0.5712, 0.3132, 0.2110]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0'), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(34, 0), (33, 1)]
inputting microbatch 34 into partition 0
before moving to cuda:0: tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0')
after: tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0')
********************
observing microbatch 34
current batch shape is torch.Size([1, 3])
batch is tensor([[-1.3570,  0.8632,  0.8395]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 34
********************
inputting microbatch 33 into partition 1
before moving to cuda:1: tensor([[0.8052, 0.5712, 0.3132, 0.2110]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 33
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 33
********************
receiving microbatch 34 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 34, tensor([[0.7917, 0.4581, 0.5763, 0.2172]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 34
result is tensor([[0.7917, 0.4581, 0.5763, 0.2172]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.7917, 0.4581, 0.5763, 0.2172]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 33 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 33, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 33
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.7917, 0.4581, 0.5763, 0.2172]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0'), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(35, 0), (34, 1)]
inputting microbatch 35 into partition 0
before moving to cuda:0: tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0')
after: tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0')
********************
observing microbatch 35
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.2962, -0.4698, -0.5158]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 35
********************
inputting microbatch 34 into partition 1
before moving to cuda:1: tensor([[0.7917, 0.4581, 0.5763, 0.2172]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 34
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 34
********************
receiving microbatch 35 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 35, tensor([[0.5002, 0.3786, 0.3595, 0.5210]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 35
result is tensor([[0.5002, 0.3786, 0.3595, 0.5210]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5002, 0.3786, 0.3595, 0.5210]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 34 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 34, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 34
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5002, 0.3786, 0.3595, 0.5210]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0'), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(36, 0), (35, 1)]
inputting microbatch 36 into partition 0
before moving to cuda:0: tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0')
after: tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0')
********************
observing microbatch 36
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.0843, -1.1506,  0.5055]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 36
********************
inputting microbatch 35 into partition 1
before moving to cuda:1: tensor([[0.5002, 0.3786, 0.3595, 0.5210]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 35
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 35
********************
receiving microbatch 36 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 36, tensor([[0.5520, 0.3620, 0.5863, 0.4921]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 36
result is tensor([[0.5520, 0.3620, 0.5863, 0.4921]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5520, 0.3620, 0.5863, 0.4921]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 35 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 35, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 35
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5520, 0.3620, 0.5863, 0.4921]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0'), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(37, 0), (36, 1)]
inputting microbatch 37 into partition 0
before moving to cuda:0: tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0')
after: tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0')
********************
observing microbatch 37
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.0525, -1.0103, -0.2487]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 37
********************
inputting microbatch 36 into partition 1
before moving to cuda:1: tensor([[0.5520, 0.3620, 0.5863, 0.4921]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 36
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 36
********************
receiving microbatch 37 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 37, tensor([[0.4616, 0.3307, 0.4825, 0.5647]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 37
result is tensor([[0.4616, 0.3307, 0.4825, 0.5647]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4616, 0.3307, 0.4825, 0.5647]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 36 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 36, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 36
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4616, 0.3307, 0.4825, 0.5647]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0'), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(38, 0), (37, 1)]
inputting microbatch 38 into partition 0
before moving to cuda:0: tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0')
after: tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0')
********************
observing microbatch 38
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.4397,  0.6993,  0.1047]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 38
********************
inputting microbatch 37 into partition 1
before moving to cuda:1: tensor([[0.4616, 0.3307, 0.4825, 0.5647]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 37
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 37
********************
receiving microbatch 38 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 38, tensor([[0.7129, 0.4672, 0.3908, 0.2987]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 38
result is tensor([[0.7129, 0.4672, 0.3908, 0.2987]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.7129, 0.4672, 0.3908, 0.2987]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 37 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 37, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 37
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.7129, 0.4672, 0.3908, 0.2987]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0'), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(39, 0), (38, 1)]
inputting microbatch 39 into partition 0
before moving to cuda:0: tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0')
after: tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0')
********************
observing microbatch 39
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.4451,  0.2846, -0.9714]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 39
********************
inputting microbatch 38 into partition 1
before moving to cuda:1: tensor([[0.7129, 0.4672, 0.3908, 0.2987]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 38
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 38
********************
receiving microbatch 39 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 39, tensor([[0.5379, 0.4305, 0.2264, 0.4708]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 39
result is tensor([[0.5379, 0.4305, 0.2264, 0.4708]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5379, 0.4305, 0.2264, 0.4708]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 38 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 38, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 38
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5379, 0.4305, 0.2264, 0.4708]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0'), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(40, 0), (39, 1)]
inputting microbatch 40 into partition 0
before moving to cuda:0: tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0')
after: tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0')
********************
observing microbatch 40
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.7086, -0.8998,  0.1943]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 40
********************
inputting microbatch 39 into partition 1
before moving to cuda:1: tensor([[0.5379, 0.4305, 0.2264, 0.4708]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 39
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 39
********************
receiving microbatch 40 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 40, tensor([[0.5223, 0.3133, 0.6186, 0.5002]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 40
result is tensor([[0.5223, 0.3133, 0.6186, 0.5002]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5223, 0.3133, 0.6186, 0.5002]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 39 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 39, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 39
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5223, 0.3133, 0.6186, 0.5002]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0'), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(41, 0), (40, 1)]
inputting microbatch 41 into partition 0
before moving to cuda:0: tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0')
after: tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0')
********************
observing microbatch 41
current batch shape is torch.Size([1, 3])
batch is tensor([[-1.0730,  1.3689,  1.9224]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 41
********************
inputting microbatch 40 into partition 1
before moving to cuda:1: tensor([[0.5223, 0.3133, 0.6186, 0.5002]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 40
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 40
********************
receiving microbatch 41 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 41, tensor([[0.9049, 0.5901, 0.6124, 0.1089]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 41
result is tensor([[0.9049, 0.5901, 0.6124, 0.1089]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9049, 0.5901, 0.6124, 0.1089]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 40 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 40, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 40
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9049, 0.5901, 0.6124, 0.1089]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0'), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(42, 0), (41, 1)]
inputting microbatch 42 into partition 0
before moving to cuda:0: tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0')
after: tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0')
********************
observing microbatch 42
current batch shape is torch.Size([1, 3])
batch is tensor([[-1.6565,  0.7724, -1.1296]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 42
********************
inputting microbatch 41 into partition 1
before moving to cuda:1: tensor([[0.9049, 0.5901, 0.6124, 0.1089]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 41
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 41
********************
receiving microbatch 42 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 42, tensor([[0.5313, 0.3179, 0.3863, 0.4356]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 42
result is tensor([[0.5313, 0.3179, 0.3863, 0.4356]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5313, 0.3179, 0.3863, 0.4356]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 41 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 41, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 41
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5313, 0.3179, 0.3863, 0.4356]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0'), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(43, 0), (42, 1)]
inputting microbatch 43 into partition 0
before moving to cuda:0: tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0')
after: tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0')
********************
observing microbatch 43
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.7143, -0.1766,  1.4326]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 43
********************
inputting microbatch 42 into partition 1
before moving to cuda:1: tensor([[0.5313, 0.3179, 0.3863, 0.4356]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 42
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 42
********************
receiving microbatch 43 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 43, tensor([[0.7886, 0.5512, 0.5069, 0.2603]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 43
result is tensor([[0.7886, 0.5512, 0.5069, 0.2603]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.7886, 0.5512, 0.5069, 0.2603]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 42 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 42, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 42
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.7886, 0.5512, 0.5069, 0.2603]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0'), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(44, 0), (43, 1)]
inputting microbatch 44 into partition 0
before moving to cuda:0: tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0')
after: tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0')
********************
observing microbatch 44
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.9175,  0.7575, -0.4561]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 44
********************
inputting microbatch 43 into partition 1
before moving to cuda:1: tensor([[0.7886, 0.5512, 0.5069, 0.2603]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 43
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 43
********************
receiving microbatch 44 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 44, tensor([[0.6775, 0.5400, 0.1919, 0.3441]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 44
result is tensor([[0.6775, 0.5400, 0.1919, 0.3441]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6775, 0.5400, 0.1919, 0.3441]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 43 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 43, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 43
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6775, 0.5400, 0.1919, 0.3441]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0'), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(45, 0), (44, 1)]
inputting microbatch 45 into partition 0
before moving to cuda:0: tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0')
after: tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0')
********************
observing microbatch 45
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.2692,  0.3594, -0.0902]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 45
********************
inputting microbatch 44 into partition 1
before moving to cuda:1: tensor([[0.6775, 0.5400, 0.1919, 0.3441]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 44
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 44
********************
receiving microbatch 45 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 45, tensor([[0.6532, 0.4369, 0.3852, 0.3606]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 45
result is tensor([[0.6532, 0.4369, 0.3852, 0.3606]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6532, 0.4369, 0.3852, 0.3606]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 44 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 44, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 44
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6532, 0.4369, 0.3852, 0.3606]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0'), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(46, 0), (45, 1)]
inputting microbatch 46 into partition 0
before moving to cuda:0: tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0')
after: tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0')
********************
observing microbatch 46
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.2273, -0.2573,  1.4403]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 46
********************
inputting microbatch 45 into partition 1
before moving to cuda:1: tensor([[0.6532, 0.4369, 0.3852, 0.3606]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 45
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 45
********************
receiving microbatch 46 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 46, tensor([[0.7744, 0.5081, 0.5787, 0.2707]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 46
result is tensor([[0.7744, 0.5081, 0.5787, 0.2707]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.7744, 0.5081, 0.5787, 0.2707]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 45 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 45, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 45
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.7744, 0.5081, 0.5787, 0.2707]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0'), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(47, 0), (46, 1)]
inputting microbatch 47 into partition 0
before moving to cuda:0: tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0')
after: tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0')
********************
observing microbatch 47
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.6452, -0.9126,  0.3324]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 47
********************
inputting microbatch 46 into partition 1
before moving to cuda:1: tensor([[0.7744, 0.5081, 0.5787, 0.2707]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 46
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 46
********************
receiving microbatch 47 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 47, tensor([[0.5419, 0.3236, 0.6283, 0.4846]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 47
result is tensor([[0.5419, 0.3236, 0.6283, 0.4846]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5419, 0.3236, 0.6283, 0.4846]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 46 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 46, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 46
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5419, 0.3236, 0.6283, 0.4846]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0'), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(48, 0), (47, 1)]
inputting microbatch 48 into partition 0
before moving to cuda:0: tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0')
after: tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0')
********************
observing microbatch 48
current batch shape is torch.Size([1, 3])
batch is tensor([[0.9751, 0.5567, 0.5707]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 48
********************
inputting microbatch 47 into partition 1
before moving to cuda:1: tensor([[0.5419, 0.3236, 0.6283, 0.4846]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 47
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 47
********************
receiving microbatch 48 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 48, tensor([[0.7743, 0.5865, 0.2953, 0.2616]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 48
result is tensor([[0.7743, 0.5865, 0.2953, 0.2616]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.7743, 0.5865, 0.2953, 0.2616]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 47 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 47, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 47
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.7743, 0.5865, 0.2953, 0.2616]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0'), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(49, 0), (48, 1)]
inputting microbatch 49 into partition 0
before moving to cuda:0: tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0')
after: tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0')
********************
observing microbatch 49
current batch shape is torch.Size([1, 3])
batch is tensor([[ 1.5058,  0.5879, -0.2815]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 49
********************
inputting microbatch 48 into partition 1
before moving to cuda:1: tensor([[0.7743, 0.5865, 0.2953, 0.2616]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 48
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 48
********************
receiving microbatch 49 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 49, tensor([[0.6916, 0.5781, 0.1718, 0.3429]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 49
result is tensor([[0.6916, 0.5781, 0.1718, 0.3429]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6916, 0.5781, 0.1718, 0.3429]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 48 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 48, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 48
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6916, 0.5781, 0.1718, 0.3429]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0'), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(50, 0), (49, 1)]
inputting microbatch 50 into partition 0
before moving to cuda:0: tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0')
after: tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0')
********************
observing microbatch 50
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.0176, -0.0702, -0.4395]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 50
********************
inputting microbatch 49 into partition 1
before moving to cuda:1: tensor([[0.6916, 0.5781, 0.1718, 0.3429]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 49
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 49
********************
receiving microbatch 50 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 50, tensor([[0.5566, 0.3960, 0.3620, 0.4580]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 50
result is tensor([[0.5566, 0.3960, 0.3620, 0.4580]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5566, 0.3960, 0.3620, 0.4580]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 49 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 49, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 49
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5566, 0.3960, 0.3620, 0.4580]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0'), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(51, 0), (50, 1)]
inputting microbatch 51 into partition 0
before moving to cuda:0: tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0')
after: tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0')
********************
observing microbatch 51
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.4283, -0.4961,  0.4762]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 51
********************
inputting microbatch 50 into partition 1
before moving to cuda:1: tensor([[0.5566, 0.3960, 0.3620, 0.4580]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 50
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 50
********************
receiving microbatch 51 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 51, tensor([[0.6204, 0.3815, 0.5721, 0.4087]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 51
result is tensor([[0.6204, 0.3815, 0.5721, 0.4087]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6204, 0.3815, 0.5721, 0.4087]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 50 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 50, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 50
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6204, 0.3815, 0.5721, 0.4087]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0'), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(52, 0), (51, 1)]
inputting microbatch 52 into partition 0
before moving to cuda:0: tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0')
after: tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0')
********************
observing microbatch 52
current batch shape is torch.Size([1, 3])
batch is tensor([[-2.2818, -1.1239,  1.2386]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 52
********************
inputting microbatch 51 into partition 1
before moving to cuda:1: tensor([[0.6204, 0.3815, 0.5721, 0.4087]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 51
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 51
********************
receiving microbatch 52 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 52, tensor([[0.6052, 0.2522, 0.8701, 0.4141]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 52
result is tensor([[0.6052, 0.2522, 0.8701, 0.4141]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6052, 0.2522, 0.8701, 0.4141]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 51 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 51, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 51
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6052, 0.2522, 0.8701, 0.4141]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0'), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(53, 0), (52, 1)]
inputting microbatch 53 into partition 0
before moving to cuda:0: tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0')
after: tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0')
********************
observing microbatch 53
current batch shape is torch.Size([1, 3])
batch is tensor([[0.2252, 0.2327, 0.2113]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 53
********************
inputting microbatch 52 into partition 1
before moving to cuda:1: tensor([[0.6052, 0.2522, 0.8701, 0.4141]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 52
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 52
********************
receiving microbatch 53 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 53, tensor([[0.6868, 0.4798, 0.3746, 0.3403]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 53
result is tensor([[0.6868, 0.4798, 0.3746, 0.3403]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6868, 0.4798, 0.3746, 0.3403]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 52 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 52, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 52
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6868, 0.4798, 0.3746, 0.3403]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0'), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(54, 0), (53, 1)]
inputting microbatch 54 into partition 0
before moving to cuda:0: tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0')
after: tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0')
********************
observing microbatch 54
current batch shape is torch.Size([1, 3])
batch is tensor([[0.5224, 0.5252, 0.5703]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 54
********************
inputting microbatch 53 into partition 1
before moving to cuda:1: tensor([[0.6868, 0.4798, 0.3746, 0.3403]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 53
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 53
********************
receiving microbatch 54 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 54, tensor([[0.7639, 0.5504, 0.3487, 0.2673]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 54
result is tensor([[0.7639, 0.5504, 0.3487, 0.2673]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.7639, 0.5504, 0.3487, 0.2673]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 53 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 53, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 53
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.7639, 0.5504, 0.3487, 0.2673]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0'), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(55, 0), (54, 1)]
inputting microbatch 55 into partition 0
before moving to cuda:0: tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0')
after: tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0')
********************
observing microbatch 55
current batch shape is torch.Size([1, 3])
batch is tensor([[-1.0096,  0.1626,  0.2474]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 55
********************
inputting microbatch 54 into partition 1
before moving to cuda:1: tensor([[0.7639, 0.5504, 0.3487, 0.2673]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 54
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 54
********************
receiving microbatch 55 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 55, tensor([[0.6581, 0.3854, 0.5417, 0.3521]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 55
result is tensor([[0.6581, 0.3854, 0.5417, 0.3521]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6581, 0.3854, 0.5417, 0.3521]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 54 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 54, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 54
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6581, 0.3854, 0.5417, 0.3521]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0'), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(56, 0), (55, 1)]
inputting microbatch 56 into partition 0
before moving to cuda:0: tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0')
after: tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0')
********************
observing microbatch 56
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.1796,  1.6534, -0.3085]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 56
********************
inputting microbatch 55 into partition 1
before moving to cuda:1: tensor([[0.6581, 0.3854, 0.5417, 0.3521]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 55
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 55
********************
receiving microbatch 56 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 56, tensor([[0.7745, 0.5766, 0.1967, 0.2318]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 56
result is tensor([[0.7745, 0.5766, 0.1967, 0.2318]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.7745, 0.5766, 0.1967, 0.2318]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 55 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 55, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 55
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.7745, 0.5766, 0.1967, 0.2318]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0'), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(57, 0), (56, 1)]
inputting microbatch 57 into partition 0
before moving to cuda:0: tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0')
after: tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0')
********************
observing microbatch 57
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.6092,  0.6093, -0.9633]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 57
********************
inputting microbatch 56 into partition 1
before moving to cuda:1: tensor([[0.7745, 0.5766, 0.1967, 0.2318]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 56
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 56
********************
receiving microbatch 57 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 57, tensor([[0.5575, 0.3842, 0.3018, 0.4315]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 57
result is tensor([[0.5575, 0.3842, 0.3018, 0.4315]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5575, 0.3842, 0.3018, 0.4315]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 56 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 56, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 56
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5575, 0.3842, 0.3018, 0.4315]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0'), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(58, 0), (57, 1)]
inputting microbatch 58 into partition 0
before moving to cuda:0: tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0')
after: tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0')
********************
observing microbatch 58
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.1161, -1.5827, -0.6464]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 58
********************
inputting microbatch 57 into partition 1
before moving to cuda:1: tensor([[0.5575, 0.3842, 0.3018, 0.4315]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 57
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 57
********************
receiving microbatch 58 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 58, tensor([[0.3362, 0.2701, 0.4924, 0.6841]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 58
result is tensor([[0.3362, 0.2701, 0.4924, 0.6841]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3362, 0.2701, 0.4924, 0.6841]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 57 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 57, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 57
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3362, 0.2701, 0.4924, 0.6841]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0'), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(59, 0), (58, 1)]
inputting microbatch 59 into partition 0
before moving to cuda:0: tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0')
after: tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0')
********************
observing microbatch 59
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.6848,  1.6880, -1.6998]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 59
********************
inputting microbatch 58 into partition 1
before moving to cuda:1: tensor([[0.3362, 0.2701, 0.4924, 0.6841]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 58
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 58
********************
receiving microbatch 59 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 59, tensor([[0.5918, 0.4329, 0.1591, 0.3742]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 59
result is tensor([[0.5918, 0.4329, 0.1591, 0.3742]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5918, 0.4329, 0.1591, 0.3742]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 58 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 58, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 58
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5918, 0.4329, 0.1591, 0.3742]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0'), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(60, 0), (59, 1)]
inputting microbatch 60 into partition 0
before moving to cuda:0: tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0')
after: tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0')
********************
observing microbatch 60
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.9304,  1.5514, -0.3217]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 60
********************
inputting microbatch 59 into partition 1
before moving to cuda:1: tensor([[0.5918, 0.4329, 0.1591, 0.3742]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 59
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 59
********************
receiving microbatch 60 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 60, tensor([[0.7446, 0.4841, 0.3100, 0.2484]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 60
result is tensor([[0.7446, 0.4841, 0.3100, 0.2484]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.7446, 0.4841, 0.3100, 0.2484]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 59 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 59, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 59
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.7446, 0.4841, 0.3100, 0.2484]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0'), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(61, 0), (60, 1)]
inputting microbatch 61 into partition 0
before moving to cuda:0: tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0')
after: tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0')
********************
observing microbatch 61
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.0377, -0.1169,  0.3499]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 61
********************
inputting microbatch 60 into partition 1
before moving to cuda:1: tensor([[0.7446, 0.4841, 0.3100, 0.2484]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 60
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 60
********************
receiving microbatch 61 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 61, tensor([[0.6598, 0.4418, 0.4536, 0.3704]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 61
result is tensor([[0.6598, 0.4418, 0.4536, 0.3704]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6598, 0.4418, 0.4536, 0.3704]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 60 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 60, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 60
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6598, 0.4418, 0.4536, 0.3704]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0'), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(62, 0), (61, 1)]
inputting microbatch 62 into partition 0
before moving to cuda:0: tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0')
after: tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0')
********************
observing microbatch 62
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.2439,  0.0276, -1.9983]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 62
********************
inputting microbatch 61 into partition 1
before moving to cuda:1: tensor([[0.6598, 0.4418, 0.4536, 0.3704]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 61
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 61
********************
receiving microbatch 62 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 62, tensor([[0.3454, 0.3042, 0.2208, 0.6333]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 62
result is tensor([[0.3454, 0.3042, 0.2208, 0.6333]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3454, 0.3042, 0.2208, 0.6333]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 61 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 61, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 61
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3454, 0.3042, 0.2208, 0.6333]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')]
====================
schedule for this step is
[(63, 0), (62, 1)]
inputting microbatch 63 into partition 0
before moving to cuda:0: tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')
after: tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')
********************
observing microbatch 63
current batch shape is torch.Size([1, 3])
batch is tensor([[ 1.4419, -0.3411,  0.3442]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 63
********************
inputting microbatch 62 into partition 1
before moving to cuda:1: tensor([[0.3454, 0.3042, 0.2208, 0.6333]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 62
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 62
********************
receiving microbatch 63 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 63, tensor([[0.6615, 0.5251, 0.3100, 0.3923]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 63
result is tensor([[0.6615, 0.5251, 0.3100, 0.3923]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6615, 0.5251, 0.3100, 0.3923]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 62 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 62, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 62
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6615, 0.5251, 0.3100, 0.3923]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)]
====================
schedule for this step is
[(63, 1)]
inputting microbatch 63 into partition 1
before moving to cuda:1: tensor([[0.6615, 0.5251, 0.3100, 0.3923]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 63
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9654, 0.9942, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 63
********************
receiving microbatch 63 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 63, tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 63
result is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9451, 0.9962, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9648, 0.9941, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9654, 0.9942, 0.7311, 0.7311, 0.7311]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)]
finished running cycle
_____________________________ test_forward_0[2-1] ______________________________

batch_size = 1, split_size = 2

    @pytest.mark.a4_2_2
    @pytest.mark.parametrize("batch_size", [1, 16, 32, 64])
    @pytest.mark.parametrize("split_size", [1, 2, 4, 8, 16])
    # @pytest.mark.parametrize("batch_size", [1])
    # @pytest.mark.parametrize("split_size", [1, 2])
    def test_forward_0(batch_size, split_size):
        model = nn.Sequential(
            nn.Linear(3, 4).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
            nn.Linear(4, 5).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
        )
    
        x = torch.randn(batch_size, 3).to('cuda:0')
        y0 = model(x).to('cpu')
    
        # move the last two layer to another device
        model[-2] = model[-2].to('cuda:1')
        model[-1] = WithDevice(nn.Sigmoid(), 'cuda:1')
        pipe = Pipe(model, split_size=split_size)
        y1 = pipe(x).to('cpu')
>       assert torch.allclose(y0, y1)
E       assert False
E        +  where False = <built-in method allclose of type object at 0x153da78cb760>(tensor([[0.4499, 0.5152, 0.2839, 0.4233, 0.6722]], grad_fn=<ToCopyBackward0>), tensor([[0.9783, 0.0000, 0.6750, 0.6750, 0.6750]], grad_fn=<ToCopyBackward0>))
E        +    where <built-in method allclose of type object at 0x153da78cb760> = torch.allclose

batch_size = 1
model      = Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
  (2): Linear(in_features=4, out_features=5, bias=True)
  (3): WithDevice(
    (_module): Sigmoid()
  )
)
pipe       = Pipe(
  (partitions): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=3, out_features=4, bias=True)
   ...inear(in_features=4, out_features=5, bias=True)
      (1): WithDevice(
        (_module): Sigmoid()
      )
    )
  )
)
split_size = 2
x          = tensor([[ 0.0887, -1.0975, -1.5350]], device='cuda:0')
y0         = tensor([[0.4499, 0.5152, 0.2839, 0.4233, 0.6722]], grad_fn=<ToCopyBackward0>)
y1         = tensor([[0.9783, 0.0000, 0.6750, 0.6750, 0.6750]], grad_fn=<ToCopyBackward0>)

tests/test_pipeline.py:83: AssertionError
----------------------------- Captured stdout call -----------------------------
in init
ModuleList(
  (0): Sequential(
    (0): Linear(in_features=3, out_features=4, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
  (1): Sequential(
    (0): Linear(in_features=4, out_features=5, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
)
[device(type='cuda', index=0), device(type='cuda', index=1)]
before running the cycle
[tensor([[ 0.0887, -1.0975, -1.5350]], device='cuda:0')]
====================
schedule for this step is
[(0, 0)]
inputting microbatch 0 into partition 0
before moving to cuda:0: tensor([[ 0.0887, -1.0975, -1.5350]], device='cuda:0')
after: tensor([[ 0.0887, -1.0975, -1.5350]], device='cuda:0')
********************
observing microbatch 0
current batch shape is torch.Size([1, 3])
batch is tensor([[ 0.0887, -1.0975, -1.5350]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 0
********************
receiving microbatch 0 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 0, tensor([[0.4497, 0.3824, 0.4682, 0.7787]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.4497, 0.3824, 0.4682, 0.7787]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4497, 0.3824, 0.4682, 0.7787]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.4497, 0.3824, 0.4682, 0.7787]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)]
====================
schedule for this step is
[(0, 1)]
inputting microbatch 0 into partition 1
before moving to cuda:1: tensor([[0.4497, 0.3824, 0.4682, 0.7787]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.7311, 0.7311, 0.9654, 0.9942]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 0
current batch shape is torch.Size([1, 4])
batch is tensor([[0.7311, 0.7311, 0.9654, 0.9942]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 0
********************
receiving microbatch 0 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 0, tensor([[0.9783, 0.0000, 0.6750, 0.6750, 0.6750]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.9783, 0.0000, 0.6750, 0.6750, 0.6750]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9783, 0.0000, 0.6750, 0.6750, 0.6750]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9783, 0.0000, 0.6750, 0.6750, 0.6750]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)]
finished running cycle
_____________________________ test_forward_0[2-16] _____________________________

batch_size = 16, split_size = 2

    @pytest.mark.a4_2_2
    @pytest.mark.parametrize("batch_size", [1, 16, 32, 64])
    @pytest.mark.parametrize("split_size", [1, 2, 4, 8, 16])
    # @pytest.mark.parametrize("batch_size", [1])
    # @pytest.mark.parametrize("split_size", [1, 2])
    def test_forward_0(batch_size, split_size):
        model = nn.Sequential(
            nn.Linear(3, 4).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
            nn.Linear(4, 5).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
        )
    
        x = torch.randn(batch_size, 3).to('cuda:0')
        y0 = model(x).to('cpu')
    
        # move the last two layer to another device
        model[-2] = model[-2].to('cuda:1')
        model[-1] = WithDevice(nn.Sigmoid(), 'cuda:1')
        pipe = Pipe(model, split_size=split_size)
        y1 = pipe(x).to('cpu')
>       assert torch.allclose(y0, y1)
E       assert False
E        +  where False = <built-in method allclose of type object at 0x153da78cb760>(tensor([[0.2967, 0.5085, 0.2638, 0.5089, 0.5096],\n        [0.3107, 0.4861, 0.2769, 0.5079, 0.5033],\n        [0.3031, 0.4777, 0.2739, 0.5123, 0.5078],\n        [0.2932, 0.4808, 0.2668, 0.5147, 0.5128],\n        [0.3096, 0.4846, 0.2794, 0.5126, 0.5142],\n        [0.2934, 0.5500, 0.2549, 0.5046, 0.5106],\n        [0.3126, 0.5329, 0.2694, 0.5002, 0.4993],\n        [0.3077, 0.5114, 0.2699, 0.5044, 0.5022],\n        [0.2994, 0.5082, 0.2664, 0.5092, 0.5111],\n        [0.3079, 0.5055, 0.2709, 0.5050, 0.5018],\n        [0.3085, 0.4643, 0.2808, 0.5141, 0.5088],\n        [0.2822, 0.5018, 0.2550, 0.5141, 0.5130],\n        [0.3017, 0.5022, 0.2696, 0.5103, 0.5124],\n        [0.3139, 0.5345, 0.2697, 0.4994, 0.4979],\n        [0.3057, 0.5310, 0.2641, 0.5007, 0.4984],\n        [0.3106, 0.5070, 0.2721, 0.5034, 0.4994]], grad_fn=<ToCopyBackward0>), tensor([[0.9481, 0.0000, 0.9158, 0.6508, 0.6508],\n        [0.9883, 0.9844, 0.0000, 0.6508, 0.6508],\n        [0.9615, 0.0000, 0.0000, 0.6508, 0.6508],\n        [0.9648, 0.0000, 0.0000, 0.6508, 0.6508],\n        [0.9133, 1.0000, 0.9996, 0.6508, 0.6508],\n        [1.0000, 0.0000, 1.0000, 0.6508, 0.6508],\n        [0.9702, 0.0000, 0.0000, 0.6508, 0.6508],\n        [0.9633, 0.0000, 0.0000, 0.6508, 0.6508],\n        [0.9605, 0.0000, 0.0000, 0.6508, 0.6508],\n        [0.8730, 0.9213, 0.0000, 0.6508, 0.6508],\n        [0.9580, 0.0000, 0.0000, 0.6508, 0.6508],\n        [0.9278, 0.0000, 0.0000, 0.6508, 0.6508],\n        [0.9593, 0.0000, 0.0000, 0.6508, 0.6508],\n        [0.8730, 0.9213, 0.0000, 0.6508, 0.6508],\n        [0.9580, 0.0000, 0.0000, 0.6508, 0.6508],\n        [0.9278, 0.0000, 0.0000, 0.6508, 0.6508]], grad_fn=<ToCopyBackward0>))
E        +    where <built-in method allclose of type object at 0x153da78cb760> = torch.allclose

batch_size = 16
model      = Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
  (2): Linear(in_features=4, out_features=5, bias=True)
  (3): WithDevice(
    (_module): Sigmoid()
  )
)
pipe       = Pipe(
  (partitions): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=3, out_features=4, bias=True)
   ...inear(in_features=4, out_features=5, bias=True)
      (1): WithDevice(
        (_module): Sigmoid()
      )
    )
  )
)
split_size = 2
x          = tensor([[-0.1136,  0.2068,  0.6507],
        [ 1.0385, -0.2547, -0.5947],
        [ 1.2122,  0.4690, -0.1661],
       ...-0.9107, -2.0256,  0.1210],
        [-0.2514, -2.0604,  1.4918],
        [ 0.5657, -1.3961,  0.4089]], device='cuda:0')
y0         = tensor([[0.2967, 0.5085, 0.2638, 0.5089, 0.5096],
        [0.3107, 0.4861, 0.2769, 0.5079, 0.5033],
        [0.3031, 0...[0.3057, 0.5310, 0.2641, 0.5007, 0.4984],
        [0.3106, 0.5070, 0.2721, 0.5034, 0.4994]], grad_fn=<ToCopyBackward0>)
y1         = tensor([[0.9481, 0.0000, 0.9158, 0.6508, 0.6508],
        [0.9883, 0.9844, 0.0000, 0.6508, 0.6508],
        [0.9615, 0...[0.9580, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.9278, 0.0000, 0.0000, 0.6508, 0.6508]], grad_fn=<ToCopyBackward0>)

tests/test_pipeline.py:83: AssertionError
----------------------------- Captured stdout call -----------------------------
in init
ModuleList(
  (0): Sequential(
    (0): Linear(in_features=3, out_features=4, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
  (1): Sequential(
    (0): Linear(in_features=4, out_features=5, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
)
[device(type='cuda', index=0), device(type='cuda', index=1)]
before running the cycle
[tensor([[-0.1136,  0.2068,  0.6507],
        [ 1.0385, -0.2547, -0.5947]], device='cuda:0'), tensor([[ 1.2122,  0.4690, -0.1661],
        [ 0.9136,  1.1403,  0.4652]], device='cuda:0'), tensor([[-0.2218,  1.7340, -2.4686],
        [-2.0983, -0.2208,  1.0352]], device='cuda:0'), tensor([[-0.9371, -1.7086,  0.0167],
        [ 0.1141, -0.9655,  0.3230]], device='cuda:0'), tensor([[-0.4301,  0.5721, -0.1049],
        [ 0.4473, -0.9558,  0.3613]], device='cuda:0'), tensor([[ 1.4525,  1.0707, -1.4545],
        [ 0.4493,  0.5765,  2.4316]], device='cuda:0'), tensor([[-0.4234,  0.9402, -0.7366],
        [-0.9107, -2.0256,  0.1210]], device='cuda:0'), tensor([[-0.2514, -2.0604,  1.4918],
        [ 0.5657, -1.3961,  0.4089]], device='cuda:0')]
====================
schedule for this step is
[(0, 0)]
inputting microbatch 0 into partition 0
before moving to cuda:0: tensor([[-0.1136,  0.2068,  0.6507],
        [ 1.0385, -0.2547, -0.5947]], device='cuda:0')
after: tensor([[-0.1136,  0.2068,  0.6507],
        [ 1.0385, -0.2547, -0.5947]], device='cuda:0')
********************
observing microbatch 0
current batch shape is torch.Size([2, 3])
batch is tensor([[-0.1136,  0.2068,  0.6507],
        [ 1.0385, -0.2547, -0.5947]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 0
********************
receiving microbatch 0 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 0, tensor([[0.5453, 0.4928, 0.4465, 0.4518],
        [0.2768, 0.6004, 0.3143, 0.4293]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.5453, 0.4928, 0.4465, 0.4518],
        [0.2768, 0.6004, 0.3143, 0.4293]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5453, 0.4928, 0.4465, 0.4518],
        [0.2768, 0.6004, 0.3143, 0.4293]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.5453, 0.4928, 0.4465, 0.4518],
        [0.2768, 0.6004, 0.3143, 0.4293]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 1.2122,  0.4690, -0.1661],
        [ 0.9136,  1.1403,  0.4652]], device='cuda:0'), tensor([[-0.2218,  1.7340, -2.4686],
        [-2.0983, -0.2208,  1.0352]], device='cuda:0'), tensor([[-0.9371, -1.7086,  0.0167],
        [ 0.1141, -0.9655,  0.3230]], device='cuda:0'), tensor([[-0.4301,  0.5721, -0.1049],
        [ 0.4473, -0.9558,  0.3613]], device='cuda:0'), tensor([[ 1.4525,  1.0707, -1.4545],
        [ 0.4493,  0.5765,  2.4316]], device='cuda:0'), tensor([[-0.4234,  0.9402, -0.7366],
        [-0.9107, -2.0256,  0.1210]], device='cuda:0'), tensor([[-0.2514, -2.0604,  1.4918],
        [ 0.5657, -1.3961,  0.4089]], device='cuda:0')]
====================
schedule for this step is
[(1, 0), (0, 1)]
inputting microbatch 1 into partition 0
before moving to cuda:0: tensor([[ 1.2122,  0.4690, -0.1661],
        [ 0.9136,  1.1403,  0.4652]], device='cuda:0')
after: tensor([[ 1.2122,  0.4690, -0.1661],
        [ 0.9136,  1.1403,  0.4652]], device='cuda:0')
********************
observing microbatch 1
current batch shape is torch.Size([2, 3])
batch is tensor([[ 1.2122,  0.4690, -0.1661],
        [ 0.9136,  1.1403,  0.4652]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 1
********************
inputting microbatch 0 into partition 1
before moving to cuda:1: tensor([[0.5453, 0.4928, 0.4465, 0.4518],
        [0.2768, 0.6004, 0.3143, 0.4293]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.0000e+00, 9.9054e-01, 9.7044e-01, 9.6185e-01],
        [9.6490e-01, 1.8655e+00, 1.0842e-19, 1.8662e+00]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 0
current batch shape is torch.Size([2, 4])
batch is tensor([[0.0000e+00, 9.9054e-01, 9.7044e-01, 9.6185e-01],
        [9.6490e-01, 1.8655e+00, 1.0842e-19, 1.8662e+00]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 0
********************
receiving microbatch 1 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 1, tensor([[0.3748, 0.5072, 0.3325, 0.5106],
        [0.5400, 0.4090, 0.3882, 0.5704]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 1
result is tensor([[0.3748, 0.5072, 0.3325, 0.5106],
        [0.5400, 0.4090, 0.3882, 0.5704]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3748, 0.5072, 0.3325, 0.5106],
        [0.5400, 0.4090, 0.3882, 0.5704]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 0 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 0, tensor([[0.9481, 0.0000, 0.9158, 0.6508, 0.6508],
        [0.9883, 0.9844, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.9481, 0.0000, 0.9158, 0.6508, 0.6508],
        [0.9883, 0.9844, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9481, 0.0000, 0.9158, 0.6508, 0.6508],
        [0.9883, 0.9844, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9481, 0.0000, 0.9158, 0.6508, 0.6508],
        [0.9883, 0.9844, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3748, 0.5072, 0.3325, 0.5106],
        [0.5400, 0.4090, 0.3882, 0.5704]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.2218,  1.7340, -2.4686],
        [-2.0983, -0.2208,  1.0352]], device='cuda:0'), tensor([[-0.9371, -1.7086,  0.0167],
        [ 0.1141, -0.9655,  0.3230]], device='cuda:0'), tensor([[-0.4301,  0.5721, -0.1049],
        [ 0.4473, -0.9558,  0.3613]], device='cuda:0'), tensor([[ 1.4525,  1.0707, -1.4545],
        [ 0.4493,  0.5765,  2.4316]], device='cuda:0'), tensor([[-0.4234,  0.9402, -0.7366],
        [-0.9107, -2.0256,  0.1210]], device='cuda:0'), tensor([[-0.2514, -2.0604,  1.4918],
        [ 0.5657, -1.3961,  0.4089]], device='cuda:0')]
====================
schedule for this step is
[(2, 0), (1, 1)]
inputting microbatch 2 into partition 0
before moving to cuda:0: tensor([[-0.2218,  1.7340, -2.4686],
        [-2.0983, -0.2208,  1.0352]], device='cuda:0')
after: tensor([[-0.2218,  1.7340, -2.4686],
        [-2.0983, -0.2208,  1.0352]], device='cuda:0')
********************
observing microbatch 2
current batch shape is torch.Size([2, 3])
batch is tensor([[-0.2218,  1.7340, -2.4686],
        [-2.0983, -0.2208,  1.0352]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 2
********************
inputting microbatch 1 into partition 1
before moving to cuda:1: tensor([[0.3748, 0.5072, 0.3325, 0.5106],
        [0.5400, 0.4090, 0.3882, 0.5704]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9481, 0.9158, 0.6508, 0.6508],
        [0.9883, 0.9844, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 1
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9481, 0.9158, 0.6508, 0.6508],
        [0.9883, 0.9844, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 1
********************
receiving microbatch 2 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 2, tensor([[0.3587, 0.4994, 0.3467, 0.4311],
        [0.6937, 0.5176, 0.5877, 0.3326]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 2
result is tensor([[0.3587, 0.4994, 0.3467, 0.4311],
        [0.6937, 0.5176, 0.5877, 0.3326]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3587, 0.4994, 0.3467, 0.4311],
        [0.6937, 0.5176, 0.5877, 0.3326]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 1 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 1, tensor([[0.9615, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.9648, 0.0000, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 1
result is tensor([[0.9615, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.9648, 0.0000, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9615, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.9648, 0.0000, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9481, 0.0000, 0.9158, 0.6508, 0.6508],
        [0.9883, 0.9844, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9615, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.9648, 0.0000, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3587, 0.4994, 0.3467, 0.4311],
        [0.6937, 0.5176, 0.5877, 0.3326]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.9371, -1.7086,  0.0167],
        [ 0.1141, -0.9655,  0.3230]], device='cuda:0'), tensor([[-0.4301,  0.5721, -0.1049],
        [ 0.4473, -0.9558,  0.3613]], device='cuda:0'), tensor([[ 1.4525,  1.0707, -1.4545],
        [ 0.4493,  0.5765,  2.4316]], device='cuda:0'), tensor([[-0.4234,  0.9402, -0.7366],
        [-0.9107, -2.0256,  0.1210]], device='cuda:0'), tensor([[-0.2514, -2.0604,  1.4918],
        [ 0.5657, -1.3961,  0.4089]], device='cuda:0')]
====================
schedule for this step is
[(3, 0), (2, 1)]
inputting microbatch 3 into partition 0
before moving to cuda:0: tensor([[-0.9371, -1.7086,  0.0167],
        [ 0.1141, -0.9655,  0.3230]], device='cuda:0')
after: tensor([[-0.9371, -1.7086,  0.0167],
        [ 0.1141, -0.9655,  0.3230]], device='cuda:0')
********************
observing microbatch 3
current batch shape is torch.Size([2, 3])
batch is tensor([[-0.9371, -1.7086,  0.0167],
        [ 0.1141, -0.9655,  0.3230]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 3
********************
inputting microbatch 2 into partition 1
before moving to cuda:1: tensor([[0.3587, 0.4994, 0.3467, 0.4311],
        [0.6937, 0.5176, 0.5877, 0.3326]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[-1.0842e-19,  1.8654e+00, -2.0000e+00,  1.7877e+00],
        [-2.0000e+00,  1.7877e+00,  3.6893e+19,  1.8662e+00]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 2
current batch shape is torch.Size([2, 4])
batch is tensor([[-1.0842e-19,  1.8654e+00, -2.0000e+00,  1.7877e+00],
        [-2.0000e+00,  1.7877e+00,  3.6893e+19,  1.8662e+00]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 2
********************
receiving microbatch 3 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 3, tensor([[0.3415, 0.7050, 0.4384, 0.2579],
        [0.3684, 0.6242, 0.3965, 0.3658]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 3
result is tensor([[0.3415, 0.7050, 0.4384, 0.2579],
        [0.3684, 0.6242, 0.3965, 0.3658]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3415, 0.7050, 0.4384, 0.2579],
        [0.3684, 0.6242, 0.3965, 0.3658]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 2 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 2, tensor([[0.9133, 1.0000, 0.9996, 0.6508, 0.6508],
        [1.0000, 0.0000, 1.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 2
result is tensor([[0.9133, 1.0000, 0.9996, 0.6508, 0.6508],
        [1.0000, 0.0000, 1.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9133, 1.0000, 0.9996, 0.6508, 0.6508],
        [1.0000, 0.0000, 1.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9481, 0.0000, 0.9158, 0.6508, 0.6508],
        [0.9883, 0.9844, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9615, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.9648, 0.0000, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9133, 1.0000, 0.9996, 0.6508, 0.6508],
        [1.0000, 0.0000, 1.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3415, 0.7050, 0.4384, 0.2579],
        [0.3684, 0.6242, 0.3965, 0.3658]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.4301,  0.5721, -0.1049],
        [ 0.4473, -0.9558,  0.3613]], device='cuda:0'), tensor([[ 1.4525,  1.0707, -1.4545],
        [ 0.4493,  0.5765,  2.4316]], device='cuda:0'), tensor([[-0.4234,  0.9402, -0.7366],
        [-0.9107, -2.0256,  0.1210]], device='cuda:0'), tensor([[-0.2514, -2.0604,  1.4918],
        [ 0.5657, -1.3961,  0.4089]], device='cuda:0')]
====================
schedule for this step is
[(4, 0), (3, 1)]
inputting microbatch 4 into partition 0
before moving to cuda:0: tensor([[-0.4301,  0.5721, -0.1049],
        [ 0.4473, -0.9558,  0.3613]], device='cuda:0')
after: tensor([[-0.4301,  0.5721, -0.1049],
        [ 0.4473, -0.9558,  0.3613]], device='cuda:0')
********************
observing microbatch 4
current batch shape is torch.Size([2, 3])
batch is tensor([[-0.4301,  0.5721, -0.1049],
        [ 0.4473, -0.9558,  0.3613]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 4
********************
inputting microbatch 3 into partition 1
before moving to cuda:1: tensor([[0.3415, 0.7050, 0.4384, 0.2579],
        [0.3684, 0.6242, 0.3965, 0.3658]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9133, 1.0000, 0.9996, 0.6508],
        [0.6508, 1.0000, 1.0000, 0.6508]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 3
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9133, 1.0000, 0.9996, 0.6508],
        [0.6508, 1.0000, 1.0000, 0.6508]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 3
********************
receiving microbatch 4 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 4, tensor([[0.5221, 0.4951, 0.4406, 0.4306],
        [0.3488, 0.6212, 0.3772, 0.3853]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 4
result is tensor([[0.5221, 0.4951, 0.4406, 0.4306],
        [0.3488, 0.6212, 0.3772, 0.3853]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5221, 0.4951, 0.4406, 0.4306],
        [0.3488, 0.6212, 0.3772, 0.3853]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 3 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 3, tensor([[0.9702, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.9633, 0.0000, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 3
result is tensor([[0.9702, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.9633, 0.0000, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9702, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.9633, 0.0000, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9481, 0.0000, 0.9158, 0.6508, 0.6508],
        [0.9883, 0.9844, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9615, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.9648, 0.0000, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9133, 1.0000, 0.9996, 0.6508, 0.6508],
        [1.0000, 0.0000, 1.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9702, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.9633, 0.0000, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5221, 0.4951, 0.4406, 0.4306],
        [0.3488, 0.6212, 0.3772, 0.3853]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 1.4525,  1.0707, -1.4545],
        [ 0.4493,  0.5765,  2.4316]], device='cuda:0'), tensor([[-0.4234,  0.9402, -0.7366],
        [-0.9107, -2.0256,  0.1210]], device='cuda:0'), tensor([[-0.2514, -2.0604,  1.4918],
        [ 0.5657, -1.3961,  0.4089]], device='cuda:0')]
====================
schedule for this step is
[(5, 0), (4, 1)]
inputting microbatch 5 into partition 0
before moving to cuda:0: tensor([[ 1.4525,  1.0707, -1.4545],
        [ 0.4493,  0.5765,  2.4316]], device='cuda:0')
after: tensor([[ 1.4525,  1.0707, -1.4545],
        [ 0.4493,  0.5765,  2.4316]], device='cuda:0')
********************
observing microbatch 5
current batch shape is torch.Size([2, 3])
batch is tensor([[ 1.4525,  1.0707, -1.4545],
        [ 0.4493,  0.5765,  2.4316]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 5
********************
inputting microbatch 4 into partition 1
before moving to cuda:1: tensor([[0.5221, 0.4951, 0.4406, 0.4306],
        [0.3488, 0.6212, 0.3772, 0.3853]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9702, 0.6508, 0.6508, 0.9633],
        [0.6508, 0.6508, 0.0000, 0.0000]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 4
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9702, 0.6508, 0.6508, 0.9633],
        [0.6508, 0.6508, 0.0000, 0.0000]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 4
********************
receiving microbatch 5 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 5, tensor([[0.2827, 0.5126, 0.2792, 0.5155],
        [0.7273, 0.3675, 0.4953, 0.5838]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 5
result is tensor([[0.2827, 0.5126, 0.2792, 0.5155],
        [0.7273, 0.3675, 0.4953, 0.5838]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.2827, 0.5126, 0.2792, 0.5155],
        [0.7273, 0.3675, 0.4953, 0.5838]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 4 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 4, tensor([[0.9605, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.8730, 0.9213, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 4
result is tensor([[0.9605, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.8730, 0.9213, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9605, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.8730, 0.9213, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9481, 0.0000, 0.9158, 0.6508, 0.6508],
        [0.9883, 0.9844, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9615, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.9648, 0.0000, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9133, 1.0000, 0.9996, 0.6508, 0.6508],
        [1.0000, 0.0000, 1.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9702, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.9633, 0.0000, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9605, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.8730, 0.9213, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.2827, 0.5126, 0.2792, 0.5155],
        [0.7273, 0.3675, 0.4953, 0.5838]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.4234,  0.9402, -0.7366],
        [-0.9107, -2.0256,  0.1210]], device='cuda:0'), tensor([[-0.2514, -2.0604,  1.4918],
        [ 0.5657, -1.3961,  0.4089]], device='cuda:0')]
====================
schedule for this step is
[(6, 0), (5, 1)]
inputting microbatch 6 into partition 0
before moving to cuda:0: tensor([[-0.4234,  0.9402, -0.7366],
        [-0.9107, -2.0256,  0.1210]], device='cuda:0')
after: tensor([[-0.4234,  0.9402, -0.7366],
        [-0.9107, -2.0256,  0.1210]], device='cuda:0')
********************
observing microbatch 6
current batch shape is torch.Size([2, 3])
batch is tensor([[-0.4234,  0.9402, -0.7366],
        [-0.9107, -2.0256,  0.1210]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 6
********************
inputting microbatch 5 into partition 1
before moving to cuda:1: tensor([[0.2827, 0.5126, 0.2792, 0.5155],
        [0.7273, 0.3675, 0.4953, 0.5838]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9605, 0.6508, 0.6508, 0.8730],
        [0.9213, 0.6508, 0.6508, 0.0000]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 5
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9605, 0.6508, 0.6508, 0.8730],
        [0.9213, 0.6508, 0.6508, 0.0000]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 5
********************
receiving microbatch 6 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 6, tensor([[0.4874, 0.4905, 0.4190, 0.4323],
        [0.3206, 0.7265, 0.4356, 0.2443]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 6
result is tensor([[0.4874, 0.4905, 0.4190, 0.4323],
        [0.3206, 0.7265, 0.4356, 0.2443]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4874, 0.4905, 0.4190, 0.4323],
        [0.3206, 0.7265, 0.4356, 0.2443]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 5 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 5, tensor([[0.9580, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.9278, 0.0000, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 5
result is tensor([[0.9580, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.9278, 0.0000, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9580, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.9278, 0.0000, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9481, 0.0000, 0.9158, 0.6508, 0.6508],
        [0.9883, 0.9844, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9615, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.9648, 0.0000, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9133, 1.0000, 0.9996, 0.6508, 0.6508],
        [1.0000, 0.0000, 1.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9702, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.9633, 0.0000, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9605, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.8730, 0.9213, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9580, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.9278, 0.0000, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4874, 0.4905, 0.4190, 0.4323],
        [0.3206, 0.7265, 0.4356, 0.2443]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.2514, -2.0604,  1.4918],
        [ 0.5657, -1.3961,  0.4089]], device='cuda:0')]
====================
schedule for this step is
[(7, 0), (6, 1)]
inputting microbatch 7 into partition 0
before moving to cuda:0: tensor([[-0.2514, -2.0604,  1.4918],
        [ 0.5657, -1.3961,  0.4089]], device='cuda:0')
after: tensor([[-0.2514, -2.0604,  1.4918],
        [ 0.5657, -1.3961,  0.4089]], device='cuda:0')
********************
observing microbatch 7
current batch shape is torch.Size([2, 3])
batch is tensor([[-0.2514, -2.0604,  1.4918],
        [ 0.5657, -1.3961,  0.4089]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 7
********************
inputting microbatch 6 into partition 1
before moving to cuda:1: tensor([[0.4874, 0.4905, 0.4190, 0.4323],
        [0.3206, 0.7265, 0.4356, 0.2443]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9580, 0.6508, 0.6508, 0.9278],
        [0.6508, 0.6508, 0.0000, 0.0000]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 6
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9580, 0.6508, 0.6508, 0.9278],
        [0.6508, 0.6508, 0.0000, 0.0000]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 6
********************
receiving microbatch 7 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 7, tensor([[0.4150, 0.6700, 0.4521, 0.3174],
        [0.3044, 0.6595, 0.3643, 0.3623]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 7
result is tensor([[0.4150, 0.6700, 0.4521, 0.3174],
        [0.3044, 0.6595, 0.3643, 0.3623]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4150, 0.6700, 0.4521, 0.3174],
        [0.3044, 0.6595, 0.3643, 0.3623]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 6 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 6, tensor([[0.9593, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.8730, 0.9213, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 6
result is tensor([[0.9593, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.8730, 0.9213, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9593, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.8730, 0.9213, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9481, 0.0000, 0.9158, 0.6508, 0.6508],
        [0.9883, 0.9844, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9615, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.9648, 0.0000, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9133, 1.0000, 0.9996, 0.6508, 0.6508],
        [1.0000, 0.0000, 1.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9702, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.9633, 0.0000, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9605, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.8730, 0.9213, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9580, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.9278, 0.0000, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9593, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.8730, 0.9213, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4150, 0.6700, 0.4521, 0.3174],
        [0.3044, 0.6595, 0.3643, 0.3623]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)]
====================
schedule for this step is
[(7, 1)]
inputting microbatch 7 into partition 1
before moving to cuda:1: tensor([[0.4150, 0.6700, 0.4521, 0.3174],
        [0.3044, 0.6595, 0.3643, 0.3623]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9593, 0.6508, 0.6508, 0.8730],
        [0.9213, 0.6508, 0.6508, 0.0000]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 7
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9593, 0.6508, 0.6508, 0.8730],
        [0.9213, 0.6508, 0.6508, 0.0000]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 7
********************
receiving microbatch 7 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 7, tensor([[0.9580, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.9278, 0.0000, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 7
result is tensor([[0.9580, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.9278, 0.0000, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9580, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.9278, 0.0000, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9481, 0.0000, 0.9158, 0.6508, 0.6508],
        [0.9883, 0.9844, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9615, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.9648, 0.0000, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9133, 1.0000, 0.9996, 0.6508, 0.6508],
        [1.0000, 0.0000, 1.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9702, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.9633, 0.0000, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9605, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.8730, 0.9213, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9580, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.9278, 0.0000, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9593, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.8730, 0.9213, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9580, 0.0000, 0.0000, 0.6508, 0.6508],
        [0.9278, 0.0000, 0.0000, 0.6508, 0.6508]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)]
finished running cycle
_____________________________ test_forward_0[2-32] _____________________________

batch_size = 32, split_size = 2

    @pytest.mark.a4_2_2
    @pytest.mark.parametrize("batch_size", [1, 16, 32, 64])
    @pytest.mark.parametrize("split_size", [1, 2, 4, 8, 16])
    # @pytest.mark.parametrize("batch_size", [1])
    # @pytest.mark.parametrize("split_size", [1, 2])
    def test_forward_0(batch_size, split_size):
        model = nn.Sequential(
            nn.Linear(3, 4).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
            nn.Linear(4, 5).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
        )
    
        x = torch.randn(batch_size, 3).to('cuda:0')
        y0 = model(x).to('cpu')
    
        # move the last two layer to another device
        model[-2] = model[-2].to('cuda:1')
        model[-1] = WithDevice(nn.Sigmoid(), 'cuda:1')
        pipe = Pipe(model, split_size=split_size)
        y1 = pipe(x).to('cpu')
>       assert torch.allclose(y0, y1)
E       assert False
E        +  where False = <built-in method allclose of type object at 0x153da78cb760>(tensor([[0.5737, 0.4075, 0.4201, 0.5002, 0.4892],\n        [0.5940, 0.4200, 0.4586, 0.5079, 0.4607],\n        [0.5876, 0.4034, 0.4301, 0.5053, 0.4749],\n        [0.5722, 0.4186, 0.4304, 0.5032, 0.4905],\n        [0.5673, 0.3965, 0.4003, 0.4961, 0.4974],\n        [0.6062, 0.4075, 0.4577, 0.5103, 0.4490],\n        [0.5706, 0.4110, 0.4200, 0.5009, 0.4936],\n        [0.5690, 0.4210, 0.4321, 0.4988, 0.4900],\n        [0.6036, 0.3992, 0.4435, 0.5097, 0.4564],\n        [0.5738, 0.3990, 0.4109, 0.4980, 0.4893],\n        [0.6283, 0.3959, 0.4671, 0.5197, 0.4285],\n        [0.5917, 0.4056, 0.4374, 0.5071, 0.4695],\n        [0.5374, 0.4030, 0.3745, 0.4886, 0.5304],\n        [0.6207, 0.3921, 0.4548, 0.5148, 0.4376],\n        [0.5902, 0.4148, 0.4474, 0.5067, 0.4679],\n        [0.6245, 0.4162, 0.4875, 0.5206, 0.4262],\n        [0.5711, 0.4177, 0.4298, 0.5002, 0.4894],\n        [0.5527, 0.4263, 0.4249, 0.4875, 0.5019],\n        [0.5667, 0.4250, 0.4340, 0.4991, 0.4920],\n        [0.5961, 0.4152, 0.4566, 0.5059, 0.4574],\n        [0.6068, 0.4174, 0.4702, 0.5118, 0.4454],\n        [0.5933, 0.3865, 0.4182, 0.5021, 0.4698],\n        [0.5981, 0.4075, 0.4538, 0.4996, 0.4504],\n        [0.5262, 0.4438, 0.4127, 0.4859, 0.5312],\n        [0.6196, 0.3901, 0.4526, 0.5121, 0.4376],\n        [0.5770, 0.4042, 0.4248, 0.4937, 0.4787],\n        [0.5784, 0.3959, 0.4124, 0.4988, 0.4844],\n        [0.5971, 0.4175, 0.4607, 0.5063, 0.4551],\n        [0.5826, 0.4060, 0.4278, 0.5037, 0.4799],\n        [0.5820, 0.4262, 0.4551, 0.5005, 0.4694],\n        [0.5726, 0.4164, 0.4296, 0.5010, 0.4885],\n        [0.6278, 0.3953, 0.4699, 0.5134, 0.4218]], grad_fn=<ToCopyBackward0>), tensor([[0.0000, 0.0000, 0.0000, 0.9285, 0.6254],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3739, 0.9530, 0.8405, 0.9030, 0.7782],\n        [0.1967, 0.8998, 0.7423, 0.7166, 0.6572],\n        [0.4285, 0.9781, 0.9107, 0.9510, 0.8564],\n        [0.5112, 0.9669, 0.8957, 0.9135, 0.8480],\n        [0.4648, 0.9807, 0.9200, 0.9558, 0.8656],\n        [0.5696, 0.9782, 0.9220, 0.9401, 0.8656],\n        [0.4763, 0.9814, 0.9224, 0.9563, 0.8666],\n        [0.5785, 0.9797, 0.9259, 0.9440, 0.8685],\n        [0.4797, 0.9816, 0.9232, 0.9565, 0.8669],\n        [0.5797, 0.9799, 0.9264, 0.9445, 0.8689],\n        [0.4807, 0.9816, 0.9234, 0.9565, 0.8669],\n        [0.5798, 0.9799, 0.9265, 0.9446, 0.8689],\n        [0.4810, 0.9816, 0.9235, 0.9565, 0.8669],\n        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689],\n        [0.4810, 0.9817, 0.9235, 0.9565, 0.8669],\n        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689],\n        [0.4811, 0.9817, 0.9235, 0.9565, 0.8669],\n        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689],\n        [0.4811, 0.9817, 0.9235, 0.9565, 0.8669],\n        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689],\n        [0.4811, 0.9817, 0.9235, 0.9565, 0.8669],\n        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689],\n        [0.4811, 0.9817, 0.9235, 0.9565, 0.8669],\n        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689],\n        [0.4811, 0.9817, 0.9235, 0.9565, 0.8669],\n        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689],\n        [0.4811, 0.9817, 0.9235, 0.9565, 0.8669],\n        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689],\n        [0.4811, 0.9817, 0.9235, 0.9565, 0.8669],\n        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], grad_fn=<ToCopyBackward0>))
E        +    where <built-in method allclose of type object at 0x153da78cb760> = torch.allclose

batch_size = 32
model      = Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
  (2): Linear(in_features=4, out_features=5, bias=True)
  (3): WithDevice(
    (_module): Sigmoid()
  )
)
pipe       = Pipe(
  (partitions): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=3, out_features=4, bias=True)
   ...inear(in_features=4, out_features=5, bias=True)
      (1): WithDevice(
        (_module): Sigmoid()
      )
    )
  )
)
split_size = 2
x          = tensor([[ 0.7998,  0.4847,  0.2964],
        [-0.7857,  0.7675,  0.4063],
        [ 0.7393,  0.6639, -0.2285],
       ...-1.3745, -0.0693,  1.2229],
        [ 0.2510,  0.8201,  0.6696],
        [-0.8806, -1.0099, -1.2571]], device='cuda:0')
y0         = tensor([[0.5737, 0.4075, 0.4201, 0.5002, 0.4892],
        [0.5940, 0.4200, 0.4586, 0.5079, 0.4607],
        [0.5876, 0...[0.5726, 0.4164, 0.4296, 0.5010, 0.4885],
        [0.6278, 0.3953, 0.4699, 0.5134, 0.4218]], grad_fn=<ToCopyBackward0>)
y1         = tensor([[0.0000, 0.0000, 0.0000, 0.9285, 0.6254],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3739, 0...[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], grad_fn=<ToCopyBackward0>)

tests/test_pipeline.py:83: AssertionError
----------------------------- Captured stdout call -----------------------------
in init
ModuleList(
  (0): Sequential(
    (0): Linear(in_features=3, out_features=4, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
  (1): Sequential(
    (0): Linear(in_features=4, out_features=5, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
)
[device(type='cuda', index=0), device(type='cuda', index=1)]
before running the cycle
[tensor([[ 0.7998,  0.4847,  0.2964],
        [-0.7857,  0.7675,  0.4063]], device='cuda:0'), tensor([[ 0.7393,  0.6639, -0.2285],
        [ 0.3869,  1.3758,  0.6423]], device='cuda:0'), tensor([[ 1.7320, -0.3938,  0.0087],
        [-0.4178,  0.3598, -0.3449]], device='cuda:0'), tensor([[ 0.8472,  0.9242,  0.4218],
        [-0.1494,  0.5751,  1.0435]], device='cuda:0'), tensor([[ 0.4357,  0.4964, -0.7170],
        [ 1.2200, -0.2328,  0.0183]], device='cuda:0'), tensor([[ 0.0270,  0.7764, -1.5153],
        [ 0.4751,  0.7950, -0.2371]], device='cuda:0'), tensor([[ 2.8779, -0.2233,  1.0237],
        [ 0.3467,  0.3232, -1.4147]], device='cuda:0'), tensor([[-0.2048,  0.8317,  0.2259],
        [-1.3181,  1.0681, -0.6035]], device='cuda:0'), tensor([[ 0.1484,  0.7391,  0.7846],
        [-0.9152, -0.9253,  2.2029]], device='cuda:0'), tensor([[-0.2809,  0.8067,  1.2320],
        [-0.8277,  0.1253,  0.2996]], device='cuda:0'), tensor([[-1.0984,  0.5757,  0.0290],
        [ 1.4562, -0.6809, -1.0192]], device='cuda:0'), tensor([[-1.3126, -1.6877,  0.3706],
        [-0.7661,  0.5762,  3.4434]], device='cuda:0'), tensor([[ 0.2690, -0.2105, -1.3864],
        [-0.0595, -1.3690,  0.6044]], device='cuda:0'), tensor([[ 1.2536, -0.4056, -0.2107],
        [-1.0577,  0.1080,  0.3876]], device='cuda:0'), tensor([[ 0.7018,  0.6763, -0.0045],
        [-1.3745, -0.0693,  1.2229]], device='cuda:0'), tensor([[ 0.2510,  0.8201,  0.6696],
        [-0.8806, -1.0099, -1.2571]], device='cuda:0')]
====================
schedule for this step is
[(0, 0)]
inputting microbatch 0 into partition 0
before moving to cuda:0: tensor([[ 0.7998,  0.4847,  0.2964],
        [-0.7857,  0.7675,  0.4063]], device='cuda:0')
after: tensor([[ 0.7998,  0.4847,  0.2964],
        [-0.7857,  0.7675,  0.4063]], device='cuda:0')
********************
observing microbatch 0
current batch shape is torch.Size([2, 3])
batch is tensor([[ 0.7998,  0.4847,  0.2964],
        [-0.7857,  0.7675,  0.4063]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 0
********************
receiving microbatch 0 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 0, tensor([[0.7011, 0.4149, 0.4021, 0.4576],
        [0.5170, 0.4149, 0.6056, 0.6213]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.7011, 0.4149, 0.4021, 0.4576],
        [0.5170, 0.4149, 0.6056, 0.6213]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.7011, 0.4149, 0.4021, 0.4576],
        [0.5170, 0.4149, 0.6056, 0.6213]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.7011, 0.4149, 0.4021, 0.4576],
        [0.5170, 0.4149, 0.6056, 0.6213]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.7393,  0.6639, -0.2285],
        [ 0.3869,  1.3758,  0.6423]], device='cuda:0'), tensor([[ 1.7320, -0.3938,  0.0087],
        [-0.4178,  0.3598, -0.3449]], device='cuda:0'), tensor([[ 0.8472,  0.9242,  0.4218],
        [-0.1494,  0.5751,  1.0435]], device='cuda:0'), tensor([[ 0.4357,  0.4964, -0.7170],
        [ 1.2200, -0.2328,  0.0183]], device='cuda:0'), tensor([[ 0.0270,  0.7764, -1.5153],
        [ 0.4751,  0.7950, -0.2371]], device='cuda:0'), tensor([[ 2.8779, -0.2233,  1.0237],
        [ 0.3467,  0.3232, -1.4147]], device='cuda:0'), tensor([[-0.2048,  0.8317,  0.2259],
        [-1.3181,  1.0681, -0.6035]], device='cuda:0'), tensor([[ 0.1484,  0.7391,  0.7846],
        [-0.9152, -0.9253,  2.2029]], device='cuda:0'), tensor([[-0.2809,  0.8067,  1.2320],
        [-0.8277,  0.1253,  0.2996]], device='cuda:0'), tensor([[-1.0984,  0.5757,  0.0290],
        [ 1.4562, -0.6809, -1.0192]], device='cuda:0'), tensor([[-1.3126, -1.6877,  0.3706],
        [-0.7661,  0.5762,  3.4434]], device='cuda:0'), tensor([[ 0.2690, -0.2105, -1.3864],
        [-0.0595, -1.3690,  0.6044]], device='cuda:0'), tensor([[ 1.2536, -0.4056, -0.2107],
        [-1.0577,  0.1080,  0.3876]], device='cuda:0'), tensor([[ 0.7018,  0.6763, -0.0045],
        [-1.3745, -0.0693,  1.2229]], device='cuda:0'), tensor([[ 0.2510,  0.8201,  0.6696],
        [-0.8806, -1.0099, -1.2571]], device='cuda:0')]
====================
schedule for this step is
[(1, 0), (0, 1)]
inputting microbatch 1 into partition 0
before moving to cuda:0: tensor([[ 0.7393,  0.6639, -0.2285],
        [ 0.3869,  1.3758,  0.6423]], device='cuda:0')
after: tensor([[ 0.7393,  0.6639, -0.2285],
        [ 0.3869,  1.3758,  0.6423]], device='cuda:0')
********************
observing microbatch 1
current batch shape is torch.Size([2, 3])
batch is tensor([[ 0.7393,  0.6639, -0.2285],
        [ 0.3869,  1.3758,  0.6423]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 1
********************
inputting microbatch 0 into partition 1
before moving to cuda:1: tensor([[0.7011, 0.4149, 0.4021, 0.4576],
        [0.5170, 0.4149, 0.6056, 0.6213]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[-3.6893e+19,  1.8645e+00, -2.0000e+00,  1.7877e+00],
        [-2.0000e+00,  1.7877e+00, -3.6893e+19,  1.8569e+00]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 0
current batch shape is torch.Size([2, 4])
batch is tensor([[-3.6893e+19,  1.8645e+00, -2.0000e+00,  1.7877e+00],
        [-2.0000e+00,  1.7877e+00, -3.6893e+19,  1.8569e+00]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 0
********************
receiving microbatch 1 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 1, tensor([[0.6658, 0.4635, 0.4585, 0.4201],
        [0.6218, 0.4402, 0.3950, 0.6272]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 1
result is tensor([[0.6658, 0.4635, 0.4585, 0.4201],
        [0.6218, 0.4402, 0.3950, 0.6272]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6658, 0.4635, 0.4585, 0.4201],
        [0.6218, 0.4402, 0.3950, 0.6272]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 0 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 0, tensor([[0.0000, 0.0000, 0.0000, 0.9285, 0.6254],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.0000, 0.0000, 0.0000, 0.9285, 0.6254],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.0000, 0.0000, 0.9285, 0.6254],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.0000, 0.0000, 0.0000, 0.9285, 0.6254],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6658, 0.4635, 0.4585, 0.4201],
        [0.6218, 0.4402, 0.3950, 0.6272]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 1.7320, -0.3938,  0.0087],
        [-0.4178,  0.3598, -0.3449]], device='cuda:0'), tensor([[ 0.8472,  0.9242,  0.4218],
        [-0.1494,  0.5751,  1.0435]], device='cuda:0'), tensor([[ 0.4357,  0.4964, -0.7170],
        [ 1.2200, -0.2328,  0.0183]], device='cuda:0'), tensor([[ 0.0270,  0.7764, -1.5153],
        [ 0.4751,  0.7950, -0.2371]], device='cuda:0'), tensor([[ 2.8779, -0.2233,  1.0237],
        [ 0.3467,  0.3232, -1.4147]], device='cuda:0'), tensor([[-0.2048,  0.8317,  0.2259],
        [-1.3181,  1.0681, -0.6035]], device='cuda:0'), tensor([[ 0.1484,  0.7391,  0.7846],
        [-0.9152, -0.9253,  2.2029]], device='cuda:0'), tensor([[-0.2809,  0.8067,  1.2320],
        [-0.8277,  0.1253,  0.2996]], device='cuda:0'), tensor([[-1.0984,  0.5757,  0.0290],
        [ 1.4562, -0.6809, -1.0192]], device='cuda:0'), tensor([[-1.3126, -1.6877,  0.3706],
        [-0.7661,  0.5762,  3.4434]], device='cuda:0'), tensor([[ 0.2690, -0.2105, -1.3864],
        [-0.0595, -1.3690,  0.6044]], device='cuda:0'), tensor([[ 1.2536, -0.4056, -0.2107],
        [-1.0577,  0.1080,  0.3876]], device='cuda:0'), tensor([[ 0.7018,  0.6763, -0.0045],
        [-1.3745, -0.0693,  1.2229]], device='cuda:0'), tensor([[ 0.2510,  0.8201,  0.6696],
        [-0.8806, -1.0099, -1.2571]], device='cuda:0')]
====================
schedule for this step is
[(2, 0), (1, 1)]
inputting microbatch 2 into partition 0
before moving to cuda:0: tensor([[ 1.7320, -0.3938,  0.0087],
        [-0.4178,  0.3598, -0.3449]], device='cuda:0')
after: tensor([[ 1.7320, -0.3938,  0.0087],
        [-0.4178,  0.3598, -0.3449]], device='cuda:0')
********************
observing microbatch 2
current batch shape is torch.Size([2, 3])
batch is tensor([[ 1.7320, -0.3938,  0.0087],
        [-0.4178,  0.3598, -0.3449]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 2
********************
inputting microbatch 1 into partition 1
before moving to cuda:1: tensor([[0.6658, 0.4635, 0.4585, 0.4201],
        [0.6218, 0.4402, 0.3950, 0.6272]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9285, 0.6254, 0.0000, 0.9285],
        [0.6254, 0.0000, 0.0000, 0.0000]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 1
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9285, 0.6254, 0.0000, 0.9285],
        [0.6254, 0.0000, 0.0000, 0.0000]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 1
********************
receiving microbatch 2 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 2, tensor([[0.8108, 0.3896, 0.3337, 0.2718],
        [0.5527, 0.4474, 0.6430, 0.4603]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 2
result is tensor([[0.8108, 0.3896, 0.3337, 0.2718],
        [0.5527, 0.4474, 0.6430, 0.4603]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.8108, 0.3896, 0.3337, 0.2718],
        [0.5527, 0.4474, 0.6430, 0.4603]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 1 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 1, tensor([[0.3739, 0.9530, 0.8405, 0.9030, 0.7782],
        [0.1967, 0.8998, 0.7423, 0.7166, 0.6572]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 1
result is tensor([[0.3739, 0.9530, 0.8405, 0.9030, 0.7782],
        [0.1967, 0.8998, 0.7423, 0.7166, 0.6572]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3739, 0.9530, 0.8405, 0.9030, 0.7782],
        [0.1967, 0.8998, 0.7423, 0.7166, 0.6572]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.0000, 0.0000, 0.0000, 0.9285, 0.6254],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3739, 0.9530, 0.8405, 0.9030, 0.7782],
        [0.1967, 0.8998, 0.7423, 0.7166, 0.6572]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8108, 0.3896, 0.3337, 0.2718],
        [0.5527, 0.4474, 0.6430, 0.4603]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.8472,  0.9242,  0.4218],
        [-0.1494,  0.5751,  1.0435]], device='cuda:0'), tensor([[ 0.4357,  0.4964, -0.7170],
        [ 1.2200, -0.2328,  0.0183]], device='cuda:0'), tensor([[ 0.0270,  0.7764, -1.5153],
        [ 0.4751,  0.7950, -0.2371]], device='cuda:0'), tensor([[ 2.8779, -0.2233,  1.0237],
        [ 0.3467,  0.3232, -1.4147]], device='cuda:0'), tensor([[-0.2048,  0.8317,  0.2259],
        [-1.3181,  1.0681, -0.6035]], device='cuda:0'), tensor([[ 0.1484,  0.7391,  0.7846],
        [-0.9152, -0.9253,  2.2029]], device='cuda:0'), tensor([[-0.2809,  0.8067,  1.2320],
        [-0.8277,  0.1253,  0.2996]], device='cuda:0'), tensor([[-1.0984,  0.5757,  0.0290],
        [ 1.4562, -0.6809, -1.0192]], device='cuda:0'), tensor([[-1.3126, -1.6877,  0.3706],
        [-0.7661,  0.5762,  3.4434]], device='cuda:0'), tensor([[ 0.2690, -0.2105, -1.3864],
        [-0.0595, -1.3690,  0.6044]], device='cuda:0'), tensor([[ 1.2536, -0.4056, -0.2107],
        [-1.0577,  0.1080,  0.3876]], device='cuda:0'), tensor([[ 0.7018,  0.6763, -0.0045],
        [-1.3745, -0.0693,  1.2229]], device='cuda:0'), tensor([[ 0.2510,  0.8201,  0.6696],
        [-0.8806, -1.0099, -1.2571]], device='cuda:0')]
====================
schedule for this step is
[(3, 0), (2, 1)]
inputting microbatch 3 into partition 0
before moving to cuda:0: tensor([[ 0.8472,  0.9242,  0.4218],
        [-0.1494,  0.5751,  1.0435]], device='cuda:0')
after: tensor([[ 0.8472,  0.9242,  0.4218],
        [-0.1494,  0.5751,  1.0435]], device='cuda:0')
********************
observing microbatch 3
current batch shape is torch.Size([2, 3])
batch is tensor([[ 0.8472,  0.9242,  0.4218],
        [-0.1494,  0.5751,  1.0435]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 3
********************
inputting microbatch 2 into partition 1
before moving to cuda:1: tensor([[0.8108, 0.3896, 0.3337, 0.2718],
        [0.5527, 0.4474, 0.6430, 0.4603]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.3739, 0.9530, 0.8405, 0.9030],
        [0.7782, 0.1967, 0.8998, 0.7423]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 2
current batch shape is torch.Size([2, 4])
batch is tensor([[0.3739, 0.9530, 0.8405, 0.9030],
        [0.7782, 0.1967, 0.8998, 0.7423]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 2
********************
receiving microbatch 3 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 3, tensor([[0.6869, 0.4320, 0.3690, 0.5180],
        [0.6272, 0.3625, 0.4557, 0.6270]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 3
result is tensor([[0.6869, 0.4320, 0.3690, 0.5180],
        [0.6272, 0.3625, 0.4557, 0.6270]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6869, 0.4320, 0.3690, 0.5180],
        [0.6272, 0.3625, 0.4557, 0.6270]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 2 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 2, tensor([[0.4285, 0.9781, 0.9107, 0.9510, 0.8564],
        [0.5112, 0.9669, 0.8957, 0.9135, 0.8480]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 2
result is tensor([[0.4285, 0.9781, 0.9107, 0.9510, 0.8564],
        [0.5112, 0.9669, 0.8957, 0.9135, 0.8480]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4285, 0.9781, 0.9107, 0.9510, 0.8564],
        [0.5112, 0.9669, 0.8957, 0.9135, 0.8480]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.0000, 0.0000, 0.0000, 0.9285, 0.6254],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3739, 0.9530, 0.8405, 0.9030, 0.7782],
        [0.1967, 0.8998, 0.7423, 0.7166, 0.6572]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4285, 0.9781, 0.9107, 0.9510, 0.8564],
        [0.5112, 0.9669, 0.8957, 0.9135, 0.8480]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6869, 0.4320, 0.3690, 0.5180],
        [0.6272, 0.3625, 0.4557, 0.6270]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.4357,  0.4964, -0.7170],
        [ 1.2200, -0.2328,  0.0183]], device='cuda:0'), tensor([[ 0.0270,  0.7764, -1.5153],
        [ 0.4751,  0.7950, -0.2371]], device='cuda:0'), tensor([[ 2.8779, -0.2233,  1.0237],
        [ 0.3467,  0.3232, -1.4147]], device='cuda:0'), tensor([[-0.2048,  0.8317,  0.2259],
        [-1.3181,  1.0681, -0.6035]], device='cuda:0'), tensor([[ 0.1484,  0.7391,  0.7846],
        [-0.9152, -0.9253,  2.2029]], device='cuda:0'), tensor([[-0.2809,  0.8067,  1.2320],
        [-0.8277,  0.1253,  0.2996]], device='cuda:0'), tensor([[-1.0984,  0.5757,  0.0290],
        [ 1.4562, -0.6809, -1.0192]], device='cuda:0'), tensor([[-1.3126, -1.6877,  0.3706],
        [-0.7661,  0.5762,  3.4434]], device='cuda:0'), tensor([[ 0.2690, -0.2105, -1.3864],
        [-0.0595, -1.3690,  0.6044]], device='cuda:0'), tensor([[ 1.2536, -0.4056, -0.2107],
        [-1.0577,  0.1080,  0.3876]], device='cuda:0'), tensor([[ 0.7018,  0.6763, -0.0045],
        [-1.3745, -0.0693,  1.2229]], device='cuda:0'), tensor([[ 0.2510,  0.8201,  0.6696],
        [-0.8806, -1.0099, -1.2571]], device='cuda:0')]
====================
schedule for this step is
[(4, 0), (3, 1)]
inputting microbatch 4 into partition 0
before moving to cuda:0: tensor([[ 0.4357,  0.4964, -0.7170],
        [ 1.2200, -0.2328,  0.0183]], device='cuda:0')
after: tensor([[ 0.4357,  0.4964, -0.7170],
        [ 1.2200, -0.2328,  0.0183]], device='cuda:0')
********************
observing microbatch 4
current batch shape is torch.Size([2, 3])
batch is tensor([[ 0.4357,  0.4964, -0.7170],
        [ 1.2200, -0.2328,  0.0183]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 4
********************
inputting microbatch 3 into partition 1
before moving to cuda:1: tensor([[0.6869, 0.4320, 0.3690, 0.5180],
        [0.6272, 0.3625, 0.4557, 0.6270]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.4285, 0.9781, 0.9107, 0.9510],
        [0.8564, 0.5112, 0.9669, 0.8957]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 3
current batch shape is torch.Size([2, 4])
batch is tensor([[0.4285, 0.9781, 0.9107, 0.9510],
        [0.8564, 0.5112, 0.9669, 0.8957]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 3
********************
receiving microbatch 4 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 4, tensor([[0.6241, 0.4879, 0.5587, 0.3686],
        [0.7649, 0.3954, 0.3957, 0.3204]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 4
result is tensor([[0.6241, 0.4879, 0.5587, 0.3686],
        [0.7649, 0.3954, 0.3957, 0.3204]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6241, 0.4879, 0.5587, 0.3686],
        [0.7649, 0.3954, 0.3957, 0.3204]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 3 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 3, tensor([[0.4648, 0.9807, 0.9200, 0.9558, 0.8656],
        [0.5696, 0.9782, 0.9220, 0.9401, 0.8656]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 3
result is tensor([[0.4648, 0.9807, 0.9200, 0.9558, 0.8656],
        [0.5696, 0.9782, 0.9220, 0.9401, 0.8656]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4648, 0.9807, 0.9200, 0.9558, 0.8656],
        [0.5696, 0.9782, 0.9220, 0.9401, 0.8656]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.0000, 0.0000, 0.0000, 0.9285, 0.6254],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3739, 0.9530, 0.8405, 0.9030, 0.7782],
        [0.1967, 0.8998, 0.7423, 0.7166, 0.6572]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4285, 0.9781, 0.9107, 0.9510, 0.8564],
        [0.5112, 0.9669, 0.8957, 0.9135, 0.8480]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4648, 0.9807, 0.9200, 0.9558, 0.8656],
        [0.5696, 0.9782, 0.9220, 0.9401, 0.8656]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6241, 0.4879, 0.5587, 0.3686],
        [0.7649, 0.3954, 0.3957, 0.3204]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.0270,  0.7764, -1.5153],
        [ 0.4751,  0.7950, -0.2371]], device='cuda:0'), tensor([[ 2.8779, -0.2233,  1.0237],
        [ 0.3467,  0.3232, -1.4147]], device='cuda:0'), tensor([[-0.2048,  0.8317,  0.2259],
        [-1.3181,  1.0681, -0.6035]], device='cuda:0'), tensor([[ 0.1484,  0.7391,  0.7846],
        [-0.9152, -0.9253,  2.2029]], device='cuda:0'), tensor([[-0.2809,  0.8067,  1.2320],
        [-0.8277,  0.1253,  0.2996]], device='cuda:0'), tensor([[-1.0984,  0.5757,  0.0290],
        [ 1.4562, -0.6809, -1.0192]], device='cuda:0'), tensor([[-1.3126, -1.6877,  0.3706],
        [-0.7661,  0.5762,  3.4434]], device='cuda:0'), tensor([[ 0.2690, -0.2105, -1.3864],
        [-0.0595, -1.3690,  0.6044]], device='cuda:0'), tensor([[ 1.2536, -0.4056, -0.2107],
        [-1.0577,  0.1080,  0.3876]], device='cuda:0'), tensor([[ 0.7018,  0.6763, -0.0045],
        [-1.3745, -0.0693,  1.2229]], device='cuda:0'), tensor([[ 0.2510,  0.8201,  0.6696],
        [-0.8806, -1.0099, -1.2571]], device='cuda:0')]
====================
schedule for this step is
[(5, 0), (4, 1)]
inputting microbatch 5 into partition 0
before moving to cuda:0: tensor([[ 0.0270,  0.7764, -1.5153],
        [ 0.4751,  0.7950, -0.2371]], device='cuda:0')
after: tensor([[ 0.0270,  0.7764, -1.5153],
        [ 0.4751,  0.7950, -0.2371]], device='cuda:0')
********************
observing microbatch 5
current batch shape is torch.Size([2, 3])
batch is tensor([[ 0.0270,  0.7764, -1.5153],
        [ 0.4751,  0.7950, -0.2371]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 5
********************
inputting microbatch 4 into partition 1
before moving to cuda:1: tensor([[0.6241, 0.4879, 0.5587, 0.3686],
        [0.7649, 0.3954, 0.3957, 0.3204]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.4648, 0.9807, 0.9200, 0.9558],
        [0.8656, 0.5696, 0.9782, 0.9220]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 4
current batch shape is torch.Size([2, 4])
batch is tensor([[0.4648, 0.9807, 0.9200, 0.9558],
        [0.8656, 0.5696, 0.9782, 0.9220]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 4
********************
receiving microbatch 5 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 5, tensor([[0.5276, 0.5613, 0.6828, 0.3385],
        [0.6301, 0.4706, 0.4928, 0.4534]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 5
result is tensor([[0.5276, 0.5613, 0.6828, 0.3385],
        [0.6301, 0.4706, 0.4928, 0.4534]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5276, 0.5613, 0.6828, 0.3385],
        [0.6301, 0.4706, 0.4928, 0.4534]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 4 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 4, tensor([[0.4763, 0.9814, 0.9224, 0.9563, 0.8666],
        [0.5785, 0.9797, 0.9259, 0.9440, 0.8685]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 4
result is tensor([[0.4763, 0.9814, 0.9224, 0.9563, 0.8666],
        [0.5785, 0.9797, 0.9259, 0.9440, 0.8685]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4763, 0.9814, 0.9224, 0.9563, 0.8666],
        [0.5785, 0.9797, 0.9259, 0.9440, 0.8685]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.0000, 0.0000, 0.0000, 0.9285, 0.6254],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3739, 0.9530, 0.8405, 0.9030, 0.7782],
        [0.1967, 0.8998, 0.7423, 0.7166, 0.6572]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4285, 0.9781, 0.9107, 0.9510, 0.8564],
        [0.5112, 0.9669, 0.8957, 0.9135, 0.8480]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4648, 0.9807, 0.9200, 0.9558, 0.8656],
        [0.5696, 0.9782, 0.9220, 0.9401, 0.8656]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4763, 0.9814, 0.9224, 0.9563, 0.8666],
        [0.5785, 0.9797, 0.9259, 0.9440, 0.8685]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5276, 0.5613, 0.6828, 0.3385],
        [0.6301, 0.4706, 0.4928, 0.4534]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 2.8779, -0.2233,  1.0237],
        [ 0.3467,  0.3232, -1.4147]], device='cuda:0'), tensor([[-0.2048,  0.8317,  0.2259],
        [-1.3181,  1.0681, -0.6035]], device='cuda:0'), tensor([[ 0.1484,  0.7391,  0.7846],
        [-0.9152, -0.9253,  2.2029]], device='cuda:0'), tensor([[-0.2809,  0.8067,  1.2320],
        [-0.8277,  0.1253,  0.2996]], device='cuda:0'), tensor([[-1.0984,  0.5757,  0.0290],
        [ 1.4562, -0.6809, -1.0192]], device='cuda:0'), tensor([[-1.3126, -1.6877,  0.3706],
        [-0.7661,  0.5762,  3.4434]], device='cuda:0'), tensor([[ 0.2690, -0.2105, -1.3864],
        [-0.0595, -1.3690,  0.6044]], device='cuda:0'), tensor([[ 1.2536, -0.4056, -0.2107],
        [-1.0577,  0.1080,  0.3876]], device='cuda:0'), tensor([[ 0.7018,  0.6763, -0.0045],
        [-1.3745, -0.0693,  1.2229]], device='cuda:0'), tensor([[ 0.2510,  0.8201,  0.6696],
        [-0.8806, -1.0099, -1.2571]], device='cuda:0')]
====================
schedule for this step is
[(6, 0), (5, 1)]
inputting microbatch 6 into partition 0
before moving to cuda:0: tensor([[ 2.8779, -0.2233,  1.0237],
        [ 0.3467,  0.3232, -1.4147]], device='cuda:0')
after: tensor([[ 2.8779, -0.2233,  1.0237],
        [ 0.3467,  0.3232, -1.4147]], device='cuda:0')
********************
observing microbatch 6
current batch shape is torch.Size([2, 3])
batch is tensor([[ 2.8779, -0.2233,  1.0237],
        [ 0.3467,  0.3232, -1.4147]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 6
********************
inputting microbatch 5 into partition 1
before moving to cuda:1: tensor([[0.5276, 0.5613, 0.6828, 0.3385],
        [0.6301, 0.4706, 0.4928, 0.4534]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.4763, 0.9814, 0.9224, 0.9563],
        [0.8666, 0.5785, 0.9797, 0.9259]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 5
current batch shape is torch.Size([2, 4])
batch is tensor([[0.4763, 0.9814, 0.9224, 0.9563],
        [0.8666, 0.5785, 0.9797, 0.9259]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 5
********************
receiving microbatch 6 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 6, tensor([[0.8925, 0.3356, 0.1421, 0.3154],
        [0.5962, 0.5286, 0.6463, 0.2845]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 6
result is tensor([[0.8925, 0.3356, 0.1421, 0.3154],
        [0.5962, 0.5286, 0.6463, 0.2845]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.8925, 0.3356, 0.1421, 0.3154],
        [0.5962, 0.5286, 0.6463, 0.2845]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 5 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 5, tensor([[0.4797, 0.9816, 0.9232, 0.9565, 0.8669],
        [0.5797, 0.9799, 0.9264, 0.9445, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 5
result is tensor([[0.4797, 0.9816, 0.9232, 0.9565, 0.8669],
        [0.5797, 0.9799, 0.9264, 0.9445, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4797, 0.9816, 0.9232, 0.9565, 0.8669],
        [0.5797, 0.9799, 0.9264, 0.9445, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.0000, 0.0000, 0.0000, 0.9285, 0.6254],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3739, 0.9530, 0.8405, 0.9030, 0.7782],
        [0.1967, 0.8998, 0.7423, 0.7166, 0.6572]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4285, 0.9781, 0.9107, 0.9510, 0.8564],
        [0.5112, 0.9669, 0.8957, 0.9135, 0.8480]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4648, 0.9807, 0.9200, 0.9558, 0.8656],
        [0.5696, 0.9782, 0.9220, 0.9401, 0.8656]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4763, 0.9814, 0.9224, 0.9563, 0.8666],
        [0.5785, 0.9797, 0.9259, 0.9440, 0.8685]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4797, 0.9816, 0.9232, 0.9565, 0.8669],
        [0.5797, 0.9799, 0.9264, 0.9445, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8925, 0.3356, 0.1421, 0.3154],
        [0.5962, 0.5286, 0.6463, 0.2845]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.2048,  0.8317,  0.2259],
        [-1.3181,  1.0681, -0.6035]], device='cuda:0'), tensor([[ 0.1484,  0.7391,  0.7846],
        [-0.9152, -0.9253,  2.2029]], device='cuda:0'), tensor([[-0.2809,  0.8067,  1.2320],
        [-0.8277,  0.1253,  0.2996]], device='cuda:0'), tensor([[-1.0984,  0.5757,  0.0290],
        [ 1.4562, -0.6809, -1.0192]], device='cuda:0'), tensor([[-1.3126, -1.6877,  0.3706],
        [-0.7661,  0.5762,  3.4434]], device='cuda:0'), tensor([[ 0.2690, -0.2105, -1.3864],
        [-0.0595, -1.3690,  0.6044]], device='cuda:0'), tensor([[ 1.2536, -0.4056, -0.2107],
        [-1.0577,  0.1080,  0.3876]], device='cuda:0'), tensor([[ 0.7018,  0.6763, -0.0045],
        [-1.3745, -0.0693,  1.2229]], device='cuda:0'), tensor([[ 0.2510,  0.8201,  0.6696],
        [-0.8806, -1.0099, -1.2571]], device='cuda:0')]
====================
schedule for this step is
[(7, 0), (6, 1)]
inputting microbatch 7 into partition 0
before moving to cuda:0: tensor([[-0.2048,  0.8317,  0.2259],
        [-1.3181,  1.0681, -0.6035]], device='cuda:0')
after: tensor([[-0.2048,  0.8317,  0.2259],
        [-1.3181,  1.0681, -0.6035]], device='cuda:0')
********************
observing microbatch 7
current batch shape is torch.Size([2, 3])
batch is tensor([[-0.2048,  0.8317,  0.2259],
        [-1.3181,  1.0681, -0.6035]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 7
********************
inputting microbatch 6 into partition 1
before moving to cuda:1: tensor([[0.8925, 0.3356, 0.1421, 0.3154],
        [0.5962, 0.5286, 0.6463, 0.2845]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.4797, 0.9816, 0.9232, 0.9565],
        [0.8669, 0.5797, 0.9799, 0.9264]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 6
current batch shape is torch.Size([2, 4])
batch is tensor([[0.4797, 0.9816, 0.9232, 0.9565],
        [0.8669, 0.5797, 0.9799, 0.9264]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 6
********************
receiving microbatch 7 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 7, tensor([[0.5718, 0.4350, 0.5405, 0.5645],
        [0.3948, 0.5041, 0.7534, 0.5767]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 7
result is tensor([[0.5718, 0.4350, 0.5405, 0.5645],
        [0.3948, 0.5041, 0.7534, 0.5767]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5718, 0.4350, 0.5405, 0.5645],
        [0.3948, 0.5041, 0.7534, 0.5767]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 6 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 6, tensor([[0.4807, 0.9816, 0.9234, 0.9565, 0.8669],
        [0.5798, 0.9799, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 6
result is tensor([[0.4807, 0.9816, 0.9234, 0.9565, 0.8669],
        [0.5798, 0.9799, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4807, 0.9816, 0.9234, 0.9565, 0.8669],
        [0.5798, 0.9799, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.0000, 0.0000, 0.0000, 0.9285, 0.6254],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3739, 0.9530, 0.8405, 0.9030, 0.7782],
        [0.1967, 0.8998, 0.7423, 0.7166, 0.6572]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4285, 0.9781, 0.9107, 0.9510, 0.8564],
        [0.5112, 0.9669, 0.8957, 0.9135, 0.8480]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4648, 0.9807, 0.9200, 0.9558, 0.8656],
        [0.5696, 0.9782, 0.9220, 0.9401, 0.8656]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4763, 0.9814, 0.9224, 0.9563, 0.8666],
        [0.5785, 0.9797, 0.9259, 0.9440, 0.8685]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4797, 0.9816, 0.9232, 0.9565, 0.8669],
        [0.5797, 0.9799, 0.9264, 0.9445, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4807, 0.9816, 0.9234, 0.9565, 0.8669],
        [0.5798, 0.9799, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5718, 0.4350, 0.5405, 0.5645],
        [0.3948, 0.5041, 0.7534, 0.5767]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.1484,  0.7391,  0.7846],
        [-0.9152, -0.9253,  2.2029]], device='cuda:0'), tensor([[-0.2809,  0.8067,  1.2320],
        [-0.8277,  0.1253,  0.2996]], device='cuda:0'), tensor([[-1.0984,  0.5757,  0.0290],
        [ 1.4562, -0.6809, -1.0192]], device='cuda:0'), tensor([[-1.3126, -1.6877,  0.3706],
        [-0.7661,  0.5762,  3.4434]], device='cuda:0'), tensor([[ 0.2690, -0.2105, -1.3864],
        [-0.0595, -1.3690,  0.6044]], device='cuda:0'), tensor([[ 1.2536, -0.4056, -0.2107],
        [-1.0577,  0.1080,  0.3876]], device='cuda:0'), tensor([[ 0.7018,  0.6763, -0.0045],
        [-1.3745, -0.0693,  1.2229]], device='cuda:0'), tensor([[ 0.2510,  0.8201,  0.6696],
        [-0.8806, -1.0099, -1.2571]], device='cuda:0')]
====================
schedule for this step is
[(8, 0), (7, 1)]
inputting microbatch 8 into partition 0
before moving to cuda:0: tensor([[ 0.1484,  0.7391,  0.7846],
        [-0.9152, -0.9253,  2.2029]], device='cuda:0')
after: tensor([[ 0.1484,  0.7391,  0.7846],
        [-0.9152, -0.9253,  2.2029]], device='cuda:0')
********************
observing microbatch 8
current batch shape is torch.Size([2, 3])
batch is tensor([[ 0.1484,  0.7391,  0.7846],
        [-0.9152, -0.9253,  2.2029]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 8
********************
inputting microbatch 7 into partition 1
before moving to cuda:1: tensor([[0.5718, 0.4350, 0.5405, 0.5645],
        [0.3948, 0.5041, 0.7534, 0.5767]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.4807, 0.9816, 0.9234, 0.9565],
        [0.8669, 0.5798, 0.9799, 0.9265]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 7
current batch shape is torch.Size([2, 4])
batch is tensor([[0.4807, 0.9816, 0.9234, 0.9565],
        [0.8669, 0.5798, 0.9799, 0.9265]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 7
********************
receiving microbatch 8 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 8, tensor([[0.6392, 0.3914, 0.4348, 0.5932],
        [0.6779, 0.2163, 0.4952, 0.6546]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 8
result is tensor([[0.6392, 0.3914, 0.4348, 0.5932],
        [0.6779, 0.2163, 0.4952, 0.6546]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6392, 0.3914, 0.4348, 0.5932],
        [0.6779, 0.2163, 0.4952, 0.6546]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 7 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 7, tensor([[0.4810, 0.9816, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 7
result is tensor([[0.4810, 0.9816, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4810, 0.9816, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.0000, 0.0000, 0.0000, 0.9285, 0.6254],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3739, 0.9530, 0.8405, 0.9030, 0.7782],
        [0.1967, 0.8998, 0.7423, 0.7166, 0.6572]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4285, 0.9781, 0.9107, 0.9510, 0.8564],
        [0.5112, 0.9669, 0.8957, 0.9135, 0.8480]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4648, 0.9807, 0.9200, 0.9558, 0.8656],
        [0.5696, 0.9782, 0.9220, 0.9401, 0.8656]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4763, 0.9814, 0.9224, 0.9563, 0.8666],
        [0.5785, 0.9797, 0.9259, 0.9440, 0.8685]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4797, 0.9816, 0.9232, 0.9565, 0.8669],
        [0.5797, 0.9799, 0.9264, 0.9445, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4807, 0.9816, 0.9234, 0.9565, 0.8669],
        [0.5798, 0.9799, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4810, 0.9816, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6392, 0.3914, 0.4348, 0.5932],
        [0.6779, 0.2163, 0.4952, 0.6546]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.2809,  0.8067,  1.2320],
        [-0.8277,  0.1253,  0.2996]], device='cuda:0'), tensor([[-1.0984,  0.5757,  0.0290],
        [ 1.4562, -0.6809, -1.0192]], device='cuda:0'), tensor([[-1.3126, -1.6877,  0.3706],
        [-0.7661,  0.5762,  3.4434]], device='cuda:0'), tensor([[ 0.2690, -0.2105, -1.3864],
        [-0.0595, -1.3690,  0.6044]], device='cuda:0'), tensor([[ 1.2536, -0.4056, -0.2107],
        [-1.0577,  0.1080,  0.3876]], device='cuda:0'), tensor([[ 0.7018,  0.6763, -0.0045],
        [-1.3745, -0.0693,  1.2229]], device='cuda:0'), tensor([[ 0.2510,  0.8201,  0.6696],
        [-0.8806, -1.0099, -1.2571]], device='cuda:0')]
====================
schedule for this step is
[(9, 0), (8, 1)]
inputting microbatch 9 into partition 0
before moving to cuda:0: tensor([[-0.2809,  0.8067,  1.2320],
        [-0.8277,  0.1253,  0.2996]], device='cuda:0')
after: tensor([[-0.2809,  0.8067,  1.2320],
        [-0.8277,  0.1253,  0.2996]], device='cuda:0')
********************
observing microbatch 9
current batch shape is torch.Size([2, 3])
batch is tensor([[-0.2809,  0.8067,  1.2320],
        [-0.8277,  0.1253,  0.2996]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 9
********************
inputting microbatch 8 into partition 1
before moving to cuda:1: tensor([[0.6392, 0.3914, 0.4348, 0.5932],
        [0.6779, 0.2163, 0.4952, 0.6546]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.4810, 0.9816, 0.9235, 0.9565],
        [0.8669, 0.5799, 0.9800, 0.9265]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 8
current batch shape is torch.Size([2, 4])
batch is tensor([[0.4810, 0.9816, 0.9235, 0.9565],
        [0.8669, 0.5799, 0.9800, 0.9265]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 8
********************
receiving microbatch 9 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 9, tensor([[0.6067, 0.3618, 0.4470, 0.6799],
        [0.5479, 0.3851, 0.6423, 0.5425]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 9
result is tensor([[0.6067, 0.3618, 0.4470, 0.6799],
        [0.5479, 0.3851, 0.6423, 0.5425]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6067, 0.3618, 0.4470, 0.6799],
        [0.5479, 0.3851, 0.6423, 0.5425]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 8 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 8, tensor([[0.4810, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 8
result is tensor([[0.4810, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4810, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.0000, 0.0000, 0.0000, 0.9285, 0.6254],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3739, 0.9530, 0.8405, 0.9030, 0.7782],
        [0.1967, 0.8998, 0.7423, 0.7166, 0.6572]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4285, 0.9781, 0.9107, 0.9510, 0.8564],
        [0.5112, 0.9669, 0.8957, 0.9135, 0.8480]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4648, 0.9807, 0.9200, 0.9558, 0.8656],
        [0.5696, 0.9782, 0.9220, 0.9401, 0.8656]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4763, 0.9814, 0.9224, 0.9563, 0.8666],
        [0.5785, 0.9797, 0.9259, 0.9440, 0.8685]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4797, 0.9816, 0.9232, 0.9565, 0.8669],
        [0.5797, 0.9799, 0.9264, 0.9445, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4807, 0.9816, 0.9234, 0.9565, 0.8669],
        [0.5798, 0.9799, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4810, 0.9816, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4810, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6067, 0.3618, 0.4470, 0.6799],
        [0.5479, 0.3851, 0.6423, 0.5425]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-1.0984,  0.5757,  0.0290],
        [ 1.4562, -0.6809, -1.0192]], device='cuda:0'), tensor([[-1.3126, -1.6877,  0.3706],
        [-0.7661,  0.5762,  3.4434]], device='cuda:0'), tensor([[ 0.2690, -0.2105, -1.3864],
        [-0.0595, -1.3690,  0.6044]], device='cuda:0'), tensor([[ 1.2536, -0.4056, -0.2107],
        [-1.0577,  0.1080,  0.3876]], device='cuda:0'), tensor([[ 0.7018,  0.6763, -0.0045],
        [-1.3745, -0.0693,  1.2229]], device='cuda:0'), tensor([[ 0.2510,  0.8201,  0.6696],
        [-0.8806, -1.0099, -1.2571]], device='cuda:0')]
====================
schedule for this step is
[(10, 0), (9, 1)]
inputting microbatch 10 into partition 0
before moving to cuda:0: tensor([[-1.0984,  0.5757,  0.0290],
        [ 1.4562, -0.6809, -1.0192]], device='cuda:0')
after: tensor([[-1.0984,  0.5757,  0.0290],
        [ 1.4562, -0.6809, -1.0192]], device='cuda:0')
********************
observing microbatch 10
current batch shape is torch.Size([2, 3])
batch is tensor([[-1.0984,  0.5757,  0.0290],
        [ 1.4562, -0.6809, -1.0192]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 10
********************
inputting microbatch 9 into partition 1
before moving to cuda:1: tensor([[0.6067, 0.3618, 0.4470, 0.6799],
        [0.5479, 0.3851, 0.6423, 0.5425]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.4810, 0.9817, 0.9235, 0.9565],
        [0.8669, 0.5799, 0.9800, 0.9265]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 9
current batch shape is torch.Size([2, 4])
batch is tensor([[0.4810, 0.9817, 0.9235, 0.9565],
        [0.8669, 0.5799, 0.9800, 0.9265]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 9
********************
receiving microbatch 10 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 10, tensor([[0.4768, 0.4292, 0.6884, 0.5805],
        [0.7726, 0.4452, 0.4843, 0.1793]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 10
result is tensor([[0.4768, 0.4292, 0.6884, 0.5805],
        [0.7726, 0.4452, 0.4843, 0.1793]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4768, 0.4292, 0.6884, 0.5805],
        [0.7726, 0.4452, 0.4843, 0.1793]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 9 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 9, tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 9
result is tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.0000, 0.0000, 0.0000, 0.9285, 0.6254],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3739, 0.9530, 0.8405, 0.9030, 0.7782],
        [0.1967, 0.8998, 0.7423, 0.7166, 0.6572]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4285, 0.9781, 0.9107, 0.9510, 0.8564],
        [0.5112, 0.9669, 0.8957, 0.9135, 0.8480]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4648, 0.9807, 0.9200, 0.9558, 0.8656],
        [0.5696, 0.9782, 0.9220, 0.9401, 0.8656]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4763, 0.9814, 0.9224, 0.9563, 0.8666],
        [0.5785, 0.9797, 0.9259, 0.9440, 0.8685]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4797, 0.9816, 0.9232, 0.9565, 0.8669],
        [0.5797, 0.9799, 0.9264, 0.9445, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4807, 0.9816, 0.9234, 0.9565, 0.8669],
        [0.5798, 0.9799, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4810, 0.9816, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4810, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4768, 0.4292, 0.6884, 0.5805],
        [0.7726, 0.4452, 0.4843, 0.1793]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-1.3126, -1.6877,  0.3706],
        [-0.7661,  0.5762,  3.4434]], device='cuda:0'), tensor([[ 0.2690, -0.2105, -1.3864],
        [-0.0595, -1.3690,  0.6044]], device='cuda:0'), tensor([[ 1.2536, -0.4056, -0.2107],
        [-1.0577,  0.1080,  0.3876]], device='cuda:0'), tensor([[ 0.7018,  0.6763, -0.0045],
        [-1.3745, -0.0693,  1.2229]], device='cuda:0'), tensor([[ 0.2510,  0.8201,  0.6696],
        [-0.8806, -1.0099, -1.2571]], device='cuda:0')]
====================
schedule for this step is
[(11, 0), (10, 1)]
inputting microbatch 11 into partition 0
before moving to cuda:0: tensor([[-1.3126, -1.6877,  0.3706],
        [-0.7661,  0.5762,  3.4434]], device='cuda:0')
after: tensor([[-1.3126, -1.6877,  0.3706],
        [-0.7661,  0.5762,  3.4434]], device='cuda:0')
********************
observing microbatch 11
current batch shape is torch.Size([2, 3])
batch is tensor([[-1.3126, -1.6877,  0.3706],
        [-0.7661,  0.5762,  3.4434]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 11
********************
inputting microbatch 10 into partition 1
before moving to cuda:1: tensor([[0.4768, 0.4292, 0.6884, 0.5805],
        [0.7726, 0.4452, 0.4843, 0.1793]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.4811, 0.9817, 0.9235, 0.9565],
        [0.8669, 0.5799, 0.9800, 0.9265]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 10
current batch shape is torch.Size([2, 4])
batch is tensor([[0.4811, 0.9817, 0.9235, 0.9565],
        [0.8669, 0.5799, 0.9800, 0.9265]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 10
********************
receiving microbatch 11 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 11, tensor([[0.6076, 0.2815, 0.7467, 0.3878],
        [0.6566, 0.2159, 0.3036, 0.8651]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 11
result is tensor([[0.6076, 0.2815, 0.7467, 0.3878],
        [0.6566, 0.2159, 0.3036, 0.8651]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6076, 0.2815, 0.7467, 0.3878],
        [0.6566, 0.2159, 0.3036, 0.8651]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 10 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 10, tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 10
result is tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.0000, 0.0000, 0.0000, 0.9285, 0.6254],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3739, 0.9530, 0.8405, 0.9030, 0.7782],
        [0.1967, 0.8998, 0.7423, 0.7166, 0.6572]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4285, 0.9781, 0.9107, 0.9510, 0.8564],
        [0.5112, 0.9669, 0.8957, 0.9135, 0.8480]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4648, 0.9807, 0.9200, 0.9558, 0.8656],
        [0.5696, 0.9782, 0.9220, 0.9401, 0.8656]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4763, 0.9814, 0.9224, 0.9563, 0.8666],
        [0.5785, 0.9797, 0.9259, 0.9440, 0.8685]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4797, 0.9816, 0.9232, 0.9565, 0.8669],
        [0.5797, 0.9799, 0.9264, 0.9445, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4807, 0.9816, 0.9234, 0.9565, 0.8669],
        [0.5798, 0.9799, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4810, 0.9816, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4810, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6076, 0.2815, 0.7467, 0.3878],
        [0.6566, 0.2159, 0.3036, 0.8651]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.2690, -0.2105, -1.3864],
        [-0.0595, -1.3690,  0.6044]], device='cuda:0'), tensor([[ 1.2536, -0.4056, -0.2107],
        [-1.0577,  0.1080,  0.3876]], device='cuda:0'), tensor([[ 0.7018,  0.6763, -0.0045],
        [-1.3745, -0.0693,  1.2229]], device='cuda:0'), tensor([[ 0.2510,  0.8201,  0.6696],
        [-0.8806, -1.0099, -1.2571]], device='cuda:0')]
====================
schedule for this step is
[(12, 0), (11, 1)]
inputting microbatch 12 into partition 0
before moving to cuda:0: tensor([[ 0.2690, -0.2105, -1.3864],
        [-0.0595, -1.3690,  0.6044]], device='cuda:0')
after: tensor([[ 0.2690, -0.2105, -1.3864],
        [-0.0595, -1.3690,  0.6044]], device='cuda:0')
********************
observing microbatch 12
current batch shape is torch.Size([2, 3])
batch is tensor([[ 0.2690, -0.2105, -1.3864],
        [-0.0595, -1.3690,  0.6044]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 12
********************
inputting microbatch 11 into partition 1
before moving to cuda:1: tensor([[0.6076, 0.2815, 0.7467, 0.3878],
        [0.6566, 0.2159, 0.3036, 0.8651]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.4811, 0.9817, 0.9235, 0.9565],
        [0.8669, 0.5799, 0.9800, 0.9265]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 11
current batch shape is torch.Size([2, 4])
batch is tensor([[0.4811, 0.9817, 0.9235, 0.9565],
        [0.8669, 0.5799, 0.9800, 0.9265]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 11
********************
receiving microbatch 12 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 12, tensor([[0.6208, 0.4939, 0.6701, 0.2454],
        [0.7263, 0.2888, 0.5549, 0.3574]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 12
result is tensor([[0.6208, 0.4939, 0.6701, 0.2454],
        [0.7263, 0.2888, 0.5549, 0.3574]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6208, 0.4939, 0.6701, 0.2454],
        [0.7263, 0.2888, 0.5549, 0.3574]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 11 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 11, tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 11
result is tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.0000, 0.0000, 0.0000, 0.9285, 0.6254],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3739, 0.9530, 0.8405, 0.9030, 0.7782],
        [0.1967, 0.8998, 0.7423, 0.7166, 0.6572]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4285, 0.9781, 0.9107, 0.9510, 0.8564],
        [0.5112, 0.9669, 0.8957, 0.9135, 0.8480]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4648, 0.9807, 0.9200, 0.9558, 0.8656],
        [0.5696, 0.9782, 0.9220, 0.9401, 0.8656]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4763, 0.9814, 0.9224, 0.9563, 0.8666],
        [0.5785, 0.9797, 0.9259, 0.9440, 0.8685]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4797, 0.9816, 0.9232, 0.9565, 0.8669],
        [0.5797, 0.9799, 0.9264, 0.9445, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4807, 0.9816, 0.9234, 0.9565, 0.8669],
        [0.5798, 0.9799, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4810, 0.9816, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4810, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6208, 0.4939, 0.6701, 0.2454],
        [0.7263, 0.2888, 0.5549, 0.3574]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 1.2536, -0.4056, -0.2107],
        [-1.0577,  0.1080,  0.3876]], device='cuda:0'), tensor([[ 0.7018,  0.6763, -0.0045],
        [-1.3745, -0.0693,  1.2229]], device='cuda:0'), tensor([[ 0.2510,  0.8201,  0.6696],
        [-0.8806, -1.0099, -1.2571]], device='cuda:0')]
====================
schedule for this step is
[(13, 0), (12, 1)]
inputting microbatch 13 into partition 0
before moving to cuda:0: tensor([[ 1.2536, -0.4056, -0.2107],
        [-1.0577,  0.1080,  0.3876]], device='cuda:0')
after: tensor([[ 1.2536, -0.4056, -0.2107],
        [-1.0577,  0.1080,  0.3876]], device='cuda:0')
********************
observing microbatch 13
current batch shape is torch.Size([2, 3])
batch is tensor([[ 1.2536, -0.4056, -0.2107],
        [-1.0577,  0.1080,  0.3876]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 13
********************
inputting microbatch 12 into partition 1
before moving to cuda:1: tensor([[0.6208, 0.4939, 0.6701, 0.2454],
        [0.7263, 0.2888, 0.5549, 0.3574]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.4811, 0.9817, 0.9235, 0.9565],
        [0.8669, 0.5799, 0.9800, 0.9265]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 12
current batch shape is torch.Size([2, 4])
batch is tensor([[0.4811, 0.9817, 0.9235, 0.9565],
        [0.8669, 0.5799, 0.9800, 0.9265]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 12
********************
receiving microbatch 13 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 13, tensor([[0.7684, 0.4019, 0.4199, 0.2790],
        [0.5263, 0.3768, 0.6643, 0.5684]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 13
result is tensor([[0.7684, 0.4019, 0.4199, 0.2790],
        [0.5263, 0.3768, 0.6643, 0.5684]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.7684, 0.4019, 0.4199, 0.2790],
        [0.5263, 0.3768, 0.6643, 0.5684]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 12 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 12, tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 12
result is tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.0000, 0.0000, 0.0000, 0.9285, 0.6254],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3739, 0.9530, 0.8405, 0.9030, 0.7782],
        [0.1967, 0.8998, 0.7423, 0.7166, 0.6572]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4285, 0.9781, 0.9107, 0.9510, 0.8564],
        [0.5112, 0.9669, 0.8957, 0.9135, 0.8480]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4648, 0.9807, 0.9200, 0.9558, 0.8656],
        [0.5696, 0.9782, 0.9220, 0.9401, 0.8656]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4763, 0.9814, 0.9224, 0.9563, 0.8666],
        [0.5785, 0.9797, 0.9259, 0.9440, 0.8685]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4797, 0.9816, 0.9232, 0.9565, 0.8669],
        [0.5797, 0.9799, 0.9264, 0.9445, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4807, 0.9816, 0.9234, 0.9565, 0.8669],
        [0.5798, 0.9799, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4810, 0.9816, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4810, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.7684, 0.4019, 0.4199, 0.2790],
        [0.5263, 0.3768, 0.6643, 0.5684]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.7018,  0.6763, -0.0045],
        [-1.3745, -0.0693,  1.2229]], device='cuda:0'), tensor([[ 0.2510,  0.8201,  0.6696],
        [-0.8806, -1.0099, -1.2571]], device='cuda:0')]
====================
schedule for this step is
[(14, 0), (13, 1)]
inputting microbatch 14 into partition 0
before moving to cuda:0: tensor([[ 0.7018,  0.6763, -0.0045],
        [-1.3745, -0.0693,  1.2229]], device='cuda:0')
after: tensor([[ 0.7018,  0.6763, -0.0045],
        [-1.3745, -0.0693,  1.2229]], device='cuda:0')
********************
observing microbatch 14
current batch shape is torch.Size([2, 3])
batch is tensor([[ 0.7018,  0.6763, -0.0045],
        [-1.3745, -0.0693,  1.2229]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 14
********************
inputting microbatch 13 into partition 1
before moving to cuda:1: tensor([[0.7684, 0.4019, 0.4199, 0.2790],
        [0.5263, 0.3768, 0.6643, 0.5684]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.4811, 0.9817, 0.9235, 0.9565],
        [0.8669, 0.5799, 0.9800, 0.9265]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 13
current batch shape is torch.Size([2, 4])
batch is tensor([[0.4811, 0.9817, 0.9235, 0.9565],
        [0.8669, 0.5799, 0.9800, 0.9265]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 13
********************
receiving microbatch 14 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 14, tensor([[0.6698, 0.4476, 0.4402, 0.4506],
        [0.5368, 0.3098, 0.6315, 0.6667]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 14
result is tensor([[0.6698, 0.4476, 0.4402, 0.4506],
        [0.5368, 0.3098, 0.6315, 0.6667]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6698, 0.4476, 0.4402, 0.4506],
        [0.5368, 0.3098, 0.6315, 0.6667]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 13 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 13, tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 13
result is tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.0000, 0.0000, 0.0000, 0.9285, 0.6254],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3739, 0.9530, 0.8405, 0.9030, 0.7782],
        [0.1967, 0.8998, 0.7423, 0.7166, 0.6572]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4285, 0.9781, 0.9107, 0.9510, 0.8564],
        [0.5112, 0.9669, 0.8957, 0.9135, 0.8480]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4648, 0.9807, 0.9200, 0.9558, 0.8656],
        [0.5696, 0.9782, 0.9220, 0.9401, 0.8656]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4763, 0.9814, 0.9224, 0.9563, 0.8666],
        [0.5785, 0.9797, 0.9259, 0.9440, 0.8685]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4797, 0.9816, 0.9232, 0.9565, 0.8669],
        [0.5797, 0.9799, 0.9264, 0.9445, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4807, 0.9816, 0.9234, 0.9565, 0.8669],
        [0.5798, 0.9799, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4810, 0.9816, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4810, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6698, 0.4476, 0.4402, 0.4506],
        [0.5368, 0.3098, 0.6315, 0.6667]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.2510,  0.8201,  0.6696],
        [-0.8806, -1.0099, -1.2571]], device='cuda:0')]
====================
schedule for this step is
[(15, 0), (14, 1)]
inputting microbatch 15 into partition 0
before moving to cuda:0: tensor([[ 0.2510,  0.8201,  0.6696],
        [-0.8806, -1.0099, -1.2571]], device='cuda:0')
after: tensor([[ 0.2510,  0.8201,  0.6696],
        [-0.8806, -1.0099, -1.2571]], device='cuda:0')
********************
observing microbatch 15
current batch shape is torch.Size([2, 3])
batch is tensor([[ 0.2510,  0.8201,  0.6696],
        [-0.8806, -1.0099, -1.2571]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 15
********************
inputting microbatch 14 into partition 1
before moving to cuda:1: tensor([[0.6698, 0.4476, 0.4402, 0.4506],
        [0.5368, 0.3098, 0.6315, 0.6667]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.4811, 0.9817, 0.9235, 0.9565],
        [0.8669, 0.5799, 0.9800, 0.9265]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 14
current batch shape is torch.Size([2, 4])
batch is tensor([[0.4811, 0.9817, 0.9235, 0.9565],
        [0.8669, 0.5799, 0.9800, 0.9265]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 14
********************
receiving microbatch 15 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 15, tensor([[0.6408, 0.4048, 0.4295, 0.5810],
        [0.5463, 0.4301, 0.8059, 0.2562]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 15
result is tensor([[0.6408, 0.4048, 0.4295, 0.5810],
        [0.5463, 0.4301, 0.8059, 0.2562]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6408, 0.4048, 0.4295, 0.5810],
        [0.5463, 0.4301, 0.8059, 0.2562]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 14 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 14, tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 14
result is tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.0000, 0.0000, 0.0000, 0.9285, 0.6254],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3739, 0.9530, 0.8405, 0.9030, 0.7782],
        [0.1967, 0.8998, 0.7423, 0.7166, 0.6572]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4285, 0.9781, 0.9107, 0.9510, 0.8564],
        [0.5112, 0.9669, 0.8957, 0.9135, 0.8480]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4648, 0.9807, 0.9200, 0.9558, 0.8656],
        [0.5696, 0.9782, 0.9220, 0.9401, 0.8656]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4763, 0.9814, 0.9224, 0.9563, 0.8666],
        [0.5785, 0.9797, 0.9259, 0.9440, 0.8685]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4797, 0.9816, 0.9232, 0.9565, 0.8669],
        [0.5797, 0.9799, 0.9264, 0.9445, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4807, 0.9816, 0.9234, 0.9565, 0.8669],
        [0.5798, 0.9799, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4810, 0.9816, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4810, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6408, 0.4048, 0.4295, 0.5810],
        [0.5463, 0.4301, 0.8059, 0.2562]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)]
====================
schedule for this step is
[(15, 1)]
inputting microbatch 15 into partition 1
before moving to cuda:1: tensor([[0.6408, 0.4048, 0.4295, 0.5810],
        [0.5463, 0.4301, 0.8059, 0.2562]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.4811, 0.9817, 0.9235, 0.9565],
        [0.8669, 0.5799, 0.9800, 0.9265]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 15
current batch shape is torch.Size([2, 4])
batch is tensor([[0.4811, 0.9817, 0.9235, 0.9565],
        [0.8669, 0.5799, 0.9800, 0.9265]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 15
********************
receiving microbatch 15 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 15, tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 15
result is tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.0000, 0.0000, 0.0000, 0.9285, 0.6254],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3739, 0.9530, 0.8405, 0.9030, 0.7782],
        [0.1967, 0.8998, 0.7423, 0.7166, 0.6572]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4285, 0.9781, 0.9107, 0.9510, 0.8564],
        [0.5112, 0.9669, 0.8957, 0.9135, 0.8480]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4648, 0.9807, 0.9200, 0.9558, 0.8656],
        [0.5696, 0.9782, 0.9220, 0.9401, 0.8656]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4763, 0.9814, 0.9224, 0.9563, 0.8666],
        [0.5785, 0.9797, 0.9259, 0.9440, 0.8685]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4797, 0.9816, 0.9232, 0.9565, 0.8669],
        [0.5797, 0.9799, 0.9264, 0.9445, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4807, 0.9816, 0.9234, 0.9565, 0.8669],
        [0.5798, 0.9799, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4810, 0.9816, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4810, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4811, 0.9817, 0.9235, 0.9565, 0.8669],
        [0.5799, 0.9800, 0.9265, 0.9446, 0.8689]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)]
finished running cycle
_____________________________ test_forward_0[2-64] _____________________________

batch_size = 64, split_size = 2

    @pytest.mark.a4_2_2
    @pytest.mark.parametrize("batch_size", [1, 16, 32, 64])
    @pytest.mark.parametrize("split_size", [1, 2, 4, 8, 16])
    # @pytest.mark.parametrize("batch_size", [1])
    # @pytest.mark.parametrize("split_size", [1, 2])
    def test_forward_0(batch_size, split_size):
        model = nn.Sequential(
            nn.Linear(3, 4).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
            nn.Linear(4, 5).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
        )
    
        x = torch.randn(batch_size, 3).to('cuda:0')
        y0 = model(x).to('cpu')
    
        # move the last two layer to another device
        model[-2] = model[-2].to('cuda:1')
        model[-1] = WithDevice(nn.Sigmoid(), 'cuda:1')
        pipe = Pipe(model, split_size=split_size)
        y1 = pipe(x).to('cpu')
>       assert torch.allclose(y0, y1)
E       assert False
E        +  where False = <built-in method allclose of type object at 0x153da78cb760>(tensor([[0.4731, 0.3764, 0.6186, 0.4832, 0.2700],\n        [0.4855, 0.3614, 0.5836, 0.4927, 0.2916],\n        [0.4736, 0.3778, 0.6036, 0.4772, 0.2849],\n        [0.4665, 0.3899, 0.6162, 0.4635, 0.2842],\n        [0.4580, 0.3891, 0.6082, 0.4769, 0.2740],\n        [0.4879, 0.3661, 0.5956, 0.4800, 0.2951],\n        [0.4754, 0.3715, 0.5996, 0.4877, 0.2797],\n        [0.4630, 0.3880, 0.6115, 0.4721, 0.2783],\n        [0.4686, 0.3872, 0.6157, 0.4661, 0.2834],\n        [0.4712, 0.3828, 0.6062, 0.4703, 0.2876],\n        [0.4714, 0.3755, 0.6021, 0.4852, 0.2781],\n        [0.4584, 0.4041, 0.6163, 0.4454, 0.2954],\n        [0.4734, 0.3757, 0.6100, 0.4829, 0.2758],\n        [0.4362, 0.4349, 0.6343, 0.4175, 0.2955],\n        [0.4360, 0.4182, 0.6284, 0.4510, 0.2701],\n        [0.4589, 0.3910, 0.6203, 0.4731, 0.2697],\n        [0.4572, 0.3965, 0.6145, 0.4631, 0.2808],\n        [0.4341, 0.4342, 0.6433, 0.4234, 0.2832],\n        [0.4605, 0.3939, 0.6079, 0.4628, 0.2874],\n        [0.4517, 0.4031, 0.6156, 0.4576, 0.2819],\n        [0.4367, 0.4202, 0.6354, 0.4466, 0.2695],\n        [0.4445, 0.3985, 0.6217, 0.4791, 0.2568],\n        [0.4462, 0.4069, 0.6209, 0.4583, 0.2747],\n        [0.4947, 0.3471, 0.5862, 0.5113, 0.2789]...648, 0.5135, 0.3040],\n        [0.4892, 0.3676, 0.5912, 0.4744, 0.3040],\n        [0.4845, 0.3719, 0.5979, 0.4726, 0.2982],\n        [0.4316, 0.4334, 0.6202, 0.4251, 0.2956],\n        [0.4712, 0.3771, 0.6011, 0.4820, 0.2813],\n        [0.4614, 0.3949, 0.6148, 0.4602, 0.2853],\n        [0.4681, 0.3812, 0.6036, 0.4781, 0.2813],\n        [0.4837, 0.3686, 0.5913, 0.4806, 0.2957],\n        [0.4883, 0.3525, 0.5844, 0.5084, 0.2791],\n        [0.4518, 0.4124, 0.6216, 0.4388, 0.2941],\n        [0.4872, 0.3770, 0.5897, 0.4564, 0.3196],\n        [0.4559, 0.3893, 0.6235, 0.4820, 0.2595],\n        [0.4752, 0.3783, 0.5955, 0.4727, 0.2952],\n        [0.4365, 0.4258, 0.6310, 0.4350, 0.2826],\n        [0.4765, 0.3709, 0.6051, 0.4882, 0.2763],\n        [0.4708, 0.3862, 0.6123, 0.4642, 0.2884],\n        [0.4946, 0.3476, 0.5943, 0.5116, 0.2735],\n        [0.4203, 0.4447, 0.6467, 0.4224, 0.2731],\n        [0.4745, 0.3748, 0.6031, 0.4823, 0.2814],\n        [0.4679, 0.3943, 0.6259, 0.4532, 0.2868],\n        [0.4727, 0.3733, 0.6020, 0.4882, 0.2764],\n        [0.4731, 0.3771, 0.6128, 0.4808, 0.2755],\n        [0.4919, 0.3566, 0.5930, 0.4949, 0.2863],\n        [0.4633, 0.3879, 0.6247, 0.4738, 0.2686]], grad_fn=<ToCopyBackward0>), tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],\n        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515],\n        [0.9622, 0.9626, 0.9957, 0.9984, 0.6515],\n        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515],\n        [0.9638, 0.9642, 0.9959, 0.9985, 0.6515],\n        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515],\n        [0.9639, 0.9643, 0.9960, 0.9985, 0.6515],\n        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515],\n        [0.9639, 0.9643, 0.9960, 0.9985, 0.6515],\n        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515],\n        [0.9639, 0.9643, 0.9960, 0.9985, 0.6515],\n        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515],\n        [0.9639, 0.9643, 0.9960, 0.9985, 0.6515],\n        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515],\n        [0.9639, 0.9643, 0.9960, 0.9985, 0.6515],\n        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515],\n        [0.9639, 0.9643, 0.9960, 0.9985, 0.6515],\n        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515],\n        [0.9639, 0.9643, 0.9960, 0.9985, 0.6515],\n        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515],\n        [0.9639, 0.9643, 0.9960, 0.9985, 0.6515],\n        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515],\n        [0.9639, 0.9643, 0.9960, 0.9985, 0.6515],\n        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]...960, 0.9985, 0.6515],\n        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515],\n        [0.9639, 0.9643, 0.9960, 0.9985, 0.6515],\n        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515],\n        [0.9639, 0.9643, 0.9960, 0.9985, 0.6515],\n        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515],\n        [0.9639, 0.9643, 0.9960, 0.9985, 0.6515],\n        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515],\n        [0.9639, 0.9643, 0.9960, 0.9985, 0.6515],\n        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515],\n        [0.9639, 0.9643, 0.9960, 0.9985, 0.6515],\n        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515],\n        [0.9639, 0.9643, 0.9960, 0.9985, 0.6515],\n        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515],\n        [0.9639, 0.9643, 0.9960, 0.9985, 0.6515],\n        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515],\n        [0.9639, 0.9643, 0.9960, 0.9985, 0.6515],\n        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515],\n        [0.9639, 0.9643, 0.9960, 0.9985, 0.6515],\n        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515],\n        [0.9639, 0.9643, 0.9960, 0.9985, 0.6515],\n        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515],\n        [0.9639, 0.9643, 0.9960, 0.9985, 0.6515],\n        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], grad_fn=<ToCopyBackward0>))
E        +    where <built-in method allclose of type object at 0x153da78cb760> = torch.allclose

batch_size = 64
model      = Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
  (2): Linear(in_features=4, out_features=5, bias=True)
  (3): WithDevice(
    (_module): Sigmoid()
  )
)
pipe       = Pipe(
  (partitions): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=3, out_features=4, bias=True)
   ...inear(in_features=4, out_features=5, bias=True)
      (1): WithDevice(
        (_module): Sigmoid()
      )
    )
  )
)
split_size = 2
x          = tensor([[ 8.6928e-01, -5.6785e-01,  2.3088e+00],
        [-8.7840e-01, -1.4226e+00, -1.5806e+00],
        [-2.2840e-01...0],
        [-6.6777e-01, -1.3018e+00,  1.0124e+00],
        [ 1.0930e+00, -1.9472e-01,  1.9428e+00]], device='cuda:0')
y0         = tensor([[0.4731, 0.3764, 0.6186, 0.4832, 0.2700],
        [0.4855, 0.3614, 0.5836, 0.4927, 0.2916],
        [0.4736, 0...[0.4919, 0.3566, 0.5930, 0.4949, 0.2863],
        [0.4633, 0.3879, 0.6247, 0.4738, 0.2686]], grad_fn=<ToCopyBackward0>)
y1         = tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515],
        [0.9622, 0...[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], grad_fn=<ToCopyBackward0>)

tests/test_pipeline.py:83: AssertionError
----------------------------- Captured stdout call -----------------------------
in init
ModuleList(
  (0): Sequential(
    (0): Linear(in_features=3, out_features=4, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
  (1): Sequential(
    (0): Linear(in_features=4, out_features=5, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
)
[device(type='cuda', index=0), device(type='cuda', index=1)]
before running the cycle
[tensor([[ 0.8693, -0.5679,  2.3088],
        [-0.8784, -1.4226, -1.5806]], device='cuda:0'), tensor([[-0.2284, -0.6220, -0.0288],
        [-0.0658, -0.0531,  0.8998]], device='cuda:0'), tensor([[ 0.7376, -0.5152, -1.1065],
        [-1.1926, -0.8643,  0.8808]], device='cuda:0'), tensor([[ 0.1018, -0.9672, -0.3339],
        [ 0.3764, -0.3448, -0.1502]], device='cuda:0'), tensor([[-0.0526, -0.1305,  1.1132],
        [-0.3683, -0.3995,  0.0282]], device='cuda:0'), tensor([[ 0.2723, -0.8525, -0.4385],
        [-0.6751,  0.4300, -0.2500]], device='cuda:0'), tensor([[ 0.4092, -0.6732,  0.9852],
        [-0.3900,  1.5801, -0.3537]], device='cuda:0'), tensor([[ 1.2932,  0.5636, -0.6612],
        [ 1.0518, -0.2321,  0.7574]], device='cuda:0'), tensor([[ 0.2921, -0.0545, -0.4510],
        [ 0.5752,  1.6007,  1.1187]], device='cuda:0'), tensor([[-0.1838, -0.1619, -1.0705],
        [ 0.2961,  0.1156, -0.9596]], device='cuda:0'), tensor([[ 1.3585,  0.8174,  0.4760],
        [ 2.3039, -0.4182, -0.2226]], device='cuda:0'), tensor([[ 0.8456,  0.2016, -0.7072],
        [-0.1242, -1.9096,  0.3081]], device='cuda:0'), tensor([[-1.4683,  0.3372,  0.8181],
        [ 1.3352,  0.6473, -1.4947]], device='cuda:0'), tensor([[-0.8815, -0.3689,  0.1639],
        [ 0.0284,  0.6285,  0.1271]], device='cuda:0'), tensor([[ 0.3068,  0.6370, -0.5781],
        [ 1.2845, -1.0440, -0.2639]], device='cuda:0'), tensor([[ 0.5464,  1.6256, -0.5388],
        [-0.2515,  0.7066,  0.7183]], device='cuda:0'), tensor([[-0.5179, -0.1676,  1.6046],
        [ 0.4222, -0.3022,  1.5582]], device='cuda:0'), tensor([[ 1.9701, -0.0909,  0.1677],
        [ 0.0402, -1.3000, -0.4937]], device='cuda:0'), tensor([[ 1.3462, -0.4426, -0.9446],
        [ 1.4703, -1.2582, -0.1105]], device='cuda:0'), tensor([[ 0.2497, -0.7672, -1.2856],
        [-0.6834,  1.4657,  0.1942]], device='cuda:0'), tensor([[-2.5381, -2.7670, -0.6195],
        [-1.8058, -0.8120,  0.4175]], device='cuda:0'), tensor([[-1.3229, -0.6276,  0.7311],
        [-0.4062,  1.1249, -2.8920]], device='cuda:0'), tensor([[ 0.0545, -0.7819, -0.6565],
        [-0.0560,  0.0145,  0.0317]], device='cuda:0'), tensor([[ 0.0978, -0.6353, -0.6948],
        [-1.1120, -0.9551, -0.5714]], device='cuda:0'), tensor([[-0.0489, -1.8358, -0.8946],
        [-0.4892,  0.7041, -0.3016]], device='cuda:0'), tensor([[-3.0215, -0.2830, -0.0862],
        [ 1.9947, -0.4977,  1.2364]], device='cuda:0'), tensor([[-0.9189, -0.6557, -1.1053],
        [ 0.4981,  1.0123, -0.4752]], device='cuda:0'), tensor([[ 0.3338, -0.8981,  0.6763],
        [-0.4201, -0.1442,  0.8817]], device='cuda:0'), tensor([[ 0.3618, -1.8074,  1.7514],
        [ 1.4417,  1.8395,  0.6025]], device='cuda:0'), tensor([[-1.4510e-03, -7.6506e-01,  4.9661e-02],
        [-2.7840e-01,  3.7254e-01,  2.5925e+00]], device='cuda:0'), tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0'), tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')]
====================
schedule for this step is
[(0, 0)]
inputting microbatch 0 into partition 0
before moving to cuda:0: tensor([[ 0.8693, -0.5679,  2.3088],
        [-0.8784, -1.4226, -1.5806]], device='cuda:0')
after: tensor([[ 0.8693, -0.5679,  2.3088],
        [-0.8784, -1.4226, -1.5806]], device='cuda:0')
********************
observing microbatch 0
current batch shape is torch.Size([2, 3])
batch is tensor([[ 0.8693, -0.5679,  2.3088],
        [-0.8784, -1.4226, -1.5806]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 0
********************
receiving microbatch 0 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 0, tensor([[0.5233, 0.4749, 0.7358, 0.6911],
        [0.3181, 0.7070, 0.5525, 0.7164]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.5233, 0.4749, 0.7358, 0.6911],
        [0.3181, 0.7070, 0.5525, 0.7164]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5233, 0.4749, 0.7358, 0.6911],
        [0.3181, 0.7070, 0.5525, 0.7164]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.5233, 0.4749, 0.7358, 0.6911],
        [0.3181, 0.7070, 0.5525, 0.7164]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.2284, -0.6220, -0.0288],
        [-0.0658, -0.0531,  0.8998]], device='cuda:0'), tensor([[ 0.7376, -0.5152, -1.1065],
        [-1.1926, -0.8643,  0.8808]], device='cuda:0'), tensor([[ 0.1018, -0.9672, -0.3339],
        [ 0.3764, -0.3448, -0.1502]], device='cuda:0'), tensor([[-0.0526, -0.1305,  1.1132],
        [-0.3683, -0.3995,  0.0282]], device='cuda:0'), tensor([[ 0.2723, -0.8525, -0.4385],
        [-0.6751,  0.4300, -0.2500]], device='cuda:0'), tensor([[ 0.4092, -0.6732,  0.9852],
        [-0.3900,  1.5801, -0.3537]], device='cuda:0'), tensor([[ 1.2932,  0.5636, -0.6612],
        [ 1.0518, -0.2321,  0.7574]], device='cuda:0'), tensor([[ 0.2921, -0.0545, -0.4510],
        [ 0.5752,  1.6007,  1.1187]], device='cuda:0'), tensor([[-0.1838, -0.1619, -1.0705],
        [ 0.2961,  0.1156, -0.9596]], device='cuda:0'), tensor([[ 1.3585,  0.8174,  0.4760],
        [ 2.3039, -0.4182, -0.2226]], device='cuda:0'), tensor([[ 0.8456,  0.2016, -0.7072],
        [-0.1242, -1.9096,  0.3081]], device='cuda:0'), tensor([[-1.4683,  0.3372,  0.8181],
        [ 1.3352,  0.6473, -1.4947]], device='cuda:0'), tensor([[-0.8815, -0.3689,  0.1639],
        [ 0.0284,  0.6285,  0.1271]], device='cuda:0'), tensor([[ 0.3068,  0.6370, -0.5781],
        [ 1.2845, -1.0440, -0.2639]], device='cuda:0'), tensor([[ 0.5464,  1.6256, -0.5388],
        [-0.2515,  0.7066,  0.7183]], device='cuda:0'), tensor([[-0.5179, -0.1676,  1.6046],
        [ 0.4222, -0.3022,  1.5582]], device='cuda:0'), tensor([[ 1.9701, -0.0909,  0.1677],
        [ 0.0402, -1.3000, -0.4937]], device='cuda:0'), tensor([[ 1.3462, -0.4426, -0.9446],
        [ 1.4703, -1.2582, -0.1105]], device='cuda:0'), tensor([[ 0.2497, -0.7672, -1.2856],
        [-0.6834,  1.4657,  0.1942]], device='cuda:0'), tensor([[-2.5381, -2.7670, -0.6195],
        [-1.8058, -0.8120,  0.4175]], device='cuda:0'), tensor([[-1.3229, -0.6276,  0.7311],
        [-0.4062,  1.1249, -2.8920]], device='cuda:0'), tensor([[ 0.0545, -0.7819, -0.6565],
        [-0.0560,  0.0145,  0.0317]], device='cuda:0'), tensor([[ 0.0978, -0.6353, -0.6948],
        [-1.1120, -0.9551, -0.5714]], device='cuda:0'), tensor([[-0.0489, -1.8358, -0.8946],
        [-0.4892,  0.7041, -0.3016]], device='cuda:0'), tensor([[-3.0215, -0.2830, -0.0862],
        [ 1.9947, -0.4977,  1.2364]], device='cuda:0'), tensor([[-0.9189, -0.6557, -1.1053],
        [ 0.4981,  1.0123, -0.4752]], device='cuda:0'), tensor([[ 0.3338, -0.8981,  0.6763],
        [-0.4201, -0.1442,  0.8817]], device='cuda:0'), tensor([[ 0.3618, -1.8074,  1.7514],
        [ 1.4417,  1.8395,  0.6025]], device='cuda:0'), tensor([[-1.4510e-03, -7.6506e-01,  4.9661e-02],
        [-2.7840e-01,  3.7254e-01,  2.5925e+00]], device='cuda:0'), tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0'), tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')]
====================
schedule for this step is
[(1, 0), (0, 1)]
inputting microbatch 1 into partition 0
before moving to cuda:0: tensor([[-0.2284, -0.6220, -0.0288],
        [-0.0658, -0.0531,  0.8998]], device='cuda:0')
after: tensor([[-0.2284, -0.6220, -0.0288],
        [-0.0658, -0.0531,  0.8998]], device='cuda:0')
********************
observing microbatch 1
current batch shape is torch.Size([2, 3])
batch is tensor([[-0.2284, -0.6220, -0.0288],
        [-0.0658, -0.0531,  0.8998]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 1
********************
inputting microbatch 0 into partition 1
before moving to cuda:1: tensor([[0.5233, 0.4749, 0.7358, 0.6911],
        [0.3181, 0.7070, 0.5525, 0.7164]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9446, 0.8689, 0.4811, 0.9817],
        [0.9235, 0.9565, 0.8669, 0.5799]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 0
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9446, 0.8689, 0.4811, 0.9817],
        [0.9235, 0.9565, 0.8669, 0.5799]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 0
********************
receiving microbatch 1 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 1, tensor([[0.4483, 0.5753, 0.5636, 0.6873],
        [0.5329, 0.4987, 0.5284, 0.6827]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 1
result is tensor([[0.4483, 0.5753, 0.5636, 0.6873],
        [0.5329, 0.4987, 0.5284, 0.6827]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4483, 0.5753, 0.5636, 0.6873],
        [0.5329, 0.4987, 0.5284, 0.6827]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 0 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 0, tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4483, 0.5753, 0.5636, 0.6873],
        [0.5329, 0.4987, 0.5284, 0.6827]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.7376, -0.5152, -1.1065],
        [-1.1926, -0.8643,  0.8808]], device='cuda:0'), tensor([[ 0.1018, -0.9672, -0.3339],
        [ 0.3764, -0.3448, -0.1502]], device='cuda:0'), tensor([[-0.0526, -0.1305,  1.1132],
        [-0.3683, -0.3995,  0.0282]], device='cuda:0'), tensor([[ 0.2723, -0.8525, -0.4385],
        [-0.6751,  0.4300, -0.2500]], device='cuda:0'), tensor([[ 0.4092, -0.6732,  0.9852],
        [-0.3900,  1.5801, -0.3537]], device='cuda:0'), tensor([[ 1.2932,  0.5636, -0.6612],
        [ 1.0518, -0.2321,  0.7574]], device='cuda:0'), tensor([[ 0.2921, -0.0545, -0.4510],
        [ 0.5752,  1.6007,  1.1187]], device='cuda:0'), tensor([[-0.1838, -0.1619, -1.0705],
        [ 0.2961,  0.1156, -0.9596]], device='cuda:0'), tensor([[ 1.3585,  0.8174,  0.4760],
        [ 2.3039, -0.4182, -0.2226]], device='cuda:0'), tensor([[ 0.8456,  0.2016, -0.7072],
        [-0.1242, -1.9096,  0.3081]], device='cuda:0'), tensor([[-1.4683,  0.3372,  0.8181],
        [ 1.3352,  0.6473, -1.4947]], device='cuda:0'), tensor([[-0.8815, -0.3689,  0.1639],
        [ 0.0284,  0.6285,  0.1271]], device='cuda:0'), tensor([[ 0.3068,  0.6370, -0.5781],
        [ 1.2845, -1.0440, -0.2639]], device='cuda:0'), tensor([[ 0.5464,  1.6256, -0.5388],
        [-0.2515,  0.7066,  0.7183]], device='cuda:0'), tensor([[-0.5179, -0.1676,  1.6046],
        [ 0.4222, -0.3022,  1.5582]], device='cuda:0'), tensor([[ 1.9701, -0.0909,  0.1677],
        [ 0.0402, -1.3000, -0.4937]], device='cuda:0'), tensor([[ 1.3462, -0.4426, -0.9446],
        [ 1.4703, -1.2582, -0.1105]], device='cuda:0'), tensor([[ 0.2497, -0.7672, -1.2856],
        [-0.6834,  1.4657,  0.1942]], device='cuda:0'), tensor([[-2.5381, -2.7670, -0.6195],
        [-1.8058, -0.8120,  0.4175]], device='cuda:0'), tensor([[-1.3229, -0.6276,  0.7311],
        [-0.4062,  1.1249, -2.8920]], device='cuda:0'), tensor([[ 0.0545, -0.7819, -0.6565],
        [-0.0560,  0.0145,  0.0317]], device='cuda:0'), tensor([[ 0.0978, -0.6353, -0.6948],
        [-1.1120, -0.9551, -0.5714]], device='cuda:0'), tensor([[-0.0489, -1.8358, -0.8946],
        [-0.4892,  0.7041, -0.3016]], device='cuda:0'), tensor([[-3.0215, -0.2830, -0.0862],
        [ 1.9947, -0.4977,  1.2364]], device='cuda:0'), tensor([[-0.9189, -0.6557, -1.1053],
        [ 0.4981,  1.0123, -0.4752]], device='cuda:0'), tensor([[ 0.3338, -0.8981,  0.6763],
        [-0.4201, -0.1442,  0.8817]], device='cuda:0'), tensor([[ 0.3618, -1.8074,  1.7514],
        [ 1.4417,  1.8395,  0.6025]], device='cuda:0'), tensor([[-1.4510e-03, -7.6506e-01,  4.9661e-02],
        [-2.7840e-01,  3.7254e-01,  2.5925e+00]], device='cuda:0'), tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0'), tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')]
====================
schedule for this step is
[(2, 0), (1, 1)]
inputting microbatch 2 into partition 0
before moving to cuda:0: tensor([[ 0.7376, -0.5152, -1.1065],
        [-1.1926, -0.8643,  0.8808]], device='cuda:0')
after: tensor([[ 0.7376, -0.5152, -1.1065],
        [-1.1926, -0.8643,  0.8808]], device='cuda:0')
********************
observing microbatch 2
current batch shape is torch.Size([2, 3])
batch is tensor([[ 0.7376, -0.5152, -1.1065],
        [-1.1926, -0.8643,  0.8808]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 2
********************
inputting microbatch 1 into partition 1
before moving to cuda:1: tensor([[0.4483, 0.5753, 0.5636, 0.6873],
        [0.5329, 0.4987, 0.5284, 0.6827]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9365, 0.9372, 0.9872, 0.9981],
        [0.6515, 0.9393, 0.9395, 0.9886]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 1
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9365, 0.9372, 0.9872, 0.9981],
        [0.6515, 0.9393, 0.9395, 0.9886]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 1
********************
receiving microbatch 2 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 2, tensor([[0.4885, 0.4836, 0.6459, 0.5517],
        [0.3915, 0.6753, 0.4926, 0.7970]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 2
result is tensor([[0.4885, 0.4836, 0.6459, 0.5517],
        [0.3915, 0.6753, 0.4926, 0.7970]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4885, 0.4836, 0.6459, 0.5517],
        [0.3915, 0.6753, 0.4926, 0.7970]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 1 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 1, tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 1
result is tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4885, 0.4836, 0.6459, 0.5517],
        [0.3915, 0.6753, 0.4926, 0.7970]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.1018, -0.9672, -0.3339],
        [ 0.3764, -0.3448, -0.1502]], device='cuda:0'), tensor([[-0.0526, -0.1305,  1.1132],
        [-0.3683, -0.3995,  0.0282]], device='cuda:0'), tensor([[ 0.2723, -0.8525, -0.4385],
        [-0.6751,  0.4300, -0.2500]], device='cuda:0'), tensor([[ 0.4092, -0.6732,  0.9852],
        [-0.3900,  1.5801, -0.3537]], device='cuda:0'), tensor([[ 1.2932,  0.5636, -0.6612],
        [ 1.0518, -0.2321,  0.7574]], device='cuda:0'), tensor([[ 0.2921, -0.0545, -0.4510],
        [ 0.5752,  1.6007,  1.1187]], device='cuda:0'), tensor([[-0.1838, -0.1619, -1.0705],
        [ 0.2961,  0.1156, -0.9596]], device='cuda:0'), tensor([[ 1.3585,  0.8174,  0.4760],
        [ 2.3039, -0.4182, -0.2226]], device='cuda:0'), tensor([[ 0.8456,  0.2016, -0.7072],
        [-0.1242, -1.9096,  0.3081]], device='cuda:0'), tensor([[-1.4683,  0.3372,  0.8181],
        [ 1.3352,  0.6473, -1.4947]], device='cuda:0'), tensor([[-0.8815, -0.3689,  0.1639],
        [ 0.0284,  0.6285,  0.1271]], device='cuda:0'), tensor([[ 0.3068,  0.6370, -0.5781],
        [ 1.2845, -1.0440, -0.2639]], device='cuda:0'), tensor([[ 0.5464,  1.6256, -0.5388],
        [-0.2515,  0.7066,  0.7183]], device='cuda:0'), tensor([[-0.5179, -0.1676,  1.6046],
        [ 0.4222, -0.3022,  1.5582]], device='cuda:0'), tensor([[ 1.9701, -0.0909,  0.1677],
        [ 0.0402, -1.3000, -0.4937]], device='cuda:0'), tensor([[ 1.3462, -0.4426, -0.9446],
        [ 1.4703, -1.2582, -0.1105]], device='cuda:0'), tensor([[ 0.2497, -0.7672, -1.2856],
        [-0.6834,  1.4657,  0.1942]], device='cuda:0'), tensor([[-2.5381, -2.7670, -0.6195],
        [-1.8058, -0.8120,  0.4175]], device='cuda:0'), tensor([[-1.3229, -0.6276,  0.7311],
        [-0.4062,  1.1249, -2.8920]], device='cuda:0'), tensor([[ 0.0545, -0.7819, -0.6565],
        [-0.0560,  0.0145,  0.0317]], device='cuda:0'), tensor([[ 0.0978, -0.6353, -0.6948],
        [-1.1120, -0.9551, -0.5714]], device='cuda:0'), tensor([[-0.0489, -1.8358, -0.8946],
        [-0.4892,  0.7041, -0.3016]], device='cuda:0'), tensor([[-3.0215, -0.2830, -0.0862],
        [ 1.9947, -0.4977,  1.2364]], device='cuda:0'), tensor([[-0.9189, -0.6557, -1.1053],
        [ 0.4981,  1.0123, -0.4752]], device='cuda:0'), tensor([[ 0.3338, -0.8981,  0.6763],
        [-0.4201, -0.1442,  0.8817]], device='cuda:0'), tensor([[ 0.3618, -1.8074,  1.7514],
        [ 1.4417,  1.8395,  0.6025]], device='cuda:0'), tensor([[-1.4510e-03, -7.6506e-01,  4.9661e-02],
        [-2.7840e-01,  3.7254e-01,  2.5925e+00]], device='cuda:0'), tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0'), tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')]
====================
schedule for this step is
[(3, 0), (2, 1)]
inputting microbatch 3 into partition 0
before moving to cuda:0: tensor([[ 0.1018, -0.9672, -0.3339],
        [ 0.3764, -0.3448, -0.1502]], device='cuda:0')
after: tensor([[ 0.1018, -0.9672, -0.3339],
        [ 0.3764, -0.3448, -0.1502]], device='cuda:0')
********************
observing microbatch 3
current batch shape is torch.Size([2, 3])
batch is tensor([[ 0.1018, -0.9672, -0.3339],
        [ 0.3764, -0.3448, -0.1502]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 3
********************
inputting microbatch 2 into partition 1
before moving to cuda:1: tensor([[0.4885, 0.4836, 0.6459, 0.5517],
        [0.3915, 0.6753, 0.4926, 0.7970]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9622, 0.9626, 0.9957, 0.9984],
        [0.6515, 0.9547, 0.9501, 0.9937]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 2
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9622, 0.9626, 0.9957, 0.9984],
        [0.6515, 0.9547, 0.9501, 0.9937]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 2
********************
receiving microbatch 3 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 3, tensor([[0.4174, 0.5853, 0.6437, 0.6661],
        [0.5046, 0.4944, 0.6006, 0.6172]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 3
result is tensor([[0.4174, 0.5853, 0.6437, 0.6661],
        [0.5046, 0.4944, 0.6006, 0.6172]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4174, 0.5853, 0.6437, 0.6661],
        [0.5046, 0.4944, 0.6006, 0.6172]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 2 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 2, tensor([[0.9638, 0.9642, 0.9959, 0.9985, 0.6515],
        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 2
result is tensor([[0.9638, 0.9642, 0.9959, 0.9985, 0.6515],
        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9638, 0.9642, 0.9959, 0.9985, 0.6515],
        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9638, 0.9642, 0.9959, 0.9985, 0.6515],
        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4174, 0.5853, 0.6437, 0.6661],
        [0.5046, 0.4944, 0.6006, 0.6172]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.0526, -0.1305,  1.1132],
        [-0.3683, -0.3995,  0.0282]], device='cuda:0'), tensor([[ 0.2723, -0.8525, -0.4385],
        [-0.6751,  0.4300, -0.2500]], device='cuda:0'), tensor([[ 0.4092, -0.6732,  0.9852],
        [-0.3900,  1.5801, -0.3537]], device='cuda:0'), tensor([[ 1.2932,  0.5636, -0.6612],
        [ 1.0518, -0.2321,  0.7574]], device='cuda:0'), tensor([[ 0.2921, -0.0545, -0.4510],
        [ 0.5752,  1.6007,  1.1187]], device='cuda:0'), tensor([[-0.1838, -0.1619, -1.0705],
        [ 0.2961,  0.1156, -0.9596]], device='cuda:0'), tensor([[ 1.3585,  0.8174,  0.4760],
        [ 2.3039, -0.4182, -0.2226]], device='cuda:0'), tensor([[ 0.8456,  0.2016, -0.7072],
        [-0.1242, -1.9096,  0.3081]], device='cuda:0'), tensor([[-1.4683,  0.3372,  0.8181],
        [ 1.3352,  0.6473, -1.4947]], device='cuda:0'), tensor([[-0.8815, -0.3689,  0.1639],
        [ 0.0284,  0.6285,  0.1271]], device='cuda:0'), tensor([[ 0.3068,  0.6370, -0.5781],
        [ 1.2845, -1.0440, -0.2639]], device='cuda:0'), tensor([[ 0.5464,  1.6256, -0.5388],
        [-0.2515,  0.7066,  0.7183]], device='cuda:0'), tensor([[-0.5179, -0.1676,  1.6046],
        [ 0.4222, -0.3022,  1.5582]], device='cuda:0'), tensor([[ 1.9701, -0.0909,  0.1677],
        [ 0.0402, -1.3000, -0.4937]], device='cuda:0'), tensor([[ 1.3462, -0.4426, -0.9446],
        [ 1.4703, -1.2582, -0.1105]], device='cuda:0'), tensor([[ 0.2497, -0.7672, -1.2856],
        [-0.6834,  1.4657,  0.1942]], device='cuda:0'), tensor([[-2.5381, -2.7670, -0.6195],
        [-1.8058, -0.8120,  0.4175]], device='cuda:0'), tensor([[-1.3229, -0.6276,  0.7311],
        [-0.4062,  1.1249, -2.8920]], device='cuda:0'), tensor([[ 0.0545, -0.7819, -0.6565],
        [-0.0560,  0.0145,  0.0317]], device='cuda:0'), tensor([[ 0.0978, -0.6353, -0.6948],
        [-1.1120, -0.9551, -0.5714]], device='cuda:0'), tensor([[-0.0489, -1.8358, -0.8946],
        [-0.4892,  0.7041, -0.3016]], device='cuda:0'), tensor([[-3.0215, -0.2830, -0.0862],
        [ 1.9947, -0.4977,  1.2364]], device='cuda:0'), tensor([[-0.9189, -0.6557, -1.1053],
        [ 0.4981,  1.0123, -0.4752]], device='cuda:0'), tensor([[ 0.3338, -0.8981,  0.6763],
        [-0.4201, -0.1442,  0.8817]], device='cuda:0'), tensor([[ 0.3618, -1.8074,  1.7514],
        [ 1.4417,  1.8395,  0.6025]], device='cuda:0'), tensor([[-1.4510e-03, -7.6506e-01,  4.9661e-02],
        [-2.7840e-01,  3.7254e-01,  2.5925e+00]], device='cuda:0'), tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0'), tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')]
====================
schedule for this step is
[(4, 0), (3, 1)]
inputting microbatch 4 into partition 0
before moving to cuda:0: tensor([[-0.0526, -0.1305,  1.1132],
        [-0.3683, -0.3995,  0.0282]], device='cuda:0')
after: tensor([[-0.0526, -0.1305,  1.1132],
        [-0.3683, -0.3995,  0.0282]], device='cuda:0')
********************
observing microbatch 4
current batch shape is torch.Size([2, 3])
batch is tensor([[-0.0526, -0.1305,  1.1132],
        [-0.3683, -0.3995,  0.0282]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 4
********************
inputting microbatch 3 into partition 1
before moving to cuda:1: tensor([[0.4174, 0.5853, 0.6437, 0.6661],
        [0.5046, 0.4944, 0.6006, 0.6172]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9638, 0.9642, 0.9959, 0.9985],
        [0.6515, 0.9560, 0.9512, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 3
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9638, 0.9642, 0.9959, 0.9985],
        [0.6515, 0.9560, 0.9512, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 3
********************
receiving microbatch 4 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 4, tensor([[0.5263, 0.5059, 0.5458, 0.6941],
        [0.4699, 0.5629, 0.5160, 0.6894]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 4
result is tensor([[0.5263, 0.5059, 0.5458, 0.6941],
        [0.4699, 0.5629, 0.5160, 0.6894]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5263, 0.5059, 0.5458, 0.6941],
        [0.4699, 0.5629, 0.5160, 0.6894]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 3 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 3, tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 3
result is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9638, 0.9642, 0.9959, 0.9985, 0.6515],
        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5263, 0.5059, 0.5458, 0.6941],
        [0.4699, 0.5629, 0.5160, 0.6894]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.2723, -0.8525, -0.4385],
        [-0.6751,  0.4300, -0.2500]], device='cuda:0'), tensor([[ 0.4092, -0.6732,  0.9852],
        [-0.3900,  1.5801, -0.3537]], device='cuda:0'), tensor([[ 1.2932,  0.5636, -0.6612],
        [ 1.0518, -0.2321,  0.7574]], device='cuda:0'), tensor([[ 0.2921, -0.0545, -0.4510],
        [ 0.5752,  1.6007,  1.1187]], device='cuda:0'), tensor([[-0.1838, -0.1619, -1.0705],
        [ 0.2961,  0.1156, -0.9596]], device='cuda:0'), tensor([[ 1.3585,  0.8174,  0.4760],
        [ 2.3039, -0.4182, -0.2226]], device='cuda:0'), tensor([[ 0.8456,  0.2016, -0.7072],
        [-0.1242, -1.9096,  0.3081]], device='cuda:0'), tensor([[-1.4683,  0.3372,  0.8181],
        [ 1.3352,  0.6473, -1.4947]], device='cuda:0'), tensor([[-0.8815, -0.3689,  0.1639],
        [ 0.0284,  0.6285,  0.1271]], device='cuda:0'), tensor([[ 0.3068,  0.6370, -0.5781],
        [ 1.2845, -1.0440, -0.2639]], device='cuda:0'), tensor([[ 0.5464,  1.6256, -0.5388],
        [-0.2515,  0.7066,  0.7183]], device='cuda:0'), tensor([[-0.5179, -0.1676,  1.6046],
        [ 0.4222, -0.3022,  1.5582]], device='cuda:0'), tensor([[ 1.9701, -0.0909,  0.1677],
        [ 0.0402, -1.3000, -0.4937]], device='cuda:0'), tensor([[ 1.3462, -0.4426, -0.9446],
        [ 1.4703, -1.2582, -0.1105]], device='cuda:0'), tensor([[ 0.2497, -0.7672, -1.2856],
        [-0.6834,  1.4657,  0.1942]], device='cuda:0'), tensor([[-2.5381, -2.7670, -0.6195],
        [-1.8058, -0.8120,  0.4175]], device='cuda:0'), tensor([[-1.3229, -0.6276,  0.7311],
        [-0.4062,  1.1249, -2.8920]], device='cuda:0'), tensor([[ 0.0545, -0.7819, -0.6565],
        [-0.0560,  0.0145,  0.0317]], device='cuda:0'), tensor([[ 0.0978, -0.6353, -0.6948],
        [-1.1120, -0.9551, -0.5714]], device='cuda:0'), tensor([[-0.0489, -1.8358, -0.8946],
        [-0.4892,  0.7041, -0.3016]], device='cuda:0'), tensor([[-3.0215, -0.2830, -0.0862],
        [ 1.9947, -0.4977,  1.2364]], device='cuda:0'), tensor([[-0.9189, -0.6557, -1.1053],
        [ 0.4981,  1.0123, -0.4752]], device='cuda:0'), tensor([[ 0.3338, -0.8981,  0.6763],
        [-0.4201, -0.1442,  0.8817]], device='cuda:0'), tensor([[ 0.3618, -1.8074,  1.7514],
        [ 1.4417,  1.8395,  0.6025]], device='cuda:0'), tensor([[-1.4510e-03, -7.6506e-01,  4.9661e-02],
        [-2.7840e-01,  3.7254e-01,  2.5925e+00]], device='cuda:0'), tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0'), tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')]
====================
schedule for this step is
[(5, 0), (4, 1)]
inputting microbatch 5 into partition 0
before moving to cuda:0: tensor([[ 0.2723, -0.8525, -0.4385],
        [-0.6751,  0.4300, -0.2500]], device='cuda:0')
after: tensor([[ 0.2723, -0.8525, -0.4385],
        [-0.6751,  0.4300, -0.2500]], device='cuda:0')
********************
observing microbatch 5
current batch shape is torch.Size([2, 3])
batch is tensor([[ 0.2723, -0.8525, -0.4385],
        [-0.6751,  0.4300, -0.2500]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 5
********************
inputting microbatch 4 into partition 1
before moving to cuda:1: tensor([[0.5263, 0.5059, 0.5458, 0.6941],
        [0.4699, 0.5629, 0.5160, 0.6894]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 4
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 4
********************
receiving microbatch 5 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 5, tensor([[0.4366, 0.5590, 0.6473, 0.6414],
        [0.5543, 0.4983, 0.3581, 0.6600]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 5
result is tensor([[0.4366, 0.5590, 0.6473, 0.6414],
        [0.5543, 0.4983, 0.3581, 0.6600]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4366, 0.5590, 0.6473, 0.6414],
        [0.5543, 0.4983, 0.3581, 0.6600]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 4 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 4, tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 4
result is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9638, 0.9642, 0.9959, 0.9985, 0.6515],
        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4366, 0.5590, 0.6473, 0.6414],
        [0.5543, 0.4983, 0.3581, 0.6600]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.4092, -0.6732,  0.9852],
        [-0.3900,  1.5801, -0.3537]], device='cuda:0'), tensor([[ 1.2932,  0.5636, -0.6612],
        [ 1.0518, -0.2321,  0.7574]], device='cuda:0'), tensor([[ 0.2921, -0.0545, -0.4510],
        [ 0.5752,  1.6007,  1.1187]], device='cuda:0'), tensor([[-0.1838, -0.1619, -1.0705],
        [ 0.2961,  0.1156, -0.9596]], device='cuda:0'), tensor([[ 1.3585,  0.8174,  0.4760],
        [ 2.3039, -0.4182, -0.2226]], device='cuda:0'), tensor([[ 0.8456,  0.2016, -0.7072],
        [-0.1242, -1.9096,  0.3081]], device='cuda:0'), tensor([[-1.4683,  0.3372,  0.8181],
        [ 1.3352,  0.6473, -1.4947]], device='cuda:0'), tensor([[-0.8815, -0.3689,  0.1639],
        [ 0.0284,  0.6285,  0.1271]], device='cuda:0'), tensor([[ 0.3068,  0.6370, -0.5781],
        [ 1.2845, -1.0440, -0.2639]], device='cuda:0'), tensor([[ 0.5464,  1.6256, -0.5388],
        [-0.2515,  0.7066,  0.7183]], device='cuda:0'), tensor([[-0.5179, -0.1676,  1.6046],
        [ 0.4222, -0.3022,  1.5582]], device='cuda:0'), tensor([[ 1.9701, -0.0909,  0.1677],
        [ 0.0402, -1.3000, -0.4937]], device='cuda:0'), tensor([[ 1.3462, -0.4426, -0.9446],
        [ 1.4703, -1.2582, -0.1105]], device='cuda:0'), tensor([[ 0.2497, -0.7672, -1.2856],
        [-0.6834,  1.4657,  0.1942]], device='cuda:0'), tensor([[-2.5381, -2.7670, -0.6195],
        [-1.8058, -0.8120,  0.4175]], device='cuda:0'), tensor([[-1.3229, -0.6276,  0.7311],
        [-0.4062,  1.1249, -2.8920]], device='cuda:0'), tensor([[ 0.0545, -0.7819, -0.6565],
        [-0.0560,  0.0145,  0.0317]], device='cuda:0'), tensor([[ 0.0978, -0.6353, -0.6948],
        [-1.1120, -0.9551, -0.5714]], device='cuda:0'), tensor([[-0.0489, -1.8358, -0.8946],
        [-0.4892,  0.7041, -0.3016]], device='cuda:0'), tensor([[-3.0215, -0.2830, -0.0862],
        [ 1.9947, -0.4977,  1.2364]], device='cuda:0'), tensor([[-0.9189, -0.6557, -1.1053],
        [ 0.4981,  1.0123, -0.4752]], device='cuda:0'), tensor([[ 0.3338, -0.8981,  0.6763],
        [-0.4201, -0.1442,  0.8817]], device='cuda:0'), tensor([[ 0.3618, -1.8074,  1.7514],
        [ 1.4417,  1.8395,  0.6025]], device='cuda:0'), tensor([[-1.4510e-03, -7.6506e-01,  4.9661e-02],
        [-2.7840e-01,  3.7254e-01,  2.5925e+00]], device='cuda:0'), tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0'), tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')]
====================
schedule for this step is
[(6, 0), (5, 1)]
inputting microbatch 6 into partition 0
before moving to cuda:0: tensor([[ 0.4092, -0.6732,  0.9852],
        [-0.3900,  1.5801, -0.3537]], device='cuda:0')
after: tensor([[ 0.4092, -0.6732,  0.9852],
        [-0.3900,  1.5801, -0.3537]], device='cuda:0')
********************
observing microbatch 6
current batch shape is torch.Size([2, 3])
batch is tensor([[ 0.4092, -0.6732,  0.9852],
        [-0.3900,  1.5801, -0.3537]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 6
********************
inputting microbatch 5 into partition 1
before moving to cuda:1: tensor([[0.4366, 0.5590, 0.6473, 0.6414],
        [0.5543, 0.4983, 0.3581, 0.6600]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 5
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 5
********************
receiving microbatch 6 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 6, tensor([[0.4783, 0.5266, 0.6715, 0.6799],
        [0.6928, 0.3525, 0.2532, 0.5669]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 6
result is tensor([[0.4783, 0.5266, 0.6715, 0.6799],
        [0.6928, 0.3525, 0.2532, 0.5669]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4783, 0.5266, 0.6715, 0.6799],
        [0.6928, 0.3525, 0.2532, 0.5669]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 5 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 5, tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 5
result is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9638, 0.9642, 0.9959, 0.9985, 0.6515],
        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4783, 0.5266, 0.6715, 0.6799],
        [0.6928, 0.3525, 0.2532, 0.5669]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 1.2932,  0.5636, -0.6612],
        [ 1.0518, -0.2321,  0.7574]], device='cuda:0'), tensor([[ 0.2921, -0.0545, -0.4510],
        [ 0.5752,  1.6007,  1.1187]], device='cuda:0'), tensor([[-0.1838, -0.1619, -1.0705],
        [ 0.2961,  0.1156, -0.9596]], device='cuda:0'), tensor([[ 1.3585,  0.8174,  0.4760],
        [ 2.3039, -0.4182, -0.2226]], device='cuda:0'), tensor([[ 0.8456,  0.2016, -0.7072],
        [-0.1242, -1.9096,  0.3081]], device='cuda:0'), tensor([[-1.4683,  0.3372,  0.8181],
        [ 1.3352,  0.6473, -1.4947]], device='cuda:0'), tensor([[-0.8815, -0.3689,  0.1639],
        [ 0.0284,  0.6285,  0.1271]], device='cuda:0'), tensor([[ 0.3068,  0.6370, -0.5781],
        [ 1.2845, -1.0440, -0.2639]], device='cuda:0'), tensor([[ 0.5464,  1.6256, -0.5388],
        [-0.2515,  0.7066,  0.7183]], device='cuda:0'), tensor([[-0.5179, -0.1676,  1.6046],
        [ 0.4222, -0.3022,  1.5582]], device='cuda:0'), tensor([[ 1.9701, -0.0909,  0.1677],
        [ 0.0402, -1.3000, -0.4937]], device='cuda:0'), tensor([[ 1.3462, -0.4426, -0.9446],
        [ 1.4703, -1.2582, -0.1105]], device='cuda:0'), tensor([[ 0.2497, -0.7672, -1.2856],
        [-0.6834,  1.4657,  0.1942]], device='cuda:0'), tensor([[-2.5381, -2.7670, -0.6195],
        [-1.8058, -0.8120,  0.4175]], device='cuda:0'), tensor([[-1.3229, -0.6276,  0.7311],
        [-0.4062,  1.1249, -2.8920]], device='cuda:0'), tensor([[ 0.0545, -0.7819, -0.6565],
        [-0.0560,  0.0145,  0.0317]], device='cuda:0'), tensor([[ 0.0978, -0.6353, -0.6948],
        [-1.1120, -0.9551, -0.5714]], device='cuda:0'), tensor([[-0.0489, -1.8358, -0.8946],
        [-0.4892,  0.7041, -0.3016]], device='cuda:0'), tensor([[-3.0215, -0.2830, -0.0862],
        [ 1.9947, -0.4977,  1.2364]], device='cuda:0'), tensor([[-0.9189, -0.6557, -1.1053],
        [ 0.4981,  1.0123, -0.4752]], device='cuda:0'), tensor([[ 0.3338, -0.8981,  0.6763],
        [-0.4201, -0.1442,  0.8817]], device='cuda:0'), tensor([[ 0.3618, -1.8074,  1.7514],
        [ 1.4417,  1.8395,  0.6025]], device='cuda:0'), tensor([[-1.4510e-03, -7.6506e-01,  4.9661e-02],
        [-2.7840e-01,  3.7254e-01,  2.5925e+00]], device='cuda:0'), tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0'), tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')]
====================
schedule for this step is
[(7, 0), (6, 1)]
inputting microbatch 7 into partition 0
before moving to cuda:0: tensor([[ 1.2932,  0.5636, -0.6612],
        [ 1.0518, -0.2321,  0.7574]], device='cuda:0')
after: tensor([[ 1.2932,  0.5636, -0.6612],
        [ 1.0518, -0.2321,  0.7574]], device='cuda:0')
********************
observing microbatch 7
current batch shape is torch.Size([2, 3])
batch is tensor([[ 1.2932,  0.5636, -0.6612],
        [ 1.0518, -0.2321,  0.7574]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 7
********************
inputting microbatch 6 into partition 1
before moving to cuda:1: tensor([[0.4783, 0.5266, 0.6715, 0.6799],
        [0.6928, 0.3525, 0.2532, 0.5669]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 6
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 6
********************
receiving microbatch 7 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 7, tensor([[0.6414, 0.3254, 0.5834, 0.4582],
        [0.5546, 0.4247, 0.6870, 0.5918]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 7
result is tensor([[0.6414, 0.3254, 0.5834, 0.4582],
        [0.5546, 0.4247, 0.6870, 0.5918]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6414, 0.3254, 0.5834, 0.4582],
        [0.5546, 0.4247, 0.6870, 0.5918]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 6 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 6, tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 6
result is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9638, 0.9642, 0.9959, 0.9985, 0.6515],
        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6414, 0.3254, 0.5834, 0.4582],
        [0.5546, 0.4247, 0.6870, 0.5918]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.2921, -0.0545, -0.4510],
        [ 0.5752,  1.6007,  1.1187]], device='cuda:0'), tensor([[-0.1838, -0.1619, -1.0705],
        [ 0.2961,  0.1156, -0.9596]], device='cuda:0'), tensor([[ 1.3585,  0.8174,  0.4760],
        [ 2.3039, -0.4182, -0.2226]], device='cuda:0'), tensor([[ 0.8456,  0.2016, -0.7072],
        [-0.1242, -1.9096,  0.3081]], device='cuda:0'), tensor([[-1.4683,  0.3372,  0.8181],
        [ 1.3352,  0.6473, -1.4947]], device='cuda:0'), tensor([[-0.8815, -0.3689,  0.1639],
        [ 0.0284,  0.6285,  0.1271]], device='cuda:0'), tensor([[ 0.3068,  0.6370, -0.5781],
        [ 1.2845, -1.0440, -0.2639]], device='cuda:0'), tensor([[ 0.5464,  1.6256, -0.5388],
        [-0.2515,  0.7066,  0.7183]], device='cuda:0'), tensor([[-0.5179, -0.1676,  1.6046],
        [ 0.4222, -0.3022,  1.5582]], device='cuda:0'), tensor([[ 1.9701, -0.0909,  0.1677],
        [ 0.0402, -1.3000, -0.4937]], device='cuda:0'), tensor([[ 1.3462, -0.4426, -0.9446],
        [ 1.4703, -1.2582, -0.1105]], device='cuda:0'), tensor([[ 0.2497, -0.7672, -1.2856],
        [-0.6834,  1.4657,  0.1942]], device='cuda:0'), tensor([[-2.5381, -2.7670, -0.6195],
        [-1.8058, -0.8120,  0.4175]], device='cuda:0'), tensor([[-1.3229, -0.6276,  0.7311],
        [-0.4062,  1.1249, -2.8920]], device='cuda:0'), tensor([[ 0.0545, -0.7819, -0.6565],
        [-0.0560,  0.0145,  0.0317]], device='cuda:0'), tensor([[ 0.0978, -0.6353, -0.6948],
        [-1.1120, -0.9551, -0.5714]], device='cuda:0'), tensor([[-0.0489, -1.8358, -0.8946],
        [-0.4892,  0.7041, -0.3016]], device='cuda:0'), tensor([[-3.0215, -0.2830, -0.0862],
        [ 1.9947, -0.4977,  1.2364]], device='cuda:0'), tensor([[-0.9189, -0.6557, -1.1053],
        [ 0.4981,  1.0123, -0.4752]], device='cuda:0'), tensor([[ 0.3338, -0.8981,  0.6763],
        [-0.4201, -0.1442,  0.8817]], device='cuda:0'), tensor([[ 0.3618, -1.8074,  1.7514],
        [ 1.4417,  1.8395,  0.6025]], device='cuda:0'), tensor([[-1.4510e-03, -7.6506e-01,  4.9661e-02],
        [-2.7840e-01,  3.7254e-01,  2.5925e+00]], device='cuda:0'), tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0'), tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')]
====================
schedule for this step is
[(8, 0), (7, 1)]
inputting microbatch 8 into partition 0
before moving to cuda:0: tensor([[ 0.2921, -0.0545, -0.4510],
        [ 0.5752,  1.6007,  1.1187]], device='cuda:0')
after: tensor([[ 0.2921, -0.0545, -0.4510],
        [ 0.5752,  1.6007,  1.1187]], device='cuda:0')
********************
observing microbatch 8
current batch shape is torch.Size([2, 3])
batch is tensor([[ 0.2921, -0.0545, -0.4510],
        [ 0.5752,  1.6007,  1.1187]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 8
********************
inputting microbatch 7 into partition 1
before moving to cuda:1: tensor([[0.6414, 0.3254, 0.5834, 0.4582],
        [0.5546, 0.4247, 0.6870, 0.5918]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 7
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 7
********************
receiving microbatch 8 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 8, tensor([[0.5329, 0.4700, 0.5434, 0.5950],
        [0.7389, 0.2791, 0.3903, 0.5456]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 8
result is tensor([[0.5329, 0.4700, 0.5434, 0.5950],
        [0.7389, 0.2791, 0.3903, 0.5456]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5329, 0.4700, 0.5434, 0.5950],
        [0.7389, 0.2791, 0.3903, 0.5456]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 7 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 7, tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 7
result is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9638, 0.9642, 0.9959, 0.9985, 0.6515],
        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5329, 0.4700, 0.5434, 0.5950],
        [0.7389, 0.2791, 0.3903, 0.5456]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.1838, -0.1619, -1.0705],
        [ 0.2961,  0.1156, -0.9596]], device='cuda:0'), tensor([[ 1.3585,  0.8174,  0.4760],
        [ 2.3039, -0.4182, -0.2226]], device='cuda:0'), tensor([[ 0.8456,  0.2016, -0.7072],
        [-0.1242, -1.9096,  0.3081]], device='cuda:0'), tensor([[-1.4683,  0.3372,  0.8181],
        [ 1.3352,  0.6473, -1.4947]], device='cuda:0'), tensor([[-0.8815, -0.3689,  0.1639],
        [ 0.0284,  0.6285,  0.1271]], device='cuda:0'), tensor([[ 0.3068,  0.6370, -0.5781],
        [ 1.2845, -1.0440, -0.2639]], device='cuda:0'), tensor([[ 0.5464,  1.6256, -0.5388],
        [-0.2515,  0.7066,  0.7183]], device='cuda:0'), tensor([[-0.5179, -0.1676,  1.6046],
        [ 0.4222, -0.3022,  1.5582]], device='cuda:0'), tensor([[ 1.9701, -0.0909,  0.1677],
        [ 0.0402, -1.3000, -0.4937]], device='cuda:0'), tensor([[ 1.3462, -0.4426, -0.9446],
        [ 1.4703, -1.2582, -0.1105]], device='cuda:0'), tensor([[ 0.2497, -0.7672, -1.2856],
        [-0.6834,  1.4657,  0.1942]], device='cuda:0'), tensor([[-2.5381, -2.7670, -0.6195],
        [-1.8058, -0.8120,  0.4175]], device='cuda:0'), tensor([[-1.3229, -0.6276,  0.7311],
        [-0.4062,  1.1249, -2.8920]], device='cuda:0'), tensor([[ 0.0545, -0.7819, -0.6565],
        [-0.0560,  0.0145,  0.0317]], device='cuda:0'), tensor([[ 0.0978, -0.6353, -0.6948],
        [-1.1120, -0.9551, -0.5714]], device='cuda:0'), tensor([[-0.0489, -1.8358, -0.8946],
        [-0.4892,  0.7041, -0.3016]], device='cuda:0'), tensor([[-3.0215, -0.2830, -0.0862],
        [ 1.9947, -0.4977,  1.2364]], device='cuda:0'), tensor([[-0.9189, -0.6557, -1.1053],
        [ 0.4981,  1.0123, -0.4752]], device='cuda:0'), tensor([[ 0.3338, -0.8981,  0.6763],
        [-0.4201, -0.1442,  0.8817]], device='cuda:0'), tensor([[ 0.3618, -1.8074,  1.7514],
        [ 1.4417,  1.8395,  0.6025]], device='cuda:0'), tensor([[-1.4510e-03, -7.6506e-01,  4.9661e-02],
        [-2.7840e-01,  3.7254e-01,  2.5925e+00]], device='cuda:0'), tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0'), tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')]
====================
schedule for this step is
[(9, 0), (8, 1)]
inputting microbatch 9 into partition 0
before moving to cuda:0: tensor([[-0.1838, -0.1619, -1.0705],
        [ 0.2961,  0.1156, -0.9596]], device='cuda:0')
after: tensor([[-0.1838, -0.1619, -1.0705],
        [ 0.2961,  0.1156, -0.9596]], device='cuda:0')
********************
observing microbatch 9
current batch shape is torch.Size([2, 3])
batch is tensor([[-0.1838, -0.1619, -1.0705],
        [ 0.2961,  0.1156, -0.9596]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 9
********************
inputting microbatch 8 into partition 1
before moving to cuda:1: tensor([[0.5329, 0.4700, 0.5434, 0.5950],
        [0.7389, 0.2791, 0.3903, 0.5456]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 8
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 8
********************
receiving microbatch 9 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 9, tensor([[0.4944, 0.5226, 0.4812, 0.6153],
        [0.5481, 0.4516, 0.5083, 0.5618]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 9
result is tensor([[0.4944, 0.5226, 0.4812, 0.6153],
        [0.5481, 0.4516, 0.5083, 0.5618]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4944, 0.5226, 0.4812, 0.6153],
        [0.5481, 0.4516, 0.5083, 0.5618]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 8 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 8, tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 8
result is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9638, 0.9642, 0.9959, 0.9985, 0.6515],
        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4944, 0.5226, 0.4812, 0.6153],
        [0.5481, 0.4516, 0.5083, 0.5618]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 1.3585,  0.8174,  0.4760],
        [ 2.3039, -0.4182, -0.2226]], device='cuda:0'), tensor([[ 0.8456,  0.2016, -0.7072],
        [-0.1242, -1.9096,  0.3081]], device='cuda:0'), tensor([[-1.4683,  0.3372,  0.8181],
        [ 1.3352,  0.6473, -1.4947]], device='cuda:0'), tensor([[-0.8815, -0.3689,  0.1639],
        [ 0.0284,  0.6285,  0.1271]], device='cuda:0'), tensor([[ 0.3068,  0.6370, -0.5781],
        [ 1.2845, -1.0440, -0.2639]], device='cuda:0'), tensor([[ 0.5464,  1.6256, -0.5388],
        [-0.2515,  0.7066,  0.7183]], device='cuda:0'), tensor([[-0.5179, -0.1676,  1.6046],
        [ 0.4222, -0.3022,  1.5582]], device='cuda:0'), tensor([[ 1.9701, -0.0909,  0.1677],
        [ 0.0402, -1.3000, -0.4937]], device='cuda:0'), tensor([[ 1.3462, -0.4426, -0.9446],
        [ 1.4703, -1.2582, -0.1105]], device='cuda:0'), tensor([[ 0.2497, -0.7672, -1.2856],
        [-0.6834,  1.4657,  0.1942]], device='cuda:0'), tensor([[-2.5381, -2.7670, -0.6195],
        [-1.8058, -0.8120,  0.4175]], device='cuda:0'), tensor([[-1.3229, -0.6276,  0.7311],
        [-0.4062,  1.1249, -2.8920]], device='cuda:0'), tensor([[ 0.0545, -0.7819, -0.6565],
        [-0.0560,  0.0145,  0.0317]], device='cuda:0'), tensor([[ 0.0978, -0.6353, -0.6948],
        [-1.1120, -0.9551, -0.5714]], device='cuda:0'), tensor([[-0.0489, -1.8358, -0.8946],
        [-0.4892,  0.7041, -0.3016]], device='cuda:0'), tensor([[-3.0215, -0.2830, -0.0862],
        [ 1.9947, -0.4977,  1.2364]], device='cuda:0'), tensor([[-0.9189, -0.6557, -1.1053],
        [ 0.4981,  1.0123, -0.4752]], device='cuda:0'), tensor([[ 0.3338, -0.8981,  0.6763],
        [-0.4201, -0.1442,  0.8817]], device='cuda:0'), tensor([[ 0.3618, -1.8074,  1.7514],
        [ 1.4417,  1.8395,  0.6025]], device='cuda:0'), tensor([[-1.4510e-03, -7.6506e-01,  4.9661e-02],
        [-2.7840e-01,  3.7254e-01,  2.5925e+00]], device='cuda:0'), tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0'), tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')]
====================
schedule for this step is
[(10, 0), (9, 1)]
inputting microbatch 10 into partition 0
before moving to cuda:0: tensor([[ 1.3585,  0.8174,  0.4760],
        [ 2.3039, -0.4182, -0.2226]], device='cuda:0')
after: tensor([[ 1.3585,  0.8174,  0.4760],
        [ 2.3039, -0.4182, -0.2226]], device='cuda:0')
********************
observing microbatch 10
current batch shape is torch.Size([2, 3])
batch is tensor([[ 1.3585,  0.8174,  0.4760],
        [ 2.3039, -0.4182, -0.2226]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 10
********************
inputting microbatch 9 into partition 1
before moving to cuda:1: tensor([[0.4944, 0.5226, 0.4812, 0.6153],
        [0.5481, 0.4516, 0.5083, 0.5618]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 9
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 9
********************
receiving microbatch 10 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 10, tensor([[0.6818, 0.2959, 0.5838, 0.4899],
        [0.5719, 0.3450, 0.8106, 0.4435]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 10
result is tensor([[0.6818, 0.2959, 0.5838, 0.4899],
        [0.5719, 0.3450, 0.8106, 0.4435]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6818, 0.2959, 0.5838, 0.4899],
        [0.5719, 0.3450, 0.8106, 0.4435]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 9 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 9, tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 9
result is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9638, 0.9642, 0.9959, 0.9985, 0.6515],
        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6818, 0.2959, 0.5838, 0.4899],
        [0.5719, 0.3450, 0.8106, 0.4435]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.8456,  0.2016, -0.7072],
        [-0.1242, -1.9096,  0.3081]], device='cuda:0'), tensor([[-1.4683,  0.3372,  0.8181],
        [ 1.3352,  0.6473, -1.4947]], device='cuda:0'), tensor([[-0.8815, -0.3689,  0.1639],
        [ 0.0284,  0.6285,  0.1271]], device='cuda:0'), tensor([[ 0.3068,  0.6370, -0.5781],
        [ 1.2845, -1.0440, -0.2639]], device='cuda:0'), tensor([[ 0.5464,  1.6256, -0.5388],
        [-0.2515,  0.7066,  0.7183]], device='cuda:0'), tensor([[-0.5179, -0.1676,  1.6046],
        [ 0.4222, -0.3022,  1.5582]], device='cuda:0'), tensor([[ 1.9701, -0.0909,  0.1677],
        [ 0.0402, -1.3000, -0.4937]], device='cuda:0'), tensor([[ 1.3462, -0.4426, -0.9446],
        [ 1.4703, -1.2582, -0.1105]], device='cuda:0'), tensor([[ 0.2497, -0.7672, -1.2856],
        [-0.6834,  1.4657,  0.1942]], device='cuda:0'), tensor([[-2.5381, -2.7670, -0.6195],
        [-1.8058, -0.8120,  0.4175]], device='cuda:0'), tensor([[-1.3229, -0.6276,  0.7311],
        [-0.4062,  1.1249, -2.8920]], device='cuda:0'), tensor([[ 0.0545, -0.7819, -0.6565],
        [-0.0560,  0.0145,  0.0317]], device='cuda:0'), tensor([[ 0.0978, -0.6353, -0.6948],
        [-1.1120, -0.9551, -0.5714]], device='cuda:0'), tensor([[-0.0489, -1.8358, -0.8946],
        [-0.4892,  0.7041, -0.3016]], device='cuda:0'), tensor([[-3.0215, -0.2830, -0.0862],
        [ 1.9947, -0.4977,  1.2364]], device='cuda:0'), tensor([[-0.9189, -0.6557, -1.1053],
        [ 0.4981,  1.0123, -0.4752]], device='cuda:0'), tensor([[ 0.3338, -0.8981,  0.6763],
        [-0.4201, -0.1442,  0.8817]], device='cuda:0'), tensor([[ 0.3618, -1.8074,  1.7514],
        [ 1.4417,  1.8395,  0.6025]], device='cuda:0'), tensor([[-1.4510e-03, -7.6506e-01,  4.9661e-02],
        [-2.7840e-01,  3.7254e-01,  2.5925e+00]], device='cuda:0'), tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0'), tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')]
====================
schedule for this step is
[(11, 0), (10, 1)]
inputting microbatch 11 into partition 0
before moving to cuda:0: tensor([[ 0.8456,  0.2016, -0.7072],
        [-0.1242, -1.9096,  0.3081]], device='cuda:0')
after: tensor([[ 0.8456,  0.2016, -0.7072],
        [-0.1242, -1.9096,  0.3081]], device='cuda:0')
********************
observing microbatch 11
current batch shape is torch.Size([2, 3])
batch is tensor([[ 0.8456,  0.2016, -0.7072],
        [-0.1242, -1.9096,  0.3081]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 11
********************
inputting microbatch 10 into partition 1
before moving to cuda:1: tensor([[0.6818, 0.2959, 0.5838, 0.4899],
        [0.5719, 0.3450, 0.8106, 0.4435]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 10
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 10
********************
receiving microbatch 11 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 11, tensor([[0.5826, 0.3968, 0.5740, 0.5183],
        [0.3108, 0.6969, 0.7420, 0.7529]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 11
result is tensor([[0.5826, 0.3968, 0.5740, 0.5183],
        [0.3108, 0.6969, 0.7420, 0.7529]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5826, 0.3968, 0.5740, 0.5183],
        [0.3108, 0.6969, 0.7420, 0.7529]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 10 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 10, tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 10
result is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9638, 0.9642, 0.9959, 0.9985, 0.6515],
        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5826, 0.3968, 0.5740, 0.5183],
        [0.3108, 0.6969, 0.7420, 0.7529]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-1.4683,  0.3372,  0.8181],
        [ 1.3352,  0.6473, -1.4947]], device='cuda:0'), tensor([[-0.8815, -0.3689,  0.1639],
        [ 0.0284,  0.6285,  0.1271]], device='cuda:0'), tensor([[ 0.3068,  0.6370, -0.5781],
        [ 1.2845, -1.0440, -0.2639]], device='cuda:0'), tensor([[ 0.5464,  1.6256, -0.5388],
        [-0.2515,  0.7066,  0.7183]], device='cuda:0'), tensor([[-0.5179, -0.1676,  1.6046],
        [ 0.4222, -0.3022,  1.5582]], device='cuda:0'), tensor([[ 1.9701, -0.0909,  0.1677],
        [ 0.0402, -1.3000, -0.4937]], device='cuda:0'), tensor([[ 1.3462, -0.4426, -0.9446],
        [ 1.4703, -1.2582, -0.1105]], device='cuda:0'), tensor([[ 0.2497, -0.7672, -1.2856],
        [-0.6834,  1.4657,  0.1942]], device='cuda:0'), tensor([[-2.5381, -2.7670, -0.6195],
        [-1.8058, -0.8120,  0.4175]], device='cuda:0'), tensor([[-1.3229, -0.6276,  0.7311],
        [-0.4062,  1.1249, -2.8920]], device='cuda:0'), tensor([[ 0.0545, -0.7819, -0.6565],
        [-0.0560,  0.0145,  0.0317]], device='cuda:0'), tensor([[ 0.0978, -0.6353, -0.6948],
        [-1.1120, -0.9551, -0.5714]], device='cuda:0'), tensor([[-0.0489, -1.8358, -0.8946],
        [-0.4892,  0.7041, -0.3016]], device='cuda:0'), tensor([[-3.0215, -0.2830, -0.0862],
        [ 1.9947, -0.4977,  1.2364]], device='cuda:0'), tensor([[-0.9189, -0.6557, -1.1053],
        [ 0.4981,  1.0123, -0.4752]], device='cuda:0'), tensor([[ 0.3338, -0.8981,  0.6763],
        [-0.4201, -0.1442,  0.8817]], device='cuda:0'), tensor([[ 0.3618, -1.8074,  1.7514],
        [ 1.4417,  1.8395,  0.6025]], device='cuda:0'), tensor([[-1.4510e-03, -7.6506e-01,  4.9661e-02],
        [-2.7840e-01,  3.7254e-01,  2.5925e+00]], device='cuda:0'), tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0'), tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')]
====================
schedule for this step is
[(12, 0), (11, 1)]
inputting microbatch 12 into partition 0
before moving to cuda:0: tensor([[-1.4683,  0.3372,  0.8181],
        [ 1.3352,  0.6473, -1.4947]], device='cuda:0')
after: tensor([[-1.4683,  0.3372,  0.8181],
        [ 1.3352,  0.6473, -1.4947]], device='cuda:0')
********************
observing microbatch 12
current batch shape is torch.Size([2, 3])
batch is tensor([[-1.4683,  0.3372,  0.8181],
        [ 1.3352,  0.6473, -1.4947]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 12
********************
inputting microbatch 11 into partition 1
before moving to cuda:1: tensor([[0.5826, 0.3968, 0.5740, 0.5183],
        [0.3108, 0.6969, 0.7420, 0.7529]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 11
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 11
********************
receiving microbatch 12 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 12, tensor([[0.5226, 0.5739, 0.3001, 0.7640],
        [0.6442, 0.3150, 0.5580, 0.4116]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 12
result is tensor([[0.5226, 0.5739, 0.3001, 0.7640],
        [0.6442, 0.3150, 0.5580, 0.4116]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5226, 0.5739, 0.3001, 0.7640],
        [0.6442, 0.3150, 0.5580, 0.4116]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 11 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 11, tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 11
result is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9638, 0.9642, 0.9959, 0.9985, 0.6515],
        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5226, 0.5739, 0.3001, 0.7640],
        [0.6442, 0.3150, 0.5580, 0.4116]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.8815, -0.3689,  0.1639],
        [ 0.0284,  0.6285,  0.1271]], device='cuda:0'), tensor([[ 0.3068,  0.6370, -0.5781],
        [ 1.2845, -1.0440, -0.2639]], device='cuda:0'), tensor([[ 0.5464,  1.6256, -0.5388],
        [-0.2515,  0.7066,  0.7183]], device='cuda:0'), tensor([[-0.5179, -0.1676,  1.6046],
        [ 0.4222, -0.3022,  1.5582]], device='cuda:0'), tensor([[ 1.9701, -0.0909,  0.1677],
        [ 0.0402, -1.3000, -0.4937]], device='cuda:0'), tensor([[ 1.3462, -0.4426, -0.9446],
        [ 1.4703, -1.2582, -0.1105]], device='cuda:0'), tensor([[ 0.2497, -0.7672, -1.2856],
        [-0.6834,  1.4657,  0.1942]], device='cuda:0'), tensor([[-2.5381, -2.7670, -0.6195],
        [-1.8058, -0.8120,  0.4175]], device='cuda:0'), tensor([[-1.3229, -0.6276,  0.7311],
        [-0.4062,  1.1249, -2.8920]], device='cuda:0'), tensor([[ 0.0545, -0.7819, -0.6565],
        [-0.0560,  0.0145,  0.0317]], device='cuda:0'), tensor([[ 0.0978, -0.6353, -0.6948],
        [-1.1120, -0.9551, -0.5714]], device='cuda:0'), tensor([[-0.0489, -1.8358, -0.8946],
        [-0.4892,  0.7041, -0.3016]], device='cuda:0'), tensor([[-3.0215, -0.2830, -0.0862],
        [ 1.9947, -0.4977,  1.2364]], device='cuda:0'), tensor([[-0.9189, -0.6557, -1.1053],
        [ 0.4981,  1.0123, -0.4752]], device='cuda:0'), tensor([[ 0.3338, -0.8981,  0.6763],
        [-0.4201, -0.1442,  0.8817]], device='cuda:0'), tensor([[ 0.3618, -1.8074,  1.7514],
        [ 1.4417,  1.8395,  0.6025]], device='cuda:0'), tensor([[-1.4510e-03, -7.6506e-01,  4.9661e-02],
        [-2.7840e-01,  3.7254e-01,  2.5925e+00]], device='cuda:0'), tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0'), tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')]
====================
schedule for this step is
[(13, 0), (12, 1)]
inputting microbatch 13 into partition 0
before moving to cuda:0: tensor([[-0.8815, -0.3689,  0.1639],
        [ 0.0284,  0.6285,  0.1271]], device='cuda:0')
after: tensor([[-0.8815, -0.3689,  0.1639],
        [ 0.0284,  0.6285,  0.1271]], device='cuda:0')
********************
observing microbatch 13
current batch shape is torch.Size([2, 3])
batch is tensor([[-0.8815, -0.3689,  0.1639],
        [ 0.0284,  0.6285,  0.1271]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 13
********************
inputting microbatch 12 into partition 1
before moving to cuda:1: tensor([[0.5226, 0.5739, 0.3001, 0.7640],
        [0.6442, 0.3150, 0.5580, 0.4116]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 12
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 12
********************
receiving microbatch 13 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 13, tensor([[0.4545, 0.6016, 0.4479, 0.7318],
        [0.6089, 0.4174, 0.4284, 0.6054]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 13
result is tensor([[0.4545, 0.6016, 0.4479, 0.7318],
        [0.6089, 0.4174, 0.4284, 0.6054]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4545, 0.6016, 0.4479, 0.7318],
        [0.6089, 0.4174, 0.4284, 0.6054]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 12 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 12, tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 12
result is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9638, 0.9642, 0.9959, 0.9985, 0.6515],
        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4545, 0.6016, 0.4479, 0.7318],
        [0.6089, 0.4174, 0.4284, 0.6054]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.3068,  0.6370, -0.5781],
        [ 1.2845, -1.0440, -0.2639]], device='cuda:0'), tensor([[ 0.5464,  1.6256, -0.5388],
        [-0.2515,  0.7066,  0.7183]], device='cuda:0'), tensor([[-0.5179, -0.1676,  1.6046],
        [ 0.4222, -0.3022,  1.5582]], device='cuda:0'), tensor([[ 1.9701, -0.0909,  0.1677],
        [ 0.0402, -1.3000, -0.4937]], device='cuda:0'), tensor([[ 1.3462, -0.4426, -0.9446],
        [ 1.4703, -1.2582, -0.1105]], device='cuda:0'), tensor([[ 0.2497, -0.7672, -1.2856],
        [-0.6834,  1.4657,  0.1942]], device='cuda:0'), tensor([[-2.5381, -2.7670, -0.6195],
        [-1.8058, -0.8120,  0.4175]], device='cuda:0'), tensor([[-1.3229, -0.6276,  0.7311],
        [-0.4062,  1.1249, -2.8920]], device='cuda:0'), tensor([[ 0.0545, -0.7819, -0.6565],
        [-0.0560,  0.0145,  0.0317]], device='cuda:0'), tensor([[ 0.0978, -0.6353, -0.6948],
        [-1.1120, -0.9551, -0.5714]], device='cuda:0'), tensor([[-0.0489, -1.8358, -0.8946],
        [-0.4892,  0.7041, -0.3016]], device='cuda:0'), tensor([[-3.0215, -0.2830, -0.0862],
        [ 1.9947, -0.4977,  1.2364]], device='cuda:0'), tensor([[-0.9189, -0.6557, -1.1053],
        [ 0.4981,  1.0123, -0.4752]], device='cuda:0'), tensor([[ 0.3338, -0.8981,  0.6763],
        [-0.4201, -0.1442,  0.8817]], device='cuda:0'), tensor([[ 0.3618, -1.8074,  1.7514],
        [ 1.4417,  1.8395,  0.6025]], device='cuda:0'), tensor([[-1.4510e-03, -7.6506e-01,  4.9661e-02],
        [-2.7840e-01,  3.7254e-01,  2.5925e+00]], device='cuda:0'), tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0'), tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')]
====================
schedule for this step is
[(14, 0), (13, 1)]
inputting microbatch 14 into partition 0
before moving to cuda:0: tensor([[ 0.3068,  0.6370, -0.5781],
        [ 1.2845, -1.0440, -0.2639]], device='cuda:0')
after: tensor([[ 0.3068,  0.6370, -0.5781],
        [ 1.2845, -1.0440, -0.2639]], device='cuda:0')
********************
observing microbatch 14
current batch shape is torch.Size([2, 3])
batch is tensor([[ 0.3068,  0.6370, -0.5781],
        [ 1.2845, -1.0440, -0.2639]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 14
********************
inputting microbatch 13 into partition 1
before moving to cuda:1: tensor([[0.4545, 0.6016, 0.4479, 0.7318],
        [0.6089, 0.4174, 0.4284, 0.6054]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 13
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 13
********************
receiving microbatch 14 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 14, tensor([[0.6134, 0.3945, 0.4467, 0.5481],
        [0.4558, 0.4946, 0.7793, 0.5713]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 14
result is tensor([[0.6134, 0.3945, 0.4467, 0.5481],
        [0.4558, 0.4946, 0.7793, 0.5713]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6134, 0.3945, 0.4467, 0.5481],
        [0.4558, 0.4946, 0.7793, 0.5713]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 13 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 13, tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 13
result is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9638, 0.9642, 0.9959, 0.9985, 0.6515],
        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6134, 0.3945, 0.4467, 0.5481],
        [0.4558, 0.4946, 0.7793, 0.5713]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.5464,  1.6256, -0.5388],
        [-0.2515,  0.7066,  0.7183]], device='cuda:0'), tensor([[-0.5179, -0.1676,  1.6046],
        [ 0.4222, -0.3022,  1.5582]], device='cuda:0'), tensor([[ 1.9701, -0.0909,  0.1677],
        [ 0.0402, -1.3000, -0.4937]], device='cuda:0'), tensor([[ 1.3462, -0.4426, -0.9446],
        [ 1.4703, -1.2582, -0.1105]], device='cuda:0'), tensor([[ 0.2497, -0.7672, -1.2856],
        [-0.6834,  1.4657,  0.1942]], device='cuda:0'), tensor([[-2.5381, -2.7670, -0.6195],
        [-1.8058, -0.8120,  0.4175]], device='cuda:0'), tensor([[-1.3229, -0.6276,  0.7311],
        [-0.4062,  1.1249, -2.8920]], device='cuda:0'), tensor([[ 0.0545, -0.7819, -0.6565],
        [-0.0560,  0.0145,  0.0317]], device='cuda:0'), tensor([[ 0.0978, -0.6353, -0.6948],
        [-1.1120, -0.9551, -0.5714]], device='cuda:0'), tensor([[-0.0489, -1.8358, -0.8946],
        [-0.4892,  0.7041, -0.3016]], device='cuda:0'), tensor([[-3.0215, -0.2830, -0.0862],
        [ 1.9947, -0.4977,  1.2364]], device='cuda:0'), tensor([[-0.9189, -0.6557, -1.1053],
        [ 0.4981,  1.0123, -0.4752]], device='cuda:0'), tensor([[ 0.3338, -0.8981,  0.6763],
        [-0.4201, -0.1442,  0.8817]], device='cuda:0'), tensor([[ 0.3618, -1.8074,  1.7514],
        [ 1.4417,  1.8395,  0.6025]], device='cuda:0'), tensor([[-1.4510e-03, -7.6506e-01,  4.9661e-02],
        [-2.7840e-01,  3.7254e-01,  2.5925e+00]], device='cuda:0'), tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0'), tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')]
====================
schedule for this step is
[(15, 0), (14, 1)]
inputting microbatch 15 into partition 0
before moving to cuda:0: tensor([[ 0.5464,  1.6256, -0.5388],
        [-0.2515,  0.7066,  0.7183]], device='cuda:0')
after: tensor([[ 0.5464,  1.6256, -0.5388],
        [-0.2515,  0.7066,  0.7183]], device='cuda:0')
********************
observing microbatch 15
current batch shape is torch.Size([2, 3])
batch is tensor([[ 0.5464,  1.6256, -0.5388],
        [-0.2515,  0.7066,  0.7183]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 15
********************
inputting microbatch 14 into partition 1
before moving to cuda:1: tensor([[0.6134, 0.3945, 0.4467, 0.5481],
        [0.4558, 0.4946, 0.7793, 0.5713]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 14
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 14
********************
receiving microbatch 15 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 15, tensor([[0.7267, 0.2801, 0.3469, 0.4700],
        [0.6130, 0.4316, 0.3959, 0.6507]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 15
result is tensor([[0.7267, 0.2801, 0.3469, 0.4700],
        [0.6130, 0.4316, 0.3959, 0.6507]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.7267, 0.2801, 0.3469, 0.4700],
        [0.6130, 0.4316, 0.3959, 0.6507]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 14 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 14, tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 14
result is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9638, 0.9642, 0.9959, 0.9985, 0.6515],
        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.7267, 0.2801, 0.3469, 0.4700],
        [0.6130, 0.4316, 0.3959, 0.6507]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.5179, -0.1676,  1.6046],
        [ 0.4222, -0.3022,  1.5582]], device='cuda:0'), tensor([[ 1.9701, -0.0909,  0.1677],
        [ 0.0402, -1.3000, -0.4937]], device='cuda:0'), tensor([[ 1.3462, -0.4426, -0.9446],
        [ 1.4703, -1.2582, -0.1105]], device='cuda:0'), tensor([[ 0.2497, -0.7672, -1.2856],
        [-0.6834,  1.4657,  0.1942]], device='cuda:0'), tensor([[-2.5381, -2.7670, -0.6195],
        [-1.8058, -0.8120,  0.4175]], device='cuda:0'), tensor([[-1.3229, -0.6276,  0.7311],
        [-0.4062,  1.1249, -2.8920]], device='cuda:0'), tensor([[ 0.0545, -0.7819, -0.6565],
        [-0.0560,  0.0145,  0.0317]], device='cuda:0'), tensor([[ 0.0978, -0.6353, -0.6948],
        [-1.1120, -0.9551, -0.5714]], device='cuda:0'), tensor([[-0.0489, -1.8358, -0.8946],
        [-0.4892,  0.7041, -0.3016]], device='cuda:0'), tensor([[-3.0215, -0.2830, -0.0862],
        [ 1.9947, -0.4977,  1.2364]], device='cuda:0'), tensor([[-0.9189, -0.6557, -1.1053],
        [ 0.4981,  1.0123, -0.4752]], device='cuda:0'), tensor([[ 0.3338, -0.8981,  0.6763],
        [-0.4201, -0.1442,  0.8817]], device='cuda:0'), tensor([[ 0.3618, -1.8074,  1.7514],
        [ 1.4417,  1.8395,  0.6025]], device='cuda:0'), tensor([[-1.4510e-03, -7.6506e-01,  4.9661e-02],
        [-2.7840e-01,  3.7254e-01,  2.5925e+00]], device='cuda:0'), tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0'), tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')]
====================
schedule for this step is
[(16, 0), (15, 1)]
inputting microbatch 16 into partition 0
before moving to cuda:0: tensor([[-0.5179, -0.1676,  1.6046],
        [ 0.4222, -0.3022,  1.5582]], device='cuda:0')
after: tensor([[-0.5179, -0.1676,  1.6046],
        [ 0.4222, -0.3022,  1.5582]], device='cuda:0')
********************
observing microbatch 16
current batch shape is torch.Size([2, 3])
batch is tensor([[-0.5179, -0.1676,  1.6046],
        [ 0.4222, -0.3022,  1.5582]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 16
********************
inputting microbatch 15 into partition 1
before moving to cuda:1: tensor([[0.7267, 0.2801, 0.3469, 0.4700],
        [0.6130, 0.4316, 0.3959, 0.6507]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 15
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 15
********************
receiving microbatch 16 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 16, tensor([[0.5083, 0.5487, 0.5017, 0.7484],
        [0.5294, 0.4842, 0.6389, 0.6830]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 16
result is tensor([[0.5083, 0.5487, 0.5017, 0.7484],
        [0.5294, 0.4842, 0.6389, 0.6830]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5083, 0.5487, 0.5017, 0.7484],
        [0.5294, 0.4842, 0.6389, 0.6830]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 15 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 15, tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 15
result is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9638, 0.9642, 0.9959, 0.9985, 0.6515],
        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5083, 0.5487, 0.5017, 0.7484],
        [0.5294, 0.4842, 0.6389, 0.6830]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 1.9701, -0.0909,  0.1677],
        [ 0.0402, -1.3000, -0.4937]], device='cuda:0'), tensor([[ 1.3462, -0.4426, -0.9446],
        [ 1.4703, -1.2582, -0.1105]], device='cuda:0'), tensor([[ 0.2497, -0.7672, -1.2856],
        [-0.6834,  1.4657,  0.1942]], device='cuda:0'), tensor([[-2.5381, -2.7670, -0.6195],
        [-1.8058, -0.8120,  0.4175]], device='cuda:0'), tensor([[-1.3229, -0.6276,  0.7311],
        [-0.4062,  1.1249, -2.8920]], device='cuda:0'), tensor([[ 0.0545, -0.7819, -0.6565],
        [-0.0560,  0.0145,  0.0317]], device='cuda:0'), tensor([[ 0.0978, -0.6353, -0.6948],
        [-1.1120, -0.9551, -0.5714]], device='cuda:0'), tensor([[-0.0489, -1.8358, -0.8946],
        [-0.4892,  0.7041, -0.3016]], device='cuda:0'), tensor([[-3.0215, -0.2830, -0.0862],
        [ 1.9947, -0.4977,  1.2364]], device='cuda:0'), tensor([[-0.9189, -0.6557, -1.1053],
        [ 0.4981,  1.0123, -0.4752]], device='cuda:0'), tensor([[ 0.3338, -0.8981,  0.6763],
        [-0.4201, -0.1442,  0.8817]], device='cuda:0'), tensor([[ 0.3618, -1.8074,  1.7514],
        [ 1.4417,  1.8395,  0.6025]], device='cuda:0'), tensor([[-1.4510e-03, -7.6506e-01,  4.9661e-02],
        [-2.7840e-01,  3.7254e-01,  2.5925e+00]], device='cuda:0'), tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0'), tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')]
====================
schedule for this step is
[(17, 0), (16, 1)]
inputting microbatch 17 into partition 0
before moving to cuda:0: tensor([[ 1.9701, -0.0909,  0.1677],
        [ 0.0402, -1.3000, -0.4937]], device='cuda:0')
after: tensor([[ 1.9701, -0.0909,  0.1677],
        [ 0.0402, -1.3000, -0.4937]], device='cuda:0')
********************
observing microbatch 17
current batch shape is torch.Size([2, 3])
batch is tensor([[ 1.9701, -0.0909,  0.1677],
        [ 0.0402, -1.3000, -0.4937]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 17
********************
inputting microbatch 16 into partition 1
before moving to cuda:1: tensor([[0.5083, 0.5487, 0.5017, 0.7484],
        [0.5294, 0.4842, 0.6389, 0.6830]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 16
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 16
********************
receiving microbatch 17 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 17, tensor([[0.6011, 0.3377, 0.7566, 0.4728],
        [0.3752, 0.6254, 0.6745, 0.6818]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 17
result is tensor([[0.6011, 0.3377, 0.7566, 0.4728],
        [0.3752, 0.6254, 0.6745, 0.6818]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6011, 0.3377, 0.7566, 0.4728],
        [0.3752, 0.6254, 0.6745, 0.6818]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 16 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 16, tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 16
result is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9638, 0.9642, 0.9959, 0.9985, 0.6515],
        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6011, 0.3377, 0.7566, 0.4728],
        [0.3752, 0.6254, 0.6745, 0.6818]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 1.3462, -0.4426, -0.9446],
        [ 1.4703, -1.2582, -0.1105]], device='cuda:0'), tensor([[ 0.2497, -0.7672, -1.2856],
        [-0.6834,  1.4657,  0.1942]], device='cuda:0'), tensor([[-2.5381, -2.7670, -0.6195],
        [-1.8058, -0.8120,  0.4175]], device='cuda:0'), tensor([[-1.3229, -0.6276,  0.7311],
        [-0.4062,  1.1249, -2.8920]], device='cuda:0'), tensor([[ 0.0545, -0.7819, -0.6565],
        [-0.0560,  0.0145,  0.0317]], device='cuda:0'), tensor([[ 0.0978, -0.6353, -0.6948],
        [-1.1120, -0.9551, -0.5714]], device='cuda:0'), tensor([[-0.0489, -1.8358, -0.8946],
        [-0.4892,  0.7041, -0.3016]], device='cuda:0'), tensor([[-3.0215, -0.2830, -0.0862],
        [ 1.9947, -0.4977,  1.2364]], device='cuda:0'), tensor([[-0.9189, -0.6557, -1.1053],
        [ 0.4981,  1.0123, -0.4752]], device='cuda:0'), tensor([[ 0.3338, -0.8981,  0.6763],
        [-0.4201, -0.1442,  0.8817]], device='cuda:0'), tensor([[ 0.3618, -1.8074,  1.7514],
        [ 1.4417,  1.8395,  0.6025]], device='cuda:0'), tensor([[-1.4510e-03, -7.6506e-01,  4.9661e-02],
        [-2.7840e-01,  3.7254e-01,  2.5925e+00]], device='cuda:0'), tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0'), tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')]
====================
schedule for this step is
[(18, 0), (17, 1)]
inputting microbatch 18 into partition 0
before moving to cuda:0: tensor([[ 1.3462, -0.4426, -0.9446],
        [ 1.4703, -1.2582, -0.1105]], device='cuda:0')
after: tensor([[ 1.3462, -0.4426, -0.9446],
        [ 1.4703, -1.2582, -0.1105]], device='cuda:0')
********************
observing microbatch 18
current batch shape is torch.Size([2, 3])
batch is tensor([[ 1.3462, -0.4426, -0.9446],
        [ 1.4703, -1.2582, -0.1105]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 18
********************
inputting microbatch 17 into partition 1
before moving to cuda:1: tensor([[0.6011, 0.3377, 0.7566, 0.4728],
        [0.3752, 0.6254, 0.6745, 0.6818]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 17
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 17
********************
receiving microbatch 18 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 18, tensor([[0.5234, 0.4246, 0.7101, 0.4992],
        [0.4393, 0.5023, 0.8165, 0.5739]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 18
result is tensor([[0.5234, 0.4246, 0.7101, 0.4992],
        [0.4393, 0.5023, 0.8165, 0.5739]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5234, 0.4246, 0.7101, 0.4992],
        [0.4393, 0.5023, 0.8165, 0.5739]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 17 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 17, tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 17
result is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9638, 0.9642, 0.9959, 0.9985, 0.6515],
        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5234, 0.4246, 0.7101, 0.4992],
        [0.4393, 0.5023, 0.8165, 0.5739]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.2497, -0.7672, -1.2856],
        [-0.6834,  1.4657,  0.1942]], device='cuda:0'), tensor([[-2.5381, -2.7670, -0.6195],
        [-1.8058, -0.8120,  0.4175]], device='cuda:0'), tensor([[-1.3229, -0.6276,  0.7311],
        [-0.4062,  1.1249, -2.8920]], device='cuda:0'), tensor([[ 0.0545, -0.7819, -0.6565],
        [-0.0560,  0.0145,  0.0317]], device='cuda:0'), tensor([[ 0.0978, -0.6353, -0.6948],
        [-1.1120, -0.9551, -0.5714]], device='cuda:0'), tensor([[-0.0489, -1.8358, -0.8946],
        [-0.4892,  0.7041, -0.3016]], device='cuda:0'), tensor([[-3.0215, -0.2830, -0.0862],
        [ 1.9947, -0.4977,  1.2364]], device='cuda:0'), tensor([[-0.9189, -0.6557, -1.1053],
        [ 0.4981,  1.0123, -0.4752]], device='cuda:0'), tensor([[ 0.3338, -0.8981,  0.6763],
        [-0.4201, -0.1442,  0.8817]], device='cuda:0'), tensor([[ 0.3618, -1.8074,  1.7514],
        [ 1.4417,  1.8395,  0.6025]], device='cuda:0'), tensor([[-1.4510e-03, -7.6506e-01,  4.9661e-02],
        [-2.7840e-01,  3.7254e-01,  2.5925e+00]], device='cuda:0'), tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0'), tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')]
====================
schedule for this step is
[(19, 0), (18, 1)]
inputting microbatch 19 into partition 0
before moving to cuda:0: tensor([[ 0.2497, -0.7672, -1.2856],
        [-0.6834,  1.4657,  0.1942]], device='cuda:0')
after: tensor([[ 0.2497, -0.7672, -1.2856],
        [-0.6834,  1.4657,  0.1942]], device='cuda:0')
********************
observing microbatch 19
current batch shape is torch.Size([2, 3])
batch is tensor([[ 0.2497, -0.7672, -1.2856],
        [-0.6834,  1.4657,  0.1942]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 19
********************
inputting microbatch 18 into partition 1
before moving to cuda:1: tensor([[0.5234, 0.4246, 0.7101, 0.4992],
        [0.4393, 0.5023, 0.8165, 0.5739]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 18
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 18
********************
receiving microbatch 19 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 19, tensor([[0.4371, 0.5525, 0.6148, 0.6017],
        [0.6757, 0.3868, 0.2460, 0.6237]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 19
result is tensor([[0.4371, 0.5525, 0.6148, 0.6017],
        [0.6757, 0.3868, 0.2460, 0.6237]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4371, 0.5525, 0.6148, 0.6017],
        [0.6757, 0.3868, 0.2460, 0.6237]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 18 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 18, tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 18
result is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9638, 0.9642, 0.9959, 0.9985, 0.6515],
        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4371, 0.5525, 0.6148, 0.6017],
        [0.6757, 0.3868, 0.2460, 0.6237]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-2.5381, -2.7670, -0.6195],
        [-1.8058, -0.8120,  0.4175]], device='cuda:0'), tensor([[-1.3229, -0.6276,  0.7311],
        [-0.4062,  1.1249, -2.8920]], device='cuda:0'), tensor([[ 0.0545, -0.7819, -0.6565],
        [-0.0560,  0.0145,  0.0317]], device='cuda:0'), tensor([[ 0.0978, -0.6353, -0.6948],
        [-1.1120, -0.9551, -0.5714]], device='cuda:0'), tensor([[-0.0489, -1.8358, -0.8946],
        [-0.4892,  0.7041, -0.3016]], device='cuda:0'), tensor([[-3.0215, -0.2830, -0.0862],
        [ 1.9947, -0.4977,  1.2364]], device='cuda:0'), tensor([[-0.9189, -0.6557, -1.1053],
        [ 0.4981,  1.0123, -0.4752]], device='cuda:0'), tensor([[ 0.3338, -0.8981,  0.6763],
        [-0.4201, -0.1442,  0.8817]], device='cuda:0'), tensor([[ 0.3618, -1.8074,  1.7514],
        [ 1.4417,  1.8395,  0.6025]], device='cuda:0'), tensor([[-1.4510e-03, -7.6506e-01,  4.9661e-02],
        [-2.7840e-01,  3.7254e-01,  2.5925e+00]], device='cuda:0'), tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0'), tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')]
====================
schedule for this step is
[(20, 0), (19, 1)]
inputting microbatch 20 into partition 0
before moving to cuda:0: tensor([[-2.5381, -2.7670, -0.6195],
        [-1.8058, -0.8120,  0.4175]], device='cuda:0')
after: tensor([[-2.5381, -2.7670, -0.6195],
        [-1.8058, -0.8120,  0.4175]], device='cuda:0')
********************
observing microbatch 20
current batch shape is torch.Size([2, 3])
batch is tensor([[-2.5381, -2.7670, -0.6195],
        [-1.8058, -0.8120,  0.4175]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 20
********************
inputting microbatch 19 into partition 1
before moving to cuda:1: tensor([[0.4371, 0.5525, 0.6148, 0.6017],
        [0.6757, 0.3868, 0.2460, 0.6237]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 19
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 19
********************
receiving microbatch 20 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 20, tensor([[0.1630, 0.8838, 0.5440, 0.8840],
        [0.3696, 0.7147, 0.3954, 0.8168]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 20
result is tensor([[0.1630, 0.8838, 0.5440, 0.8840],
        [0.3696, 0.7147, 0.3954, 0.8168]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.1630, 0.8838, 0.5440, 0.8840],
        [0.3696, 0.7147, 0.3954, 0.8168]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 19 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 19, tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 19
result is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9638, 0.9642, 0.9959, 0.9985, 0.6515],
        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.1630, 0.8838, 0.5440, 0.8840],
        [0.3696, 0.7147, 0.3954, 0.8168]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-1.3229, -0.6276,  0.7311],
        [-0.4062,  1.1249, -2.8920]], device='cuda:0'), tensor([[ 0.0545, -0.7819, -0.6565],
        [-0.0560,  0.0145,  0.0317]], device='cuda:0'), tensor([[ 0.0978, -0.6353, -0.6948],
        [-1.1120, -0.9551, -0.5714]], device='cuda:0'), tensor([[-0.0489, -1.8358, -0.8946],
        [-0.4892,  0.7041, -0.3016]], device='cuda:0'), tensor([[-3.0215, -0.2830, -0.0862],
        [ 1.9947, -0.4977,  1.2364]], device='cuda:0'), tensor([[-0.9189, -0.6557, -1.1053],
        [ 0.4981,  1.0123, -0.4752]], device='cuda:0'), tensor([[ 0.3338, -0.8981,  0.6763],
        [-0.4201, -0.1442,  0.8817]], device='cuda:0'), tensor([[ 0.3618, -1.8074,  1.7514],
        [ 1.4417,  1.8395,  0.6025]], device='cuda:0'), tensor([[-1.4510e-03, -7.6506e-01,  4.9661e-02],
        [-2.7840e-01,  3.7254e-01,  2.5925e+00]], device='cuda:0'), tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0'), tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')]
====================
schedule for this step is
[(21, 0), (20, 1)]
inputting microbatch 21 into partition 0
before moving to cuda:0: tensor([[-1.3229, -0.6276,  0.7311],
        [-0.4062,  1.1249, -2.8920]], device='cuda:0')
after: tensor([[-1.3229, -0.6276,  0.7311],
        [-0.4062,  1.1249, -2.8920]], device='cuda:0')
********************
observing microbatch 21
current batch shape is torch.Size([2, 3])
batch is tensor([[-1.3229, -0.6276,  0.7311],
        [-0.4062,  1.1249, -2.8920]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 21
********************
inputting microbatch 20 into partition 1
before moving to cuda:1: tensor([[0.1630, 0.8838, 0.5440, 0.8840],
        [0.3696, 0.7147, 0.3954, 0.8168]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 20
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 20
********************
receiving microbatch 21 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 21, tensor([[0.4124, 0.6621, 0.4394, 0.7912],
        [0.6189, 0.4031, 0.2537, 0.4778]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 21
result is tensor([[0.4124, 0.6621, 0.4394, 0.7912],
        [0.6189, 0.4031, 0.2537, 0.4778]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4124, 0.6621, 0.4394, 0.7912],
        [0.6189, 0.4031, 0.2537, 0.4778]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 20 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 20, tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 20
result is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9638, 0.9642, 0.9959, 0.9985, 0.6515],
        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4124, 0.6621, 0.4394, 0.7912],
        [0.6189, 0.4031, 0.2537, 0.4778]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.0545, -0.7819, -0.6565],
        [-0.0560,  0.0145,  0.0317]], device='cuda:0'), tensor([[ 0.0978, -0.6353, -0.6948],
        [-1.1120, -0.9551, -0.5714]], device='cuda:0'), tensor([[-0.0489, -1.8358, -0.8946],
        [-0.4892,  0.7041, -0.3016]], device='cuda:0'), tensor([[-3.0215, -0.2830, -0.0862],
        [ 1.9947, -0.4977,  1.2364]], device='cuda:0'), tensor([[-0.9189, -0.6557, -1.1053],
        [ 0.4981,  1.0123, -0.4752]], device='cuda:0'), tensor([[ 0.3338, -0.8981,  0.6763],
        [-0.4201, -0.1442,  0.8817]], device='cuda:0'), tensor([[ 0.3618, -1.8074,  1.7514],
        [ 1.4417,  1.8395,  0.6025]], device='cuda:0'), tensor([[-1.4510e-03, -7.6506e-01,  4.9661e-02],
        [-2.7840e-01,  3.7254e-01,  2.5925e+00]], device='cuda:0'), tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0'), tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')]
====================
schedule for this step is
[(22, 0), (21, 1)]
inputting microbatch 22 into partition 0
before moving to cuda:0: tensor([[ 0.0545, -0.7819, -0.6565],
        [-0.0560,  0.0145,  0.0317]], device='cuda:0')
after: tensor([[ 0.0545, -0.7819, -0.6565],
        [-0.0560,  0.0145,  0.0317]], device='cuda:0')
********************
observing microbatch 22
current batch shape is torch.Size([2, 3])
batch is tensor([[ 0.0545, -0.7819, -0.6565],
        [-0.0560,  0.0145,  0.0317]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 22
********************
inputting microbatch 21 into partition 1
before moving to cuda:1: tensor([[0.4124, 0.6621, 0.4394, 0.7912],
        [0.6189, 0.4031, 0.2537, 0.4778]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 21
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 21
********************
receiving microbatch 22 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 22, tensor([[0.4341, 0.5697, 0.6066, 0.6467],
        [0.5323, 0.4913, 0.4997, 0.6424]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 22
result is tensor([[0.4341, 0.5697, 0.6066, 0.6467],
        [0.5323, 0.4913, 0.4997, 0.6424]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4341, 0.5697, 0.6066, 0.6467],
        [0.5323, 0.4913, 0.4997, 0.6424]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 21 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 21, tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 21
result is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9638, 0.9642, 0.9959, 0.9985, 0.6515],
        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4341, 0.5697, 0.6066, 0.6467],
        [0.5323, 0.4913, 0.4997, 0.6424]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.0978, -0.6353, -0.6948],
        [-1.1120, -0.9551, -0.5714]], device='cuda:0'), tensor([[-0.0489, -1.8358, -0.8946],
        [-0.4892,  0.7041, -0.3016]], device='cuda:0'), tensor([[-3.0215, -0.2830, -0.0862],
        [ 1.9947, -0.4977,  1.2364]], device='cuda:0'), tensor([[-0.9189, -0.6557, -1.1053],
        [ 0.4981,  1.0123, -0.4752]], device='cuda:0'), tensor([[ 0.3338, -0.8981,  0.6763],
        [-0.4201, -0.1442,  0.8817]], device='cuda:0'), tensor([[ 0.3618, -1.8074,  1.7514],
        [ 1.4417,  1.8395,  0.6025]], device='cuda:0'), tensor([[-1.4510e-03, -7.6506e-01,  4.9661e-02],
        [-2.7840e-01,  3.7254e-01,  2.5925e+00]], device='cuda:0'), tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0'), tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')]
====================
schedule for this step is
[(23, 0), (22, 1)]
inputting microbatch 23 into partition 0
before moving to cuda:0: tensor([[ 0.0978, -0.6353, -0.6948],
        [-1.1120, -0.9551, -0.5714]], device='cuda:0')
after: tensor([[ 0.0978, -0.6353, -0.6948],
        [-1.1120, -0.9551, -0.5714]], device='cuda:0')
********************
observing microbatch 23
current batch shape is torch.Size([2, 3])
batch is tensor([[ 0.0978, -0.6353, -0.6948],
        [-1.1120, -0.9551, -0.5714]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 23
********************
inputting microbatch 22 into partition 1
before moving to cuda:1: tensor([[0.4341, 0.5697, 0.6066, 0.6467],
        [0.5323, 0.4913, 0.4997, 0.6424]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 22
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 22
********************
receiving microbatch 23 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 23, tensor([[0.4528, 0.5502, 0.5917, 0.6335],
        [0.3699, 0.6793, 0.4812, 0.7483]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 23
result is tensor([[0.4528, 0.5502, 0.5917, 0.6335],
        [0.3699, 0.6793, 0.4812, 0.7483]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4528, 0.5502, 0.5917, 0.6335],
        [0.3699, 0.6793, 0.4812, 0.7483]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 22 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 22, tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 22
result is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9638, 0.9642, 0.9959, 0.9985, 0.6515],
        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4528, 0.5502, 0.5917, 0.6335],
        [0.3699, 0.6793, 0.4812, 0.7483]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.0489, -1.8358, -0.8946],
        [-0.4892,  0.7041, -0.3016]], device='cuda:0'), tensor([[-3.0215, -0.2830, -0.0862],
        [ 1.9947, -0.4977,  1.2364]], device='cuda:0'), tensor([[-0.9189, -0.6557, -1.1053],
        [ 0.4981,  1.0123, -0.4752]], device='cuda:0'), tensor([[ 0.3338, -0.8981,  0.6763],
        [-0.4201, -0.1442,  0.8817]], device='cuda:0'), tensor([[ 0.3618, -1.8074,  1.7514],
        [ 1.4417,  1.8395,  0.6025]], device='cuda:0'), tensor([[-1.4510e-03, -7.6506e-01,  4.9661e-02],
        [-2.7840e-01,  3.7254e-01,  2.5925e+00]], device='cuda:0'), tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0'), tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')]
====================
schedule for this step is
[(24, 0), (23, 1)]
inputting microbatch 24 into partition 0
before moving to cuda:0: tensor([[-0.0489, -1.8358, -0.8946],
        [-0.4892,  0.7041, -0.3016]], device='cuda:0')
after: tensor([[-0.0489, -1.8358, -0.8946],
        [-0.4892,  0.7041, -0.3016]], device='cuda:0')
********************
observing microbatch 24
current batch shape is torch.Size([2, 3])
batch is tensor([[-0.0489, -1.8358, -0.8946],
        [-0.4892,  0.7041, -0.3016]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 24
********************
inputting microbatch 23 into partition 1
before moving to cuda:1: tensor([[0.4528, 0.5502, 0.5917, 0.6335],
        [0.3699, 0.6793, 0.4812, 0.7483]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 23
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 23
********************
receiving microbatch 24 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 24, tensor([[0.3102, 0.6855, 0.7192, 0.6998],
        [0.5933, 0.4527, 0.3446, 0.6274]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 24
result is tensor([[0.3102, 0.6855, 0.7192, 0.6998],
        [0.5933, 0.4527, 0.3446, 0.6274]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3102, 0.6855, 0.7192, 0.6998],
        [0.5933, 0.4527, 0.3446, 0.6274]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 23 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 23, tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 23
result is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9638, 0.9642, 0.9959, 0.9985, 0.6515],
        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3102, 0.6855, 0.7192, 0.6998],
        [0.5933, 0.4527, 0.3446, 0.6274]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-3.0215, -0.2830, -0.0862],
        [ 1.9947, -0.4977,  1.2364]], device='cuda:0'), tensor([[-0.9189, -0.6557, -1.1053],
        [ 0.4981,  1.0123, -0.4752]], device='cuda:0'), tensor([[ 0.3338, -0.8981,  0.6763],
        [-0.4201, -0.1442,  0.8817]], device='cuda:0'), tensor([[ 0.3618, -1.8074,  1.7514],
        [ 1.4417,  1.8395,  0.6025]], device='cuda:0'), tensor([[-1.4510e-03, -7.6506e-01,  4.9661e-02],
        [-2.7840e-01,  3.7254e-01,  2.5925e+00]], device='cuda:0'), tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0'), tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')]
====================
schedule for this step is
[(25, 0), (24, 1)]
inputting microbatch 25 into partition 0
before moving to cuda:0: tensor([[-3.0215, -0.2830, -0.0862],
        [ 1.9947, -0.4977,  1.2364]], device='cuda:0')
after: tensor([[-3.0215, -0.2830, -0.0862],
        [ 1.9947, -0.4977,  1.2364]], device='cuda:0')
********************
observing microbatch 25
current batch shape is torch.Size([2, 3])
batch is tensor([[-3.0215, -0.2830, -0.0862],
        [ 1.9947, -0.4977,  1.2364]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 25
********************
inputting microbatch 24 into partition 1
before moving to cuda:1: tensor([[0.3102, 0.6855, 0.7192, 0.6998],
        [0.5933, 0.4527, 0.3446, 0.6274]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 24
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 24
********************
receiving microbatch 25 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 25, tensor([[0.3786, 0.7500, 0.1971, 0.8486],
        [0.5654, 0.3758, 0.8137, 0.5439]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 25
result is tensor([[0.3786, 0.7500, 0.1971, 0.8486],
        [0.5654, 0.3758, 0.8137, 0.5439]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3786, 0.7500, 0.1971, 0.8486],
        [0.5654, 0.3758, 0.8137, 0.5439]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 24 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 24, tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 24
result is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9638, 0.9642, 0.9959, 0.9985, 0.6515],
        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3786, 0.7500, 0.1971, 0.8486],
        [0.5654, 0.3758, 0.8137, 0.5439]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.9189, -0.6557, -1.1053],
        [ 0.4981,  1.0123, -0.4752]], device='cuda:0'), tensor([[ 0.3338, -0.8981,  0.6763],
        [-0.4201, -0.1442,  0.8817]], device='cuda:0'), tensor([[ 0.3618, -1.8074,  1.7514],
        [ 1.4417,  1.8395,  0.6025]], device='cuda:0'), tensor([[-1.4510e-03, -7.6506e-01,  4.9661e-02],
        [-2.7840e-01,  3.7254e-01,  2.5925e+00]], device='cuda:0'), tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0'), tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')]
====================
schedule for this step is
[(26, 0), (25, 1)]
inputting microbatch 26 into partition 0
before moving to cuda:0: tensor([[-0.9189, -0.6557, -1.1053],
        [ 0.4981,  1.0123, -0.4752]], device='cuda:0')
after: tensor([[-0.9189, -0.6557, -1.1053],
        [ 0.4981,  1.0123, -0.4752]], device='cuda:0')
********************
observing microbatch 26
current batch shape is torch.Size([2, 3])
batch is tensor([[-0.9189, -0.6557, -1.1053],
        [ 0.4981,  1.0123, -0.4752]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 26
********************
inputting microbatch 25 into partition 1
before moving to cuda:1: tensor([[0.3786, 0.7500, 0.1971, 0.8486],
        [0.5654, 0.3758, 0.8137, 0.5439]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 25
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 25
********************
receiving microbatch 26 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 26, tensor([[0.4062, 0.6355, 0.4525, 0.7004],
        [0.6632, 0.3411, 0.4226, 0.5134]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 26
result is tensor([[0.4062, 0.6355, 0.4525, 0.7004],
        [0.6632, 0.3411, 0.4226, 0.5134]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4062, 0.6355, 0.4525, 0.7004],
        [0.6632, 0.3411, 0.4226, 0.5134]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 25 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 25, tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 25
result is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9638, 0.9642, 0.9959, 0.9985, 0.6515],
        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4062, 0.6355, 0.4525, 0.7004],
        [0.6632, 0.3411, 0.4226, 0.5134]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.3338, -0.8981,  0.6763],
        [-0.4201, -0.1442,  0.8817]], device='cuda:0'), tensor([[ 0.3618, -1.8074,  1.7514],
        [ 1.4417,  1.8395,  0.6025]], device='cuda:0'), tensor([[-1.4510e-03, -7.6506e-01,  4.9661e-02],
        [-2.7840e-01,  3.7254e-01,  2.5925e+00]], device='cuda:0'), tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0'), tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')]
====================
schedule for this step is
[(27, 0), (26, 1)]
inputting microbatch 27 into partition 0
before moving to cuda:0: tensor([[ 0.3338, -0.8981,  0.6763],
        [-0.4201, -0.1442,  0.8817]], device='cuda:0')
after: tensor([[ 0.3338, -0.8981,  0.6763],
        [-0.4201, -0.1442,  0.8817]], device='cuda:0')
********************
observing microbatch 27
current batch shape is torch.Size([2, 3])
batch is tensor([[ 0.3338, -0.8981,  0.6763],
        [-0.4201, -0.1442,  0.8817]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 27
********************
inputting microbatch 26 into partition 1
before moving to cuda:1: tensor([[0.4062, 0.6355, 0.4525, 0.7004],
        [0.6632, 0.3411, 0.4226, 0.5134]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 26
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 26
********************
receiving microbatch 27 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 27, tensor([[0.4451, 0.5577, 0.6837, 0.6849],
        [0.5075, 0.5386, 0.4941, 0.7139]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 27
result is tensor([[0.4451, 0.5577, 0.6837, 0.6849],
        [0.5075, 0.5386, 0.4941, 0.7139]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4451, 0.5577, 0.6837, 0.6849],
        [0.5075, 0.5386, 0.4941, 0.7139]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 26 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 26, tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 26
result is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9638, 0.9642, 0.9959, 0.9985, 0.6515],
        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4451, 0.5577, 0.6837, 0.6849],
        [0.5075, 0.5386, 0.4941, 0.7139]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.3618, -1.8074,  1.7514],
        [ 1.4417,  1.8395,  0.6025]], device='cuda:0'), tensor([[-1.4510e-03, -7.6506e-01,  4.9661e-02],
        [-2.7840e-01,  3.7254e-01,  2.5925e+00]], device='cuda:0'), tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0'), tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')]
====================
schedule for this step is
[(28, 0), (27, 1)]
inputting microbatch 28 into partition 0
before moving to cuda:0: tensor([[ 0.3618, -1.8074,  1.7514],
        [ 1.4417,  1.8395,  0.6025]], device='cuda:0')
after: tensor([[ 0.3618, -1.8074,  1.7514],
        [ 1.4417,  1.8395,  0.6025]], device='cuda:0')
********************
observing microbatch 28
current batch shape is torch.Size([2, 3])
batch is tensor([[ 0.3618, -1.8074,  1.7514],
        [ 1.4417,  1.8395,  0.6025]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 28
********************
inputting microbatch 27 into partition 1
before moving to cuda:1: tensor([[0.4451, 0.5577, 0.6837, 0.6849],
        [0.5075, 0.5386, 0.4941, 0.7139]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 27
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 27
********************
receiving microbatch 28 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 28, tensor([[0.3524, 0.6497, 0.8010, 0.7649],
        [0.7812, 0.2067, 0.4569, 0.4286]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 28
result is tensor([[0.3524, 0.6497, 0.8010, 0.7649],
        [0.7812, 0.2067, 0.4569, 0.4286]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3524, 0.6497, 0.8010, 0.7649],
        [0.7812, 0.2067, 0.4569, 0.4286]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 27 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 27, tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 27
result is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9638, 0.9642, 0.9959, 0.9985, 0.6515],
        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3524, 0.6497, 0.8010, 0.7649],
        [0.7812, 0.2067, 0.4569, 0.4286]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-1.4510e-03, -7.6506e-01,  4.9661e-02],
        [-2.7840e-01,  3.7254e-01,  2.5925e+00]], device='cuda:0'), tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0'), tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')]
====================
schedule for this step is
[(29, 0), (28, 1)]
inputting microbatch 29 into partition 0
before moving to cuda:0: tensor([[-1.4510e-03, -7.6506e-01,  4.9661e-02],
        [-2.7840e-01,  3.7254e-01,  2.5925e+00]], device='cuda:0')
after: tensor([[-1.4510e-03, -7.6506e-01,  4.9661e-02],
        [-2.7840e-01,  3.7254e-01,  2.5925e+00]], device='cuda:0')
********************
observing microbatch 29
current batch shape is torch.Size([2, 3])
batch is tensor([[-1.4510e-03, -7.6506e-01,  4.9661e-02],
        [-2.7840e-01,  3.7254e-01,  2.5925e+00]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 29
********************
inputting microbatch 28 into partition 1
before moving to cuda:1: tensor([[0.3524, 0.6497, 0.8010, 0.7649],
        [0.7812, 0.2067, 0.4569, 0.4286]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 28
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 28
********************
receiving microbatch 29 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 29, tensor([[0.4412, 0.5718, 0.6134, 0.6797],
        [0.5923, 0.4683, 0.4818, 0.7424]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 29
result is tensor([[0.4412, 0.5718, 0.6134, 0.6797],
        [0.5923, 0.4683, 0.4818, 0.7424]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4412, 0.5718, 0.6134, 0.6797],
        [0.5923, 0.4683, 0.4818, 0.7424]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 28 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 28, tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 28
result is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9638, 0.9642, 0.9959, 0.9985, 0.6515],
        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4412, 0.5718, 0.6134, 0.6797],
        [0.5923, 0.4683, 0.4818, 0.7424]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0'), tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')]
====================
schedule for this step is
[(30, 0), (29, 1)]
inputting microbatch 30 into partition 0
before moving to cuda:0: tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0')
after: tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0')
********************
observing microbatch 30
current batch shape is torch.Size([2, 3])
batch is tensor([[ 0.3813, -0.9408, -0.2687],
        [ 0.4360, -0.5742,  1.3510]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 30
********************
inputting microbatch 29 into partition 1
before moving to cuda:1: tensor([[0.4412, 0.5718, 0.6134, 0.6797],
        [0.5923, 0.4683, 0.4818, 0.7424]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 29
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 29
********************
receiving microbatch 30 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 30, tensor([[0.4322, 0.5593, 0.6747, 0.6443],
        [0.4951, 0.5131, 0.6702, 0.6874]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 30
result is tensor([[0.4322, 0.5593, 0.6747, 0.6443],
        [0.4951, 0.5131, 0.6702, 0.6874]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4322, 0.5593, 0.6747, 0.6443],
        [0.4951, 0.5131, 0.6702, 0.6874]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 29 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 29, tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 29
result is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9638, 0.9642, 0.9959, 0.9985, 0.6515],
        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4322, 0.5593, 0.6747, 0.6443],
        [0.4951, 0.5131, 0.6702, 0.6874]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')]
====================
schedule for this step is
[(31, 0), (30, 1)]
inputting microbatch 31 into partition 0
before moving to cuda:0: tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')
after: tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')
********************
observing microbatch 31
current batch shape is torch.Size([2, 3])
batch is tensor([[-0.6678, -1.3018,  1.0124],
        [ 1.0930, -0.1947,  1.9428]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 31
********************
inputting microbatch 30 into partition 1
before moving to cuda:1: tensor([[0.4322, 0.5593, 0.6747, 0.6443],
        [0.4951, 0.5131, 0.6702, 0.6874]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 30
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 30
********************
receiving microbatch 31 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 31, tensor([[0.3632, 0.6784, 0.6227, 0.7861],
        [0.5728, 0.4162, 0.7109, 0.6381]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 31
result is tensor([[0.3632, 0.6784, 0.6227, 0.7861],
        [0.5728, 0.4162, 0.7109, 0.6381]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3632, 0.6784, 0.6227, 0.7861],
        [0.5728, 0.4162, 0.7109, 0.6381]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 30 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 30, tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 30
result is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9638, 0.9642, 0.9959, 0.9985, 0.6515],
        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3632, 0.6784, 0.6227, 0.7861],
        [0.5728, 0.4162, 0.7109, 0.6381]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)]
====================
schedule for this step is
[(31, 1)]
inputting microbatch 31 into partition 1
before moving to cuda:1: tensor([[0.3632, 0.6784, 0.6227, 0.7861],
        [0.5728, 0.4162, 0.7109, 0.6381]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 31
current batch shape is torch.Size([2, 4])
batch is tensor([[0.9639, 0.9643, 0.9960, 0.9985],
        [0.6515, 0.9561, 0.9513, 0.9940]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 31
********************
receiving microbatch 31 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 31, tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 31
result is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9365, 0.9372, 0.9872, 0.9981, 0.6515],
        [0.9393, 0.9395, 0.9886, 0.9965, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9622, 0.9626, 0.9957, 0.9984, 0.6515],
        [0.9547, 0.9501, 0.9937, 0.9971, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9638, 0.9642, 0.9959, 0.9985, 0.6515],
        [0.9560, 0.9512, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9639, 0.9643, 0.9960, 0.9985, 0.6515],
        [0.9561, 0.9513, 0.9940, 0.9972, 0.6515]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)]
finished running cycle
_____________________________ test_forward_0[4-1] ______________________________

batch_size = 1, split_size = 4

    @pytest.mark.a4_2_2
    @pytest.mark.parametrize("batch_size", [1, 16, 32, 64])
    @pytest.mark.parametrize("split_size", [1, 2, 4, 8, 16])
    # @pytest.mark.parametrize("batch_size", [1])
    # @pytest.mark.parametrize("split_size", [1, 2])
    def test_forward_0(batch_size, split_size):
        model = nn.Sequential(
            nn.Linear(3, 4).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
            nn.Linear(4, 5).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
        )
    
        x = torch.randn(batch_size, 3).to('cuda:0')
        y0 = model(x).to('cpu')
    
        # move the last two layer to another device
        model[-2] = model[-2].to('cuda:1')
        model[-1] = WithDevice(nn.Sigmoid(), 'cuda:1')
        pipe = Pipe(model, split_size=split_size)
        y1 = pipe(x).to('cpu')
>       assert torch.allclose(y0, y1)
E       assert False
E        +  where False = <built-in method allclose of type object at 0x153da78cb760>(tensor([[0.4215, 0.4766, 0.5560, 0.5169, 0.4414]], grad_fn=<ToCopyBackward0>), tensor([[0.9884, 0.9835, 0.9853, 0.9823, 0.6573]], grad_fn=<ToCopyBackward0>))
E        +    where <built-in method allclose of type object at 0x153da78cb760> = torch.allclose

batch_size = 1
model      = Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
  (2): Linear(in_features=4, out_features=5, bias=True)
  (3): WithDevice(
    (_module): Sigmoid()
  )
)
pipe       = Pipe(
  (partitions): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=3, out_features=4, bias=True)
   ...inear(in_features=4, out_features=5, bias=True)
      (1): WithDevice(
        (_module): Sigmoid()
      )
    )
  )
)
split_size = 4
x          = tensor([[ 1.9735, -1.3235,  1.2081]], device='cuda:0')
y0         = tensor([[0.4215, 0.4766, 0.5560, 0.5169, 0.4414]], grad_fn=<ToCopyBackward0>)
y1         = tensor([[0.9884, 0.9835, 0.9853, 0.9823, 0.6573]], grad_fn=<ToCopyBackward0>)

tests/test_pipeline.py:83: AssertionError
----------------------------- Captured stdout call -----------------------------
in init
ModuleList(
  (0): Sequential(
    (0): Linear(in_features=3, out_features=4, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
  (1): Sequential(
    (0): Linear(in_features=4, out_features=5, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
)
[device(type='cuda', index=0), device(type='cuda', index=1)]
before running the cycle
[tensor([[ 1.9735, -1.3235,  1.2081]], device='cuda:0')]
====================
schedule for this step is
[(0, 0)]
inputting microbatch 0 into partition 0
before moving to cuda:0: tensor([[ 1.9735, -1.3235,  1.2081]], device='cuda:0')
after: tensor([[ 1.9735, -1.3235,  1.2081]], device='cuda:0')
********************
observing microbatch 0
current batch shape is torch.Size([1, 3])
batch is tensor([[ 1.9735, -1.3235,  1.2081]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 0
********************
receiving microbatch 0 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 0, tensor([[0.3288, 0.6465, 0.2210, 0.4747]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.3288, 0.6465, 0.2210, 0.4747]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3288, 0.6465, 0.2210, 0.4747]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.3288, 0.6465, 0.2210, 0.4747]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)]
====================
schedule for this step is
[(0, 1)]
inputting microbatch 0 into partition 1
before moving to cuda:1: tensor([[0.3288, 0.6465, 0.2210, 0.4747]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9972, 0.6515, 0.9639, 0.9643]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 0
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9972, 0.6515, 0.9639, 0.9643]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 0
********************
receiving microbatch 0 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 0, tensor([[0.9884, 0.9835, 0.9853, 0.9823, 0.6573]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.9884, 0.9835, 0.9853, 0.9823, 0.6573]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9884, 0.9835, 0.9853, 0.9823, 0.6573]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9884, 0.9835, 0.9853, 0.9823, 0.6573]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)]
finished running cycle
_____________________________ test_forward_0[4-16] _____________________________

batch_size = 16, split_size = 4

    @pytest.mark.a4_2_2
    @pytest.mark.parametrize("batch_size", [1, 16, 32, 64])
    @pytest.mark.parametrize("split_size", [1, 2, 4, 8, 16])
    # @pytest.mark.parametrize("batch_size", [1])
    # @pytest.mark.parametrize("split_size", [1, 2])
    def test_forward_0(batch_size, split_size):
        model = nn.Sequential(
            nn.Linear(3, 4).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
            nn.Linear(4, 5).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
        )
    
        x = torch.randn(batch_size, 3).to('cuda:0')
        y0 = model(x).to('cpu')
    
        # move the last two layer to another device
        model[-2] = model[-2].to('cuda:1')
        model[-1] = WithDevice(nn.Sigmoid(), 'cuda:1')
        pipe = Pipe(model, split_size=split_size)
        y1 = pipe(x).to('cpu')
>       assert torch.allclose(y0, y1)
E       assert False
E        +  where False = <built-in method allclose of type object at 0x153da78cb760>(tensor([[0.4500, 0.5000, 0.5923, 0.3900, 0.4303],\n        [0.4738, 0.5365, 0.5504, 0.3494, 0.4563],\n        [0.4042, 0.4918, 0.5021, 0.3750, 0.5112],\n        [0.4479, 0.5153, 0.5586, 0.3650, 0.4542],\n        [0.4557, 0.5192, 0.5628, 0.3640, 0.4495],\n        [0.4669, 0.5298, 0.5460, 0.3553, 0.4633],\n        [0.4535, 0.5203, 0.5549, 0.3611, 0.4563],\n        [0.4438, 0.5223, 0.5300, 0.3517, 0.4779],\n        [0.4122, 0.4847, 0.5647, 0.3884, 0.4581],\n        [0.4287, 0.5013, 0.5458, 0.3739, 0.4717],\n        [0.3905, 0.4797, 0.5483, 0.3818, 0.4707],\n        [0.3915, 0.4813, 0.5243, 0.3817, 0.4926],\n        [0.4733, 0.5348, 0.5427, 0.3518, 0.4652],\n        [0.4863, 0.5454, 0.5582, 0.3432, 0.4468],\n        [0.4433, 0.5039, 0.5736, 0.3795, 0.4451],\n        [0.4728, 0.5405, 0.5317, 0.3430, 0.4723]], grad_fn=<ToCopyBackward0>), tensor([[0.0000, 0.8962, 0.8068, 0.5209, 0.7351],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.8568, 0.7282, 0.7276, 0.6587],\n        [0.0000, 0.9880, 0.9725, 0.9737, 0.9676],\n        [0.0000, 0.9883, 0.9731, 0.9746, 0.9658],\n        [0.0000, 0.8568, 0.7282, 0.7276, 0.6587],\n        [0.0000, 0.8568, 0.7282, 0.7276, 0.6587],\n        [0.0000, 0.9951, 0.9884, 0.9887, 0.9848],\n        [0.0000, 0.9951, 0.9884, 0.9887, 0.9848],\n        [0.0000, 0.9883, 0.9731, 0.9746, 0.9658],\n        [0.0000, 0.9883, 0.9731, 0.9746, 0.9658],\n        [0.0000, 0.9953, 0.9890, 0.9892, 0.9855],\n        [0.0000, 0.9953, 0.9890, 0.9892, 0.9855],\n        [0.0000, 0.9951, 0.9884, 0.9887, 0.9848],\n        [0.0000, 0.9951, 0.9884, 0.9887, 0.9848]], grad_fn=<ToCopyBackward0>))
E        +    where <built-in method allclose of type object at 0x153da78cb760> = torch.allclose

batch_size = 16
model      = Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
  (2): Linear(in_features=4, out_features=5, bias=True)
  (3): WithDevice(
    (_module): Sigmoid()
  )
)
pipe       = Pipe(
  (partitions): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=3, out_features=4, bias=True)
   ...inear(in_features=4, out_features=5, bias=True)
      (1): WithDevice(
        (_module): Sigmoid()
      )
    )
  )
)
split_size = 4
x          = tensor([[ 0.1226, -0.0290, -2.1123],
        [-0.6777, -0.8866,  0.7281],
        [ 1.7827,  0.9968,  1.7658],
       ...-1.2191, -1.3099,  0.5654],
        [-0.1844,  0.3277, -0.9426],
        [-0.2818, -1.1182,  1.6039]], device='cuda:0')
y0         = tensor([[0.4500, 0.5000, 0.5923, 0.3900, 0.4303],
        [0.4738, 0.5365, 0.5504, 0.3494, 0.4563],
        [0.4042, 0...[0.4433, 0.5039, 0.5736, 0.3795, 0.4451],
        [0.4728, 0.5405, 0.5317, 0.3430, 0.4723]], grad_fn=<ToCopyBackward0>)
y1         = tensor([[0.0000, 0.8962, 0.8068, 0.5209, 0.7351],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0...[0.0000, 0.9951, 0.9884, 0.9887, 0.9848],
        [0.0000, 0.9951, 0.9884, 0.9887, 0.9848]], grad_fn=<ToCopyBackward0>)

tests/test_pipeline.py:83: AssertionError
----------------------------- Captured stdout call -----------------------------
in init
ModuleList(
  (0): Sequential(
    (0): Linear(in_features=3, out_features=4, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
  (1): Sequential(
    (0): Linear(in_features=4, out_features=5, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
)
[device(type='cuda', index=0), device(type='cuda', index=1)]
before running the cycle
[tensor([[ 0.1226, -0.0290, -2.1123],
        [-0.6777, -0.8866,  0.7281],
        [ 1.7827,  0.9968,  1.7658],
        [-0.6469,  0.2670,  0.1094]], device='cuda:0'), tensor([[-0.6703, -0.0358, -0.0544],
        [-0.0369, -0.8804,  0.6471],
        [-0.5965, -0.0091,  0.3197],
        [-0.4831,  0.2086,  1.4993]], device='cuda:0'), tensor([[ 0.1533,  1.4780, -1.0336],
        [ 0.3832,  0.5359,  0.1162],
        [-0.7568,  2.7789,  0.1425],
        [ 0.5807,  2.2228,  0.7964]], device='cuda:0'), tensor([[ 0.1039, -1.2552,  0.8196],
        [-1.2191, -1.3099,  0.5654],
        [-0.1844,  0.3277, -0.9426],
        [-0.2818, -1.1182,  1.6039]], device='cuda:0')]
====================
schedule for this step is
[(0, 0)]
inputting microbatch 0 into partition 0
before moving to cuda:0: tensor([[ 0.1226, -0.0290, -2.1123],
        [-0.6777, -0.8866,  0.7281],
        [ 1.7827,  0.9968,  1.7658],
        [-0.6469,  0.2670,  0.1094]], device='cuda:0')
after: tensor([[ 0.1226, -0.0290, -2.1123],
        [-0.6777, -0.8866,  0.7281],
        [ 1.7827,  0.9968,  1.7658],
        [-0.6469,  0.2670,  0.1094]], device='cuda:0')
********************
observing microbatch 0
current batch shape is torch.Size([4, 3])
batch is tensor([[ 0.1226, -0.0290, -2.1123],
        [-0.6777, -0.8866,  0.7281],
        [ 1.7827,  0.9968,  1.7658],
        [-0.6469,  0.2670,  0.1094]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 0
********************
receiving microbatch 0 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 0, tensor([[0.5731, 0.7185, 0.6884, 0.3137],
        [0.3963, 0.7173, 0.6623, 0.6427],
        [0.2253, 0.1477, 0.2966, 0.2060],
        [0.4976, 0.6106, 0.6237, 0.4842]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.5731, 0.7185, 0.6884, 0.3137],
        [0.3963, 0.7173, 0.6623, 0.6427],
        [0.2253, 0.1477, 0.2966, 0.2060],
        [0.4976, 0.6106, 0.6237, 0.4842]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5731, 0.7185, 0.6884, 0.3137],
        [0.3963, 0.7173, 0.6623, 0.6427],
        [0.2253, 0.1477, 0.2966, 0.2060],
        [0.4976, 0.6106, 0.6237, 0.4842]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.5731, 0.7185, 0.6884, 0.3137],
        [0.3963, 0.7173, 0.6623, 0.6427],
        [0.2253, 0.1477, 0.2966, 0.2060],
        [0.4976, 0.6106, 0.6237, 0.4842]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.6703, -0.0358, -0.0544],
        [-0.0369, -0.8804,  0.6471],
        [-0.5965, -0.0091,  0.3197],
        [-0.4831,  0.2086,  1.4993]], device='cuda:0'), tensor([[ 0.1533,  1.4780, -1.0336],
        [ 0.3832,  0.5359,  0.1162],
        [-0.7568,  2.7789,  0.1425],
        [ 0.5807,  2.2228,  0.7964]], device='cuda:0'), tensor([[ 0.1039, -1.2552,  0.8196],
        [-1.2191, -1.3099,  0.5654],
        [-0.1844,  0.3277, -0.9426],
        [-0.2818, -1.1182,  1.6039]], device='cuda:0')]
====================
schedule for this step is
[(1, 0), (0, 1)]
inputting microbatch 1 into partition 0
before moving to cuda:0: tensor([[-0.6703, -0.0358, -0.0544],
        [-0.0369, -0.8804,  0.6471],
        [-0.5965, -0.0091,  0.3197],
        [-0.4831,  0.2086,  1.4993]], device='cuda:0')
after: tensor([[-0.6703, -0.0358, -0.0544],
        [-0.0369, -0.8804,  0.6471],
        [-0.5965, -0.0091,  0.3197],
        [-0.4831,  0.2086,  1.4993]], device='cuda:0')
********************
observing microbatch 1
current batch shape is torch.Size([4, 3])
batch is tensor([[-0.6703, -0.0358, -0.0544],
        [-0.0369, -0.8804,  0.6471],
        [-0.5965, -0.0091,  0.3197],
        [-0.4831,  0.2086,  1.4993]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 1
********************
inputting microbatch 0 into partition 1
before moving to cuda:1: tensor([[0.5731, 0.7185, 0.6884, 0.3137],
        [0.3963, 0.7173, 0.6623, 0.6427],
        [0.2253, 0.1477, 0.2966, 0.2060],
        [0.4976, 0.6106, 0.6237, 0.4842]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[-2.0000e+00,  1.8721e+00, -2.0000e+00,  1.8709e+00],
        [ 0.0000e+00,  1.8713e+00, -3.6893e+19,  1.8706e+00],
        [-3.6893e+19,  1.7893e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 0
current batch shape is torch.Size([4, 4])
batch is tensor([[-2.0000e+00,  1.8721e+00, -2.0000e+00,  1.8709e+00],
        [ 0.0000e+00,  1.8713e+00, -3.6893e+19,  1.8706e+00],
        [-3.6893e+19,  1.7893e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 0
********************
receiving microbatch 1 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 1, tensor([[0.4959, 0.6657, 0.6513, 0.5127],
        [0.3471, 0.6511, 0.6155, 0.5567],
        [0.4632, 0.6261, 0.6254, 0.5185],
        [0.3803, 0.4883, 0.5415, 0.5379]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 1
result is tensor([[0.4959, 0.6657, 0.6513, 0.5127],
        [0.3471, 0.6511, 0.6155, 0.5567],
        [0.4632, 0.6261, 0.6254, 0.5185],
        [0.3803, 0.4883, 0.5415, 0.5379]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4959, 0.6657, 0.6513, 0.5127],
        [0.3471, 0.6511, 0.6155, 0.5567],
        [0.4632, 0.6261, 0.6254, 0.5185],
        [0.3803, 0.4883, 0.5415, 0.5379]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 0 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 0, tensor([[0.0000, 0.8962, 0.8068, 0.5209, 0.7351],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.8568, 0.7282, 0.7276, 0.6587]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.0000, 0.8962, 0.8068, 0.5209, 0.7351],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.8568, 0.7282, 0.7276, 0.6587]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.8962, 0.8068, 0.5209, 0.7351],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.8568, 0.7282, 0.7276, 0.6587]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.0000, 0.8962, 0.8068, 0.5209, 0.7351],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.8568, 0.7282, 0.7276, 0.6587]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4959, 0.6657, 0.6513, 0.5127],
        [0.3471, 0.6511, 0.6155, 0.5567],
        [0.4632, 0.6261, 0.6254, 0.5185],
        [0.3803, 0.4883, 0.5415, 0.5379]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.1533,  1.4780, -1.0336],
        [ 0.3832,  0.5359,  0.1162],
        [-0.7568,  2.7789,  0.1425],
        [ 0.5807,  2.2228,  0.7964]], device='cuda:0'), tensor([[ 0.1039, -1.2552,  0.8196],
        [-1.2191, -1.3099,  0.5654],
        [-0.1844,  0.3277, -0.9426],
        [-0.2818, -1.1182,  1.6039]], device='cuda:0')]
====================
schedule for this step is
[(2, 0), (1, 1)]
inputting microbatch 2 into partition 0
before moving to cuda:0: tensor([[ 0.1533,  1.4780, -1.0336],
        [ 0.3832,  0.5359,  0.1162],
        [-0.7568,  2.7789,  0.1425],
        [ 0.5807,  2.2228,  0.7964]], device='cuda:0')
after: tensor([[ 0.1533,  1.4780, -1.0336],
        [ 0.3832,  0.5359,  0.1162],
        [-0.7568,  2.7789,  0.1425],
        [ 0.5807,  2.2228,  0.7964]], device='cuda:0')
********************
observing microbatch 2
current batch shape is torch.Size([4, 3])
batch is tensor([[ 0.1533,  1.4780, -1.0336],
        [ 0.3832,  0.5359,  0.1162],
        [-0.7568,  2.7789,  0.1425],
        [ 0.5807,  2.2228,  0.7964]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 2
********************
inputting microbatch 1 into partition 1
before moving to cuda:1: tensor([[0.4959, 0.6657, 0.6513, 0.5127],
        [0.3471, 0.6511, 0.6155, 0.5567],
        [0.4632, 0.6261, 0.6254, 0.5185],
        [0.3803, 0.4883, 0.5415, 0.5379]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.8962, 0.8068, 0.5209, 0.7351],
        [0.8568, 0.7282, 0.7276, 0.6587],
        [0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 1
current batch shape is torch.Size([4, 4])
batch is tensor([[0.8962, 0.8068, 0.5209, 0.7351],
        [0.8568, 0.7282, 0.7276, 0.6587],
        [0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 1
********************
receiving microbatch 2 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 2, tensor([[0.5705, 0.4274, 0.5428, 0.2223],
        [0.4172, 0.4409, 0.5210, 0.3255],
        [0.6337, 0.2789, 0.4758, 0.2476],
        [0.4373, 0.1783, 0.3650, 0.1906]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 2
result is tensor([[0.5705, 0.4274, 0.5428, 0.2223],
        [0.4172, 0.4409, 0.5210, 0.3255],
        [0.6337, 0.2789, 0.4758, 0.2476],
        [0.4373, 0.1783, 0.3650, 0.1906]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5705, 0.4274, 0.5428, 0.2223],
        [0.4172, 0.4409, 0.5210, 0.3255],
        [0.6337, 0.2789, 0.4758, 0.2476],
        [0.4373, 0.1783, 0.3650, 0.1906]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 1 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 1, tensor([[0.0000, 0.9880, 0.9725, 0.9737, 0.9676],
        [0.0000, 0.9883, 0.9731, 0.9746, 0.9658],
        [0.0000, 0.8568, 0.7282, 0.7276, 0.6587],
        [0.0000, 0.8568, 0.7282, 0.7276, 0.6587]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 1
result is tensor([[0.0000, 0.9880, 0.9725, 0.9737, 0.9676],
        [0.0000, 0.9883, 0.9731, 0.9746, 0.9658],
        [0.0000, 0.8568, 0.7282, 0.7276, 0.6587],
        [0.0000, 0.8568, 0.7282, 0.7276, 0.6587]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9880, 0.9725, 0.9737, 0.9676],
        [0.0000, 0.9883, 0.9731, 0.9746, 0.9658],
        [0.0000, 0.8568, 0.7282, 0.7276, 0.6587],
        [0.0000, 0.8568, 0.7282, 0.7276, 0.6587]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.0000, 0.8962, 0.8068, 0.5209, 0.7351],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.8568, 0.7282, 0.7276, 0.6587]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9880, 0.9725, 0.9737, 0.9676],
        [0.0000, 0.9883, 0.9731, 0.9746, 0.9658],
        [0.0000, 0.8568, 0.7282, 0.7276, 0.6587],
        [0.0000, 0.8568, 0.7282, 0.7276, 0.6587]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5705, 0.4274, 0.5428, 0.2223],
        [0.4172, 0.4409, 0.5210, 0.3255],
        [0.6337, 0.2789, 0.4758, 0.2476],
        [0.4373, 0.1783, 0.3650, 0.1906]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.1039, -1.2552,  0.8196],
        [-1.2191, -1.3099,  0.5654],
        [-0.1844,  0.3277, -0.9426],
        [-0.2818, -1.1182,  1.6039]], device='cuda:0')]
====================
schedule for this step is
[(3, 0), (2, 1)]
inputting microbatch 3 into partition 0
before moving to cuda:0: tensor([[ 0.1039, -1.2552,  0.8196],
        [-1.2191, -1.3099,  0.5654],
        [-0.1844,  0.3277, -0.9426],
        [-0.2818, -1.1182,  1.6039]], device='cuda:0')
after: tensor([[ 0.1039, -1.2552,  0.8196],
        [-1.2191, -1.3099,  0.5654],
        [-0.1844,  0.3277, -0.9426],
        [-0.2818, -1.1182,  1.6039]], device='cuda:0')
********************
observing microbatch 3
current batch shape is torch.Size([4, 3])
batch is tensor([[ 0.1039, -1.2552,  0.8196],
        [-1.2191, -1.3099,  0.5654],
        [-0.1844,  0.3277, -0.9426],
        [-0.2818, -1.1182,  1.6039]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 3
********************
inputting microbatch 2 into partition 1
before moving to cuda:1: tensor([[0.5705, 0.4274, 0.5428, 0.2223],
        [0.4172, 0.4409, 0.5210, 0.3255],
        [0.6337, 0.2789, 0.4758, 0.2476],
        [0.4373, 0.1783, 0.3650, 0.1906]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9880, 0.9725, 0.9737, 0.9676],
        [0.9883, 0.9731, 0.9746, 0.9658],
        [0.8568, 0.7282, 0.7276, 0.6587],
        [0.8568, 0.7282, 0.7276, 0.6587]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 2
current batch shape is torch.Size([4, 4])
batch is tensor([[0.9880, 0.9725, 0.9737, 0.9676],
        [0.9883, 0.9731, 0.9746, 0.9658],
        [0.8568, 0.7282, 0.7276, 0.6587],
        [0.8568, 0.7282, 0.7276, 0.6587]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 2
********************
receiving microbatch 3 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 3, tensor([[0.3073, 0.6707, 0.6177, 0.5874],
        [0.4347, 0.8184, 0.7311, 0.7370],
        [0.5350, 0.6258, 0.6358, 0.3670],
        [0.2950, 0.6410, 0.6009, 0.6578]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 3
result is tensor([[0.3073, 0.6707, 0.6177, 0.5874],
        [0.4347, 0.8184, 0.7311, 0.7370],
        [0.5350, 0.6258, 0.6358, 0.3670],
        [0.2950, 0.6410, 0.6009, 0.6578]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3073, 0.6707, 0.6177, 0.5874],
        [0.4347, 0.8184, 0.7311, 0.7370],
        [0.5350, 0.6258, 0.6358, 0.3670],
        [0.2950, 0.6410, 0.6009, 0.6578]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 2 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 2, tensor([[0.0000, 0.9951, 0.9884, 0.9887, 0.9848],
        [0.0000, 0.9951, 0.9884, 0.9887, 0.9848],
        [0.0000, 0.9883, 0.9731, 0.9746, 0.9658],
        [0.0000, 0.9883, 0.9731, 0.9746, 0.9658]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 2
result is tensor([[0.0000, 0.9951, 0.9884, 0.9887, 0.9848],
        [0.0000, 0.9951, 0.9884, 0.9887, 0.9848],
        [0.0000, 0.9883, 0.9731, 0.9746, 0.9658],
        [0.0000, 0.9883, 0.9731, 0.9746, 0.9658]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9951, 0.9884, 0.9887, 0.9848],
        [0.0000, 0.9951, 0.9884, 0.9887, 0.9848],
        [0.0000, 0.9883, 0.9731, 0.9746, 0.9658],
        [0.0000, 0.9883, 0.9731, 0.9746, 0.9658]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.0000, 0.8962, 0.8068, 0.5209, 0.7351],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.8568, 0.7282, 0.7276, 0.6587]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9880, 0.9725, 0.9737, 0.9676],
        [0.0000, 0.9883, 0.9731, 0.9746, 0.9658],
        [0.0000, 0.8568, 0.7282, 0.7276, 0.6587],
        [0.0000, 0.8568, 0.7282, 0.7276, 0.6587]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9951, 0.9884, 0.9887, 0.9848],
        [0.0000, 0.9951, 0.9884, 0.9887, 0.9848],
        [0.0000, 0.9883, 0.9731, 0.9746, 0.9658],
        [0.0000, 0.9883, 0.9731, 0.9746, 0.9658]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3073, 0.6707, 0.6177, 0.5874],
        [0.4347, 0.8184, 0.7311, 0.7370],
        [0.5350, 0.6258, 0.6358, 0.3670],
        [0.2950, 0.6410, 0.6009, 0.6578]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)]
====================
schedule for this step is
[(3, 1)]
inputting microbatch 3 into partition 1
before moving to cuda:1: tensor([[0.3073, 0.6707, 0.6177, 0.5874],
        [0.4347, 0.8184, 0.7311, 0.7370],
        [0.5350, 0.6258, 0.6358, 0.3670],
        [0.2950, 0.6410, 0.6009, 0.6578]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9951, 0.9884, 0.9887, 0.9848],
        [0.9951, 0.9884, 0.9887, 0.9848],
        [0.9883, 0.9731, 0.9746, 0.9658],
        [0.9883, 0.9731, 0.9746, 0.9658]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 3
current batch shape is torch.Size([4, 4])
batch is tensor([[0.9951, 0.9884, 0.9887, 0.9848],
        [0.9951, 0.9884, 0.9887, 0.9848],
        [0.9883, 0.9731, 0.9746, 0.9658],
        [0.9883, 0.9731, 0.9746, 0.9658]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 3
********************
receiving microbatch 3 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 3, tensor([[0.0000, 0.9953, 0.9890, 0.9892, 0.9855],
        [0.0000, 0.9953, 0.9890, 0.9892, 0.9855],
        [0.0000, 0.9951, 0.9884, 0.9887, 0.9848],
        [0.0000, 0.9951, 0.9884, 0.9887, 0.9848]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 3
result is tensor([[0.0000, 0.9953, 0.9890, 0.9892, 0.9855],
        [0.0000, 0.9953, 0.9890, 0.9892, 0.9855],
        [0.0000, 0.9951, 0.9884, 0.9887, 0.9848],
        [0.0000, 0.9951, 0.9884, 0.9887, 0.9848]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.9953, 0.9890, 0.9892, 0.9855],
        [0.0000, 0.9953, 0.9890, 0.9892, 0.9855],
        [0.0000, 0.9951, 0.9884, 0.9887, 0.9848],
        [0.0000, 0.9951, 0.9884, 0.9887, 0.9848]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.0000, 0.8962, 0.8068, 0.5209, 0.7351],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.8568, 0.7282, 0.7276, 0.6587]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9880, 0.9725, 0.9737, 0.9676],
        [0.0000, 0.9883, 0.9731, 0.9746, 0.9658],
        [0.0000, 0.8568, 0.7282, 0.7276, 0.6587],
        [0.0000, 0.8568, 0.7282, 0.7276, 0.6587]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9951, 0.9884, 0.9887, 0.9848],
        [0.0000, 0.9951, 0.9884, 0.9887, 0.9848],
        [0.0000, 0.9883, 0.9731, 0.9746, 0.9658],
        [0.0000, 0.9883, 0.9731, 0.9746, 0.9658]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.9953, 0.9890, 0.9892, 0.9855],
        [0.0000, 0.9953, 0.9890, 0.9892, 0.9855],
        [0.0000, 0.9951, 0.9884, 0.9887, 0.9848],
        [0.0000, 0.9951, 0.9884, 0.9887, 0.9848]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)]
finished running cycle
_____________________________ test_forward_0[4-32] _____________________________

batch_size = 32, split_size = 4

    @pytest.mark.a4_2_2
    @pytest.mark.parametrize("batch_size", [1, 16, 32, 64])
    @pytest.mark.parametrize("split_size", [1, 2, 4, 8, 16])
    # @pytest.mark.parametrize("batch_size", [1])
    # @pytest.mark.parametrize("split_size", [1, 2])
    def test_forward_0(batch_size, split_size):
        model = nn.Sequential(
            nn.Linear(3, 4).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
            nn.Linear(4, 5).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
        )
    
        x = torch.randn(batch_size, 3).to('cuda:0')
        y0 = model(x).to('cpu')
    
        # move the last two layer to another device
        model[-2] = model[-2].to('cuda:1')
        model[-1] = WithDevice(nn.Sigmoid(), 'cuda:1')
        pipe = Pipe(model, split_size=split_size)
        y1 = pipe(x).to('cpu')
>       assert torch.allclose(y0, y1)
E       assert False
E        +  where False = <built-in method allclose of type object at 0x153da78cb760>(tensor([[0.5131, 0.7104, 0.6665, 0.3776, 0.3566],\n        [0.5158, 0.7120, 0.6565, 0.3569, 0.3565],\n        [0.5079, 0.7525, 0.6833, 0.3878, 0.3480],\n        [0.5088, 0.6803, 0.6249, 0.3455, 0.3765],\n        [0.5110, 0.6960, 0.6403, 0.3527, 0.3674],\n        [0.5034, 0.6711, 0.6312, 0.3671, 0.3799],\n        [0.5129, 0.7327, 0.6688, 0.3700, 0.3527],\n        [0.5020, 0.6738, 0.6364, 0.3751, 0.3789],\n        [0.5068, 0.6644, 0.6155, 0.3466, 0.3844],\n        [0.5053, 0.6602, 0.6221, 0.3578, 0.3832],\n        [0.4911, 0.6715, 0.6402, 0.4039, 0.3875],\n        [0.5105, 0.6787, 0.6412, 0.3637, 0.3706],\n        [0.5081, 0.6638, 0.6222, 0.3515, 0.3808],\n        [0.5044, 0.6698, 0.6265, 0.3602, 0.3809],\n        [0.5062, 0.6889, 0.6606, 0.3931, 0.3667],\n        [0.5035, 0.7187, 0.6594, 0.3842, 0.3659],\n        [0.5010, 0.6774, 0.6468, 0.3896, 0.3763],\n        [0.5004, 0.6824, 0.6444, 0.3855, 0.3770],\n        [0.5059, 0.7137, 0.6513, 0.3709, 0.3670],\n        [0.5090, 0.6795, 0.6246, 0.3451, 0.3766],\n        [0.5018, 0.7031, 0.6599, 0.3954, 0.3690],\n        [0.5059, 0.6613, 0.6203, 0.3541, 0.3832],\n        [0.5166, 0.6939, 0.6319, 0.3313, 0.3650],\n        [0.5074, 0.7167, 0.6640, 0.3831, 0.3614],\n        [0.5144, 0.7274, 0.6620, 0.3592, 0.3535],\n        [0.5027, 0.6908, 0.6517, 0.3873, 0.3721],\n        [0.5007, 0.6624, 0.6368, 0.3830, 0.3812],\n        [0.5075, 0.7010, 0.6377, 0.3557, 0.3715],\n        [0.5099, 0.7091, 0.6528, 0.3656, 0.3632],\n        [0.5004, 0.6674, 0.6332, 0.3767, 0.3819],\n        [0.5095, 0.7232, 0.6631, 0.3740, 0.3587],\n        [0.5117, 0.6995, 0.6318, 0.3375, 0.3679]], grad_fn=<ToCopyBackward0>), tensor([[0.8852, 0.7727, 0.6931, 0.7869, 0.8090],\n        [0.9163, 0.7507, 0.6543, 0.4134, 0.4884],\n        [0.9533, 0.8055, 0.5853, 0.3726, 0.5781],\n        [0.9834, 0.9542, 0.9971, 0.9649, 0.4984],\n        [0.9105, 0.7847, 0.7502, 0.5701, 0.5651],\n        [0.8966, 0.7984, 0.6881, 0.5840, 0.6617],\n        [0.8676, 0.6731, 0.5300, 0.5574, 0.6675],\n        [0.8966, 0.6741, 0.7730, 0.6788, 0.5365],\n        [0.9005, 0.7893, 0.6507, 0.4382, 0.5579],\n        [0.8753, 0.7685, 0.6421, 0.6806, 0.7546],\n        [0.8762, 0.7299, 0.5693, 0.5334, 0.6605],\n        [0.8860, 0.7031, 0.7262, 0.7016, 0.6427],\n        [0.8916, 0.7889, 0.6414, 0.4360, 0.5648],\n        [0.8712, 0.7641, 0.6327, 0.6706, 0.7502],\n        [0.8896, 0.7582, 0.6193, 0.5476, 0.6544],\n        [0.8891, 0.7038, 0.7272, 0.6745, 0.6144],\n        [0.0000, 0.0000, 0.0000, 1.0000, 1.0000],\n        [0.8713, 0.7642, 0.6312, 0.6641, 0.7462],\n        [0.8904, 0.7563, 0.6227, 0.5566, 0.6572],\n        [0.8939, 0.7132, 0.7388, 0.6619, 0.5966],\n        [0.8984, 0.8506, 0.9898, 0.9994, 0.9821],\n        [0.8948, 0.8468, 0.9884, 0.9992, 0.9810],\n        [0.8933, 0.8394, 0.9998, 1.0000, 0.9789],\n        [1.0000, 1.0000, 1.0000, 0.9812, 0.0000],\n        [0.9220, 0.7980, 0.7202, 0.5453, 0.5926],\n        [0.9271, 0.8130, 0.7850, 0.5890, 0.5711],\n        [0.9215, 0.8264, 0.7316, 0.5300, 0.6001],\n        [0.9288, 0.8290, 0.7561, 0.5668, 0.6080],\n        [0.9000, 0.7926, 0.6579, 0.4422, 0.5581],\n        [0.8847, 0.7766, 0.6827, 0.7110, 0.7546],\n        [0.8871, 0.7136, 0.5961, 0.5320, 0.6210],\n        [0.9059, 0.7247, 0.7826, 0.6445, 0.5337]], grad_fn=<ToCopyBackward0>))
E        +    where <built-in method allclose of type object at 0x153da78cb760> = torch.allclose

batch_size = 32
model      = Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
  (2): Linear(in_features=4, out_features=5, bias=True)
  (3): WithDevice(
    (_module): Sigmoid()
  )
)
pipe       = Pipe(
  (partitions): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=3, out_features=4, bias=True)
   ...inear(in_features=4, out_features=5, bias=True)
      (1): WithDevice(
        (_module): Sigmoid()
      )
    )
  )
)
split_size = 4
x          = tensor([[ 1.2203, -1.2662, -0.3792],
        [ 1.3922, -0.8359,  0.6696],
        [ 1.1796,  2.0901, -1.0240],
       ...-0.9838, -0.5087, -0.4179],
        [ 0.6562,  0.7662, -0.0807],
        [ 0.4170,  0.5647,  1.7400]], device='cuda:0')
y0         = tensor([[0.5131, 0.7104, 0.6665, 0.3776, 0.3566],
        [0.5158, 0.7120, 0.6565, 0.3569, 0.3565],
        [0.5079, 0...[0.5095, 0.7232, 0.6631, 0.3740, 0.3587],
        [0.5117, 0.6995, 0.6318, 0.3375, 0.3679]], grad_fn=<ToCopyBackward0>)
y1         = tensor([[0.8852, 0.7727, 0.6931, 0.7869, 0.8090],
        [0.9163, 0.7507, 0.6543, 0.4134, 0.4884],
        [0.9533, 0...[0.8871, 0.7136, 0.5961, 0.5320, 0.6210],
        [0.9059, 0.7247, 0.7826, 0.6445, 0.5337]], grad_fn=<ToCopyBackward0>)

tests/test_pipeline.py:83: AssertionError
----------------------------- Captured stdout call -----------------------------
in init
ModuleList(
  (0): Sequential(
    (0): Linear(in_features=3, out_features=4, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
  (1): Sequential(
    (0): Linear(in_features=4, out_features=5, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
)
[device(type='cuda', index=0), device(type='cuda', index=1)]
before running the cycle
[tensor([[ 1.2203, -1.2662, -0.3792],
        [ 1.3922, -0.8359,  0.6696],
        [ 1.1796,  2.0901, -1.0240],
        [-0.4230,  0.3070,  1.2848]], device='cuda:0'), tensor([[ 0.4000, -0.1836,  0.8442],
        [-0.7482, -0.3969,  0.0835],
        [ 1.2598,  0.6115,  0.1684],
        [-0.7457, -0.4091, -0.3293]], device='cuda:0'), tensor([[-1.2403,  0.4829,  1.3563],
        [-0.8527, -0.8311,  0.6968],
        [-2.1280,  0.9749, -1.6517],
        [ 0.2927, -1.5780,  0.3103]], device='cuda:0'), tensor([[-0.5795, -0.8846,  1.0728],
        [-0.8203, -0.1893,  0.4660],
        [ 0.3330, -1.6714, -1.1780],
        [-0.3428,  1.8752, -0.5608]], device='cuda:0'), tensor([[-0.6003, -0.7824, -1.0311],
        [-0.7846, -0.0071, -0.7999],
        [-0.2631,  1.7376,  0.1175],
        [-0.4106,  0.2368,  1.3091]], device='cuda:0'), tensor([[-0.3607,  0.4549, -1.1764],
        [-0.8865, -0.5876,  0.8998],
        [ 1.0546, -0.8214,  2.1473],
        [ 0.3789,  0.4940, -0.5583]], device='cuda:0'), tensor([[ 1.4131,  0.2799,  0.6919],
        [-0.3713, -0.1812, -0.8628],
        [-0.8171, -1.3296, -0.7251],
        [-0.4401,  1.5685,  0.8596]], device='cuda:0'), tensor([[ 0.5011,  0.2184,  0.2455],
        [-0.9838, -0.5087, -0.4179],
        [ 0.6562,  0.7662, -0.0807],
        [ 0.4170,  0.5647,  1.7400]], device='cuda:0')]
====================
schedule for this step is
[(0, 0)]
inputting microbatch 0 into partition 0
before moving to cuda:0: tensor([[ 1.2203, -1.2662, -0.3792],
        [ 1.3922, -0.8359,  0.6696],
        [ 1.1796,  2.0901, -1.0240],
        [-0.4230,  0.3070,  1.2848]], device='cuda:0')
after: tensor([[ 1.2203, -1.2662, -0.3792],
        [ 1.3922, -0.8359,  0.6696],
        [ 1.1796,  2.0901, -1.0240],
        [-0.4230,  0.3070,  1.2848]], device='cuda:0')
********************
observing microbatch 0
current batch shape is torch.Size([4, 3])
batch is tensor([[ 1.2203, -1.2662, -0.3792],
        [ 1.3922, -0.8359,  0.6696],
        [ 1.1796,  2.0901, -1.0240],
        [-0.4230,  0.3070,  1.2848]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 0
********************
receiving microbatch 0 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 0, tensor([[0.4518, 0.5458, 0.6517, 0.5987],
        [0.3874, 0.5698, 0.5584, 0.7045],
        [0.8785, 0.6936, 0.4390, 0.7091],
        [0.2498, 0.4067, 0.2393, 0.6785]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.4518, 0.5458, 0.6517, 0.5987],
        [0.3874, 0.5698, 0.5584, 0.7045],
        [0.8785, 0.6936, 0.4390, 0.7091],
        [0.2498, 0.4067, 0.2393, 0.6785]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4518, 0.5458, 0.6517, 0.5987],
        [0.3874, 0.5698, 0.5584, 0.7045],
        [0.8785, 0.6936, 0.4390, 0.7091],
        [0.2498, 0.4067, 0.2393, 0.6785]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.4518, 0.5458, 0.6517, 0.5987],
        [0.3874, 0.5698, 0.5584, 0.7045],
        [0.8785, 0.6936, 0.4390, 0.7091],
        [0.2498, 0.4067, 0.2393, 0.6785]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.4000, -0.1836,  0.8442],
        [-0.7482, -0.3969,  0.0835],
        [ 1.2598,  0.6115,  0.1684],
        [-0.7457, -0.4091, -0.3293]], device='cuda:0'), tensor([[-1.2403,  0.4829,  1.3563],
        [-0.8527, -0.8311,  0.6968],
        [-2.1280,  0.9749, -1.6517],
        [ 0.2927, -1.5780,  0.3103]], device='cuda:0'), tensor([[-0.5795, -0.8846,  1.0728],
        [-0.8203, -0.1893,  0.4660],
        [ 0.3330, -1.6714, -1.1780],
        [-0.3428,  1.8752, -0.5608]], device='cuda:0'), tensor([[-0.6003, -0.7824, -1.0311],
        [-0.7846, -0.0071, -0.7999],
        [-0.2631,  1.7376,  0.1175],
        [-0.4106,  0.2368,  1.3091]], device='cuda:0'), tensor([[-0.3607,  0.4549, -1.1764],
        [-0.8865, -0.5876,  0.8998],
        [ 1.0546, -0.8214,  2.1473],
        [ 0.3789,  0.4940, -0.5583]], device='cuda:0'), tensor([[ 1.4131,  0.2799,  0.6919],
        [-0.3713, -0.1812, -0.8628],
        [-0.8171, -1.3296, -0.7251],
        [-0.4401,  1.5685,  0.8596]], device='cuda:0'), tensor([[ 0.5011,  0.2184,  0.2455],
        [-0.9838, -0.5087, -0.4179],
        [ 0.6562,  0.7662, -0.0807],
        [ 0.4170,  0.5647,  1.7400]], device='cuda:0')]
====================
schedule for this step is
[(1, 0), (0, 1)]
inputting microbatch 1 into partition 0
before moving to cuda:0: tensor([[ 0.4000, -0.1836,  0.8442],
        [-0.7482, -0.3969,  0.0835],
        [ 1.2598,  0.6115,  0.1684],
        [-0.7457, -0.4091, -0.3293]], device='cuda:0')
after: tensor([[ 0.4000, -0.1836,  0.8442],
        [-0.7482, -0.3969,  0.0835],
        [ 1.2598,  0.6115,  0.1684],
        [-0.7457, -0.4091, -0.3293]], device='cuda:0')
********************
observing microbatch 1
current batch shape is torch.Size([4, 3])
batch is tensor([[ 0.4000, -0.1836,  0.8442],
        [-0.7482, -0.3969,  0.0835],
        [ 1.2598,  0.6115,  0.1684],
        [-0.7457, -0.4091, -0.3293]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 1
********************
inputting microbatch 0 into partition 1
before moving to cuda:1: tensor([[0.4518, 0.5458, 0.6517, 0.5987],
        [0.3874, 0.5698, 0.5584, 0.7045],
        [0.8785, 0.6936, 0.4390, 0.7091],
        [0.2498, 0.4067, 0.2393, 0.6785]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[ 4.8107e-01,  9.8166e-01,  9.2348e-01,  9.5650e-01],
        [ 8.6695e-01,  5.7989e-01,  9.7996e-01,  9.2653e-01],
        [ 9.4463e-01,  8.6892e-01,  2.0000e+00,  1.7700e+00],
        [ 2.0000e+00,  1.8700e+00, -1.0842e-19,  1.8566e+00]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 0
current batch shape is torch.Size([4, 4])
batch is tensor([[ 4.8107e-01,  9.8166e-01,  9.2348e-01,  9.5650e-01],
        [ 8.6695e-01,  5.7989e-01,  9.7996e-01,  9.2653e-01],
        [ 9.4463e-01,  8.6892e-01,  2.0000e+00,  1.7700e+00],
        [ 2.0000e+00,  1.8700e+00, -1.0842e-19,  1.8566e+00]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 0
********************
receiving microbatch 1 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 1, tensor([[0.3311, 0.4838, 0.3816, 0.6808],
        [0.2736, 0.3575, 0.3318, 0.5316],
        [0.6330, 0.6258, 0.4693, 0.7263],
        [0.3206, 0.3627, 0.3628, 0.4991]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 1
result is tensor([[0.3311, 0.4838, 0.3816, 0.6808],
        [0.2736, 0.3575, 0.3318, 0.5316],
        [0.6330, 0.6258, 0.4693, 0.7263],
        [0.3206, 0.3627, 0.3628, 0.4991]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3311, 0.4838, 0.3816, 0.6808],
        [0.2736, 0.3575, 0.3318, 0.5316],
        [0.6330, 0.6258, 0.4693, 0.7263],
        [0.3206, 0.3627, 0.3628, 0.4991]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 0 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 0, tensor([[0.8852, 0.7727, 0.6931, 0.7869, 0.8090],
        [0.9163, 0.7507, 0.6543, 0.4134, 0.4884],
        [0.9533, 0.8055, 0.5853, 0.3726, 0.5781],
        [0.9834, 0.9542, 0.9971, 0.9649, 0.4984]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.8852, 0.7727, 0.6931, 0.7869, 0.8090],
        [0.9163, 0.7507, 0.6543, 0.4134, 0.4884],
        [0.9533, 0.8055, 0.5853, 0.3726, 0.5781],
        [0.9834, 0.9542, 0.9971, 0.9649, 0.4984]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.8852, 0.7727, 0.6931, 0.7869, 0.8090],
        [0.9163, 0.7507, 0.6543, 0.4134, 0.4884],
        [0.9533, 0.8055, 0.5853, 0.3726, 0.5781],
        [0.9834, 0.9542, 0.9971, 0.9649, 0.4984]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.8852, 0.7727, 0.6931, 0.7869, 0.8090],
        [0.9163, 0.7507, 0.6543, 0.4134, 0.4884],
        [0.9533, 0.8055, 0.5853, 0.3726, 0.5781],
        [0.9834, 0.9542, 0.9971, 0.9649, 0.4984]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3311, 0.4838, 0.3816, 0.6808],
        [0.2736, 0.3575, 0.3318, 0.5316],
        [0.6330, 0.6258, 0.4693, 0.7263],
        [0.3206, 0.3627, 0.3628, 0.4991]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-1.2403,  0.4829,  1.3563],
        [-0.8527, -0.8311,  0.6968],
        [-2.1280,  0.9749, -1.6517],
        [ 0.2927, -1.5780,  0.3103]], device='cuda:0'), tensor([[-0.5795, -0.8846,  1.0728],
        [-0.8203, -0.1893,  0.4660],
        [ 0.3330, -1.6714, -1.1780],
        [-0.3428,  1.8752, -0.5608]], device='cuda:0'), tensor([[-0.6003, -0.7824, -1.0311],
        [-0.7846, -0.0071, -0.7999],
        [-0.2631,  1.7376,  0.1175],
        [-0.4106,  0.2368,  1.3091]], device='cuda:0'), tensor([[-0.3607,  0.4549, -1.1764],
        [-0.8865, -0.5876,  0.8998],
        [ 1.0546, -0.8214,  2.1473],
        [ 0.3789,  0.4940, -0.5583]], device='cuda:0'), tensor([[ 1.4131,  0.2799,  0.6919],
        [-0.3713, -0.1812, -0.8628],
        [-0.8171, -1.3296, -0.7251],
        [-0.4401,  1.5685,  0.8596]], device='cuda:0'), tensor([[ 0.5011,  0.2184,  0.2455],
        [-0.9838, -0.5087, -0.4179],
        [ 0.6562,  0.7662, -0.0807],
        [ 0.4170,  0.5647,  1.7400]], device='cuda:0')]
====================
schedule for this step is
[(2, 0), (1, 1)]
inputting microbatch 2 into partition 0
before moving to cuda:0: tensor([[-1.2403,  0.4829,  1.3563],
        [-0.8527, -0.8311,  0.6968],
        [-2.1280,  0.9749, -1.6517],
        [ 0.2927, -1.5780,  0.3103]], device='cuda:0')
after: tensor([[-1.2403,  0.4829,  1.3563],
        [-0.8527, -0.8311,  0.6968],
        [-2.1280,  0.9749, -1.6517],
        [ 0.2927, -1.5780,  0.3103]], device='cuda:0')
********************
observing microbatch 2
current batch shape is torch.Size([4, 3])
batch is tensor([[-1.2403,  0.4829,  1.3563],
        [-0.8527, -0.8311,  0.6968],
        [-2.1280,  0.9749, -1.6517],
        [ 0.2927, -1.5780,  0.3103]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 2
********************
inputting microbatch 1 into partition 1
before moving to cuda:1: tensor([[0.3311, 0.4838, 0.3816, 0.6808],
        [0.2736, 0.3575, 0.3318, 0.5316],
        [0.6330, 0.6258, 0.4693, 0.7263],
        [0.3206, 0.3627, 0.3628, 0.4991]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.8852, 0.7727, 0.6931, 0.7869],
        [0.8090, 0.9163, 0.7507, 0.6543],
        [0.4134, 0.4884, 0.9533, 0.8055],
        [0.5853, 0.3726, 0.5781, 0.9834]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 1
current batch shape is torch.Size([4, 4])
batch is tensor([[0.8852, 0.7727, 0.6931, 0.7869],
        [0.8090, 0.9163, 0.7507, 0.6543],
        [0.4134, 0.4884, 0.9533, 0.8055],
        [0.5853, 0.3726, 0.5781, 0.9834]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 1
********************
receiving microbatch 2 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 2, tensor([[0.1888, 0.3264, 0.1665, 0.6357],
        [0.1666, 0.3210, 0.3085, 0.5484],
        [0.5108, 0.2957, 0.2292, 0.3668],
        [0.2280, 0.4162, 0.5196, 0.5662]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 2
result is tensor([[0.1888, 0.3264, 0.1665, 0.6357],
        [0.1666, 0.3210, 0.3085, 0.5484],
        [0.5108, 0.2957, 0.2292, 0.3668],
        [0.2280, 0.4162, 0.5196, 0.5662]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.1888, 0.3264, 0.1665, 0.6357],
        [0.1666, 0.3210, 0.3085, 0.5484],
        [0.5108, 0.2957, 0.2292, 0.3668],
        [0.2280, 0.4162, 0.5196, 0.5662]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 1 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 1, tensor([[0.9105, 0.7847, 0.7502, 0.5701, 0.5651],
        [0.8966, 0.7984, 0.6881, 0.5840, 0.6617],
        [0.8676, 0.6731, 0.5300, 0.5574, 0.6675],
        [0.8966, 0.6741, 0.7730, 0.6788, 0.5365]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 1
result is tensor([[0.9105, 0.7847, 0.7502, 0.5701, 0.5651],
        [0.8966, 0.7984, 0.6881, 0.5840, 0.6617],
        [0.8676, 0.6731, 0.5300, 0.5574, 0.6675],
        [0.8966, 0.6741, 0.7730, 0.6788, 0.5365]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9105, 0.7847, 0.7502, 0.5701, 0.5651],
        [0.8966, 0.7984, 0.6881, 0.5840, 0.6617],
        [0.8676, 0.6731, 0.5300, 0.5574, 0.6675],
        [0.8966, 0.6741, 0.7730, 0.6788, 0.5365]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.8852, 0.7727, 0.6931, 0.7869, 0.8090],
        [0.9163, 0.7507, 0.6543, 0.4134, 0.4884],
        [0.9533, 0.8055, 0.5853, 0.3726, 0.5781],
        [0.9834, 0.9542, 0.9971, 0.9649, 0.4984]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9105, 0.7847, 0.7502, 0.5701, 0.5651],
        [0.8966, 0.7984, 0.6881, 0.5840, 0.6617],
        [0.8676, 0.6731, 0.5300, 0.5574, 0.6675],
        [0.8966, 0.6741, 0.7730, 0.6788, 0.5365]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.1888, 0.3264, 0.1665, 0.6357],
        [0.1666, 0.3210, 0.3085, 0.5484],
        [0.5108, 0.2957, 0.2292, 0.3668],
        [0.2280, 0.4162, 0.5196, 0.5662]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.5795, -0.8846,  1.0728],
        [-0.8203, -0.1893,  0.4660],
        [ 0.3330, -1.6714, -1.1780],
        [-0.3428,  1.8752, -0.5608]], device='cuda:0'), tensor([[-0.6003, -0.7824, -1.0311],
        [-0.7846, -0.0071, -0.7999],
        [-0.2631,  1.7376,  0.1175],
        [-0.4106,  0.2368,  1.3091]], device='cuda:0'), tensor([[-0.3607,  0.4549, -1.1764],
        [-0.8865, -0.5876,  0.8998],
        [ 1.0546, -0.8214,  2.1473],
        [ 0.3789,  0.4940, -0.5583]], device='cuda:0'), tensor([[ 1.4131,  0.2799,  0.6919],
        [-0.3713, -0.1812, -0.8628],
        [-0.8171, -1.3296, -0.7251],
        [-0.4401,  1.5685,  0.8596]], device='cuda:0'), tensor([[ 0.5011,  0.2184,  0.2455],
        [-0.9838, -0.5087, -0.4179],
        [ 0.6562,  0.7662, -0.0807],
        [ 0.4170,  0.5647,  1.7400]], device='cuda:0')]
====================
schedule for this step is
[(3, 0), (2, 1)]
inputting microbatch 3 into partition 0
before moving to cuda:0: tensor([[-0.5795, -0.8846,  1.0728],
        [-0.8203, -0.1893,  0.4660],
        [ 0.3330, -1.6714, -1.1780],
        [-0.3428,  1.8752, -0.5608]], device='cuda:0')
after: tensor([[-0.5795, -0.8846,  1.0728],
        [-0.8203, -0.1893,  0.4660],
        [ 0.3330, -1.6714, -1.1780],
        [-0.3428,  1.8752, -0.5608]], device='cuda:0')
********************
observing microbatch 3
current batch shape is torch.Size([4, 3])
batch is tensor([[-0.5795, -0.8846,  1.0728],
        [-0.8203, -0.1893,  0.4660],
        [ 0.3330, -1.6714, -1.1780],
        [-0.3428,  1.8752, -0.5608]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 3
********************
inputting microbatch 2 into partition 1
before moving to cuda:1: tensor([[0.1888, 0.3264, 0.1665, 0.6357],
        [0.1666, 0.3210, 0.3085, 0.5484],
        [0.5108, 0.2957, 0.2292, 0.3668],
        [0.2280, 0.4162, 0.5196, 0.5662]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9105, 0.7847, 0.7502, 0.5701],
        [0.5651, 0.8966, 0.7984, 0.6881],
        [0.5840, 0.6617, 0.8676, 0.6731],
        [0.5300, 0.5574, 0.6675, 0.8966]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 2
current batch shape is torch.Size([4, 4])
batch is tensor([[0.9105, 0.7847, 0.7502, 0.5701],
        [0.5651, 0.8966, 0.7984, 0.6881],
        [0.5840, 0.6617, 0.8676, 0.6731],
        [0.5300, 0.5574, 0.6675, 0.8966]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 2
********************
receiving microbatch 3 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 3, tensor([[0.1531, 0.3418, 0.3133, 0.5946],
        [0.2472, 0.3536, 0.2847, 0.5663],
        [0.3971, 0.4376, 0.6460, 0.4492],
        [0.6963, 0.5132, 0.2641, 0.6286]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 3
result is tensor([[0.1531, 0.3418, 0.3133, 0.5946],
        [0.2472, 0.3536, 0.2847, 0.5663],
        [0.3971, 0.4376, 0.6460, 0.4492],
        [0.6963, 0.5132, 0.2641, 0.6286]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.1531, 0.3418, 0.3133, 0.5946],
        [0.2472, 0.3536, 0.2847, 0.5663],
        [0.3971, 0.4376, 0.6460, 0.4492],
        [0.6963, 0.5132, 0.2641, 0.6286]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 2 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 2, tensor([[0.9005, 0.7893, 0.6507, 0.4382, 0.5579],
        [0.8753, 0.7685, 0.6421, 0.6806, 0.7546],
        [0.8762, 0.7299, 0.5693, 0.5334, 0.6605],
        [0.8860, 0.7031, 0.7262, 0.7016, 0.6427]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 2
result is tensor([[0.9005, 0.7893, 0.6507, 0.4382, 0.5579],
        [0.8753, 0.7685, 0.6421, 0.6806, 0.7546],
        [0.8762, 0.7299, 0.5693, 0.5334, 0.6605],
        [0.8860, 0.7031, 0.7262, 0.7016, 0.6427]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9005, 0.7893, 0.6507, 0.4382, 0.5579],
        [0.8753, 0.7685, 0.6421, 0.6806, 0.7546],
        [0.8762, 0.7299, 0.5693, 0.5334, 0.6605],
        [0.8860, 0.7031, 0.7262, 0.7016, 0.6427]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.8852, 0.7727, 0.6931, 0.7869, 0.8090],
        [0.9163, 0.7507, 0.6543, 0.4134, 0.4884],
        [0.9533, 0.8055, 0.5853, 0.3726, 0.5781],
        [0.9834, 0.9542, 0.9971, 0.9649, 0.4984]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9105, 0.7847, 0.7502, 0.5701, 0.5651],
        [0.8966, 0.7984, 0.6881, 0.5840, 0.6617],
        [0.8676, 0.6731, 0.5300, 0.5574, 0.6675],
        [0.8966, 0.6741, 0.7730, 0.6788, 0.5365]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9005, 0.7893, 0.6507, 0.4382, 0.5579],
        [0.8753, 0.7685, 0.6421, 0.6806, 0.7546],
        [0.8762, 0.7299, 0.5693, 0.5334, 0.6605],
        [0.8860, 0.7031, 0.7262, 0.7016, 0.6427]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.1531, 0.3418, 0.3133, 0.5946],
        [0.2472, 0.3536, 0.2847, 0.5663],
        [0.3971, 0.4376, 0.6460, 0.4492],
        [0.6963, 0.5132, 0.2641, 0.6286]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.6003, -0.7824, -1.0311],
        [-0.7846, -0.0071, -0.7999],
        [-0.2631,  1.7376,  0.1175],
        [-0.4106,  0.2368,  1.3091]], device='cuda:0'), tensor([[-0.3607,  0.4549, -1.1764],
        [-0.8865, -0.5876,  0.8998],
        [ 1.0546, -0.8214,  2.1473],
        [ 0.3789,  0.4940, -0.5583]], device='cuda:0'), tensor([[ 1.4131,  0.2799,  0.6919],
        [-0.3713, -0.1812, -0.8628],
        [-0.8171, -1.3296, -0.7251],
        [-0.4401,  1.5685,  0.8596]], device='cuda:0'), tensor([[ 0.5011,  0.2184,  0.2455],
        [-0.9838, -0.5087, -0.4179],
        [ 0.6562,  0.7662, -0.0807],
        [ 0.4170,  0.5647,  1.7400]], device='cuda:0')]
====================
schedule for this step is
[(4, 0), (3, 1)]
inputting microbatch 4 into partition 0
before moving to cuda:0: tensor([[-0.6003, -0.7824, -1.0311],
        [-0.7846, -0.0071, -0.7999],
        [-0.2631,  1.7376,  0.1175],
        [-0.4106,  0.2368,  1.3091]], device='cuda:0')
after: tensor([[-0.6003, -0.7824, -1.0311],
        [-0.7846, -0.0071, -0.7999],
        [-0.2631,  1.7376,  0.1175],
        [-0.4106,  0.2368,  1.3091]], device='cuda:0')
********************
observing microbatch 4
current batch shape is torch.Size([4, 3])
batch is tensor([[-0.6003, -0.7824, -1.0311],
        [-0.7846, -0.0071, -0.7999],
        [-0.2631,  1.7376,  0.1175],
        [-0.4106,  0.2368,  1.3091]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 4
********************
inputting microbatch 3 into partition 1
before moving to cuda:1: tensor([[0.1531, 0.3418, 0.3133, 0.5946],
        [0.2472, 0.3536, 0.2847, 0.5663],
        [0.3971, 0.4376, 0.6460, 0.4492],
        [0.6963, 0.5132, 0.2641, 0.6286]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9005, 0.7893, 0.6507, 0.4382],
        [0.5579, 0.8753, 0.7685, 0.6421],
        [0.6806, 0.7546, 0.8762, 0.7299],
        [0.5693, 0.5334, 0.6605, 0.8860]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 3
current batch shape is torch.Size([4, 4])
batch is tensor([[0.9005, 0.7893, 0.6507, 0.4382],
        [0.5579, 0.8753, 0.7685, 0.6421],
        [0.6806, 0.7546, 0.8762, 0.7299],
        [0.5693, 0.5334, 0.6605, 0.8860]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 3
********************
receiving microbatch 4 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 4, tensor([[0.3804, 0.3717, 0.4615, 0.4364],
        [0.4285, 0.3821, 0.3641, 0.4806],
        [0.6024, 0.5062, 0.2384, 0.6753],
        [0.2414, 0.4046, 0.2429, 0.6777]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 4
result is tensor([[0.3804, 0.3717, 0.4615, 0.4364],
        [0.4285, 0.3821, 0.3641, 0.4806],
        [0.6024, 0.5062, 0.2384, 0.6753],
        [0.2414, 0.4046, 0.2429, 0.6777]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3804, 0.3717, 0.4615, 0.4364],
        [0.4285, 0.3821, 0.3641, 0.4806],
        [0.6024, 0.5062, 0.2384, 0.6753],
        [0.2414, 0.4046, 0.2429, 0.6777]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 3 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 3, tensor([[0.8916, 0.7889, 0.6414, 0.4360, 0.5648],
        [0.8712, 0.7641, 0.6327, 0.6706, 0.7502],
        [0.8896, 0.7582, 0.6193, 0.5476, 0.6544],
        [0.8891, 0.7038, 0.7272, 0.6745, 0.6144]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 3
result is tensor([[0.8916, 0.7889, 0.6414, 0.4360, 0.5648],
        [0.8712, 0.7641, 0.6327, 0.6706, 0.7502],
        [0.8896, 0.7582, 0.6193, 0.5476, 0.6544],
        [0.8891, 0.7038, 0.7272, 0.6745, 0.6144]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.8916, 0.7889, 0.6414, 0.4360, 0.5648],
        [0.8712, 0.7641, 0.6327, 0.6706, 0.7502],
        [0.8896, 0.7582, 0.6193, 0.5476, 0.6544],
        [0.8891, 0.7038, 0.7272, 0.6745, 0.6144]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.8852, 0.7727, 0.6931, 0.7869, 0.8090],
        [0.9163, 0.7507, 0.6543, 0.4134, 0.4884],
        [0.9533, 0.8055, 0.5853, 0.3726, 0.5781],
        [0.9834, 0.9542, 0.9971, 0.9649, 0.4984]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9105, 0.7847, 0.7502, 0.5701, 0.5651],
        [0.8966, 0.7984, 0.6881, 0.5840, 0.6617],
        [0.8676, 0.6731, 0.5300, 0.5574, 0.6675],
        [0.8966, 0.6741, 0.7730, 0.6788, 0.5365]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9005, 0.7893, 0.6507, 0.4382, 0.5579],
        [0.8753, 0.7685, 0.6421, 0.6806, 0.7546],
        [0.8762, 0.7299, 0.5693, 0.5334, 0.6605],
        [0.8860, 0.7031, 0.7262, 0.7016, 0.6427]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8916, 0.7889, 0.6414, 0.4360, 0.5648],
        [0.8712, 0.7641, 0.6327, 0.6706, 0.7502],
        [0.8896, 0.7582, 0.6193, 0.5476, 0.6544],
        [0.8891, 0.7038, 0.7272, 0.6745, 0.6144]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3804, 0.3717, 0.4615, 0.4364],
        [0.4285, 0.3821, 0.3641, 0.4806],
        [0.6024, 0.5062, 0.2384, 0.6753],
        [0.2414, 0.4046, 0.2429, 0.6777]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.3607,  0.4549, -1.1764],
        [-0.8865, -0.5876,  0.8998],
        [ 1.0546, -0.8214,  2.1473],
        [ 0.3789,  0.4940, -0.5583]], device='cuda:0'), tensor([[ 1.4131,  0.2799,  0.6919],
        [-0.3713, -0.1812, -0.8628],
        [-0.8171, -1.3296, -0.7251],
        [-0.4401,  1.5685,  0.8596]], device='cuda:0'), tensor([[ 0.5011,  0.2184,  0.2455],
        [-0.9838, -0.5087, -0.4179],
        [ 0.6562,  0.7662, -0.0807],
        [ 0.4170,  0.5647,  1.7400]], device='cuda:0')]
====================
schedule for this step is
[(5, 0), (4, 1)]
inputting microbatch 5 into partition 0
before moving to cuda:0: tensor([[-0.3607,  0.4549, -1.1764],
        [-0.8865, -0.5876,  0.8998],
        [ 1.0546, -0.8214,  2.1473],
        [ 0.3789,  0.4940, -0.5583]], device='cuda:0')
after: tensor([[-0.3607,  0.4549, -1.1764],
        [-0.8865, -0.5876,  0.8998],
        [ 1.0546, -0.8214,  2.1473],
        [ 0.3789,  0.4940, -0.5583]], device='cuda:0')
********************
observing microbatch 5
current batch shape is torch.Size([4, 3])
batch is tensor([[-0.3607,  0.4549, -1.1764],
        [-0.8865, -0.5876,  0.8998],
        [ 1.0546, -0.8214,  2.1473],
        [ 0.3789,  0.4940, -0.5583]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 5
********************
inputting microbatch 4 into partition 1
before moving to cuda:1: tensor([[0.3804, 0.3717, 0.4615, 0.4364],
        [0.4285, 0.3821, 0.3641, 0.4806],
        [0.6024, 0.5062, 0.2384, 0.6753],
        [0.2414, 0.4046, 0.2429, 0.6777]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[-3.6893e+19,  1.7180e+00,  6.4144e-01,  4.3604e-01],
        [ 5.6483e-01,  8.7120e-01,  7.6407e-01,  6.3265e-01],
        [ 6.7056e-01,  7.5021e-01,  8.8957e-01,  7.5824e-01],
        [ 6.1927e-01,  5.4760e-01,  6.5443e-01,  8.8907e-01]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 4
current batch shape is torch.Size([4, 4])
batch is tensor([[-3.6893e+19,  1.7180e+00,  6.4144e-01,  4.3604e-01],
        [ 5.6483e-01,  8.7120e-01,  7.6407e-01,  6.3265e-01],
        [ 6.7056e-01,  7.5021e-01,  8.8957e-01,  7.5824e-01],
        [ 6.1927e-01,  5.4760e-01,  6.5443e-01,  8.8907e-01]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 4
********************
receiving microbatch 5 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 5, tensor([[0.5963, 0.4551, 0.4041, 0.5078],
        [0.1670, 0.3248, 0.2760, 0.5740],
        [0.1903, 0.5109, 0.4020, 0.7735],
        [0.6084, 0.5326, 0.4358, 0.6123]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 5
result is tensor([[0.5963, 0.4551, 0.4041, 0.5078],
        [0.1670, 0.3248, 0.2760, 0.5740],
        [0.1903, 0.5109, 0.4020, 0.7735],
        [0.6084, 0.5326, 0.4358, 0.6123]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5963, 0.4551, 0.4041, 0.5078],
        [0.1670, 0.3248, 0.2760, 0.5740],
        [0.1903, 0.5109, 0.4020, 0.7735],
        [0.6084, 0.5326, 0.4358, 0.6123]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 4 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 4, tensor([[0.0000, 0.0000, 0.0000, 1.0000, 1.0000],
        [0.8713, 0.7642, 0.6312, 0.6641, 0.7462],
        [0.8904, 0.7563, 0.6227, 0.5566, 0.6572],
        [0.8939, 0.7132, 0.7388, 0.6619, 0.5966]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 4
result is tensor([[0.0000, 0.0000, 0.0000, 1.0000, 1.0000],
        [0.8713, 0.7642, 0.6312, 0.6641, 0.7462],
        [0.8904, 0.7563, 0.6227, 0.5566, 0.6572],
        [0.8939, 0.7132, 0.7388, 0.6619, 0.5966]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.0000, 0.0000, 1.0000, 1.0000],
        [0.8713, 0.7642, 0.6312, 0.6641, 0.7462],
        [0.8904, 0.7563, 0.6227, 0.5566, 0.6572],
        [0.8939, 0.7132, 0.7388, 0.6619, 0.5966]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.8852, 0.7727, 0.6931, 0.7869, 0.8090],
        [0.9163, 0.7507, 0.6543, 0.4134, 0.4884],
        [0.9533, 0.8055, 0.5853, 0.3726, 0.5781],
        [0.9834, 0.9542, 0.9971, 0.9649, 0.4984]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9105, 0.7847, 0.7502, 0.5701, 0.5651],
        [0.8966, 0.7984, 0.6881, 0.5840, 0.6617],
        [0.8676, 0.6731, 0.5300, 0.5574, 0.6675],
        [0.8966, 0.6741, 0.7730, 0.6788, 0.5365]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9005, 0.7893, 0.6507, 0.4382, 0.5579],
        [0.8753, 0.7685, 0.6421, 0.6806, 0.7546],
        [0.8762, 0.7299, 0.5693, 0.5334, 0.6605],
        [0.8860, 0.7031, 0.7262, 0.7016, 0.6427]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8916, 0.7889, 0.6414, 0.4360, 0.5648],
        [0.8712, 0.7641, 0.6327, 0.6706, 0.7502],
        [0.8896, 0.7582, 0.6193, 0.5476, 0.6544],
        [0.8891, 0.7038, 0.7272, 0.6745, 0.6144]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.0000, 0.0000, 1.0000, 1.0000],
        [0.8713, 0.7642, 0.6312, 0.6641, 0.7462],
        [0.8904, 0.7563, 0.6227, 0.5566, 0.6572],
        [0.8939, 0.7132, 0.7388, 0.6619, 0.5966]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5963, 0.4551, 0.4041, 0.5078],
        [0.1670, 0.3248, 0.2760, 0.5740],
        [0.1903, 0.5109, 0.4020, 0.7735],
        [0.6084, 0.5326, 0.4358, 0.6123]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 1.4131,  0.2799,  0.6919],
        [-0.3713, -0.1812, -0.8628],
        [-0.8171, -1.3296, -0.7251],
        [-0.4401,  1.5685,  0.8596]], device='cuda:0'), tensor([[ 0.5011,  0.2184,  0.2455],
        [-0.9838, -0.5087, -0.4179],
        [ 0.6562,  0.7662, -0.0807],
        [ 0.4170,  0.5647,  1.7400]], device='cuda:0')]
====================
schedule for this step is
[(6, 0), (5, 1)]
inputting microbatch 6 into partition 0
before moving to cuda:0: tensor([[ 1.4131,  0.2799,  0.6919],
        [-0.3713, -0.1812, -0.8628],
        [-0.8171, -1.3296, -0.7251],
        [-0.4401,  1.5685,  0.8596]], device='cuda:0')
after: tensor([[ 1.4131,  0.2799,  0.6919],
        [-0.3713, -0.1812, -0.8628],
        [-0.8171, -1.3296, -0.7251],
        [-0.4401,  1.5685,  0.8596]], device='cuda:0')
********************
observing microbatch 6
current batch shape is torch.Size([4, 3])
batch is tensor([[ 1.4131,  0.2799,  0.6919],
        [-0.3713, -0.1812, -0.8628],
        [-0.8171, -1.3296, -0.7251],
        [-0.4401,  1.5685,  0.8596]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 6
********************
inputting microbatch 5 into partition 1
before moving to cuda:1: tensor([[0.5963, 0.4551, 0.4041, 0.5078],
        [0.1670, 0.3248, 0.2760, 0.5740],
        [0.1903, 0.5109, 0.4020, 0.7735],
        [0.6084, 0.5326, 0.4358, 0.6123]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[ 0.0000e+00,  1.8750e+00,  0.0000e+00,  1.8750e+00],
        [-1.0842e-19,  1.8428e+00, -0.0000e+00,  1.8160e+00],
        [-0.0000e+00,  1.7828e+00, -2.0000e+00,  1.7910e+00],
        [ 3.6893e+19,  1.8116e+00, -3.6893e+19,  1.8476e+00]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 5
current batch shape is torch.Size([4, 4])
batch is tensor([[ 0.0000e+00,  1.8750e+00,  0.0000e+00,  1.8750e+00],
        [-1.0842e-19,  1.8428e+00, -0.0000e+00,  1.8160e+00],
        [-0.0000e+00,  1.7828e+00, -2.0000e+00,  1.7910e+00],
        [ 3.6893e+19,  1.8116e+00, -3.6893e+19,  1.8476e+00]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 5
********************
receiving microbatch 6 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 6, tensor([[0.5371, 0.6209, 0.4713, 0.7533],
        [0.4655, 0.4208, 0.4273, 0.4981],
        [0.2558, 0.3226, 0.4553, 0.4154],
        [0.4546, 0.4675, 0.1936, 0.7055]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 6
result is tensor([[0.5371, 0.6209, 0.4713, 0.7533],
        [0.4655, 0.4208, 0.4273, 0.4981],
        [0.2558, 0.3226, 0.4553, 0.4154],
        [0.4546, 0.4675, 0.1936, 0.7055]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5371, 0.6209, 0.4713, 0.7533],
        [0.4655, 0.4208, 0.4273, 0.4981],
        [0.2558, 0.3226, 0.4553, 0.4154],
        [0.4546, 0.4675, 0.1936, 0.7055]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 5 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 5, tensor([[0.8984, 0.8506, 0.9898, 0.9994, 0.9821],
        [0.8948, 0.8468, 0.9884, 0.9992, 0.9810],
        [0.8933, 0.8394, 0.9998, 1.0000, 0.9789],
        [1.0000, 1.0000, 1.0000, 0.9812, 0.0000]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 5
result is tensor([[0.8984, 0.8506, 0.9898, 0.9994, 0.9821],
        [0.8948, 0.8468, 0.9884, 0.9992, 0.9810],
        [0.8933, 0.8394, 0.9998, 1.0000, 0.9789],
        [1.0000, 1.0000, 1.0000, 0.9812, 0.0000]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.8984, 0.8506, 0.9898, 0.9994, 0.9821],
        [0.8948, 0.8468, 0.9884, 0.9992, 0.9810],
        [0.8933, 0.8394, 0.9998, 1.0000, 0.9789],
        [1.0000, 1.0000, 1.0000, 0.9812, 0.0000]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.8852, 0.7727, 0.6931, 0.7869, 0.8090],
        [0.9163, 0.7507, 0.6543, 0.4134, 0.4884],
        [0.9533, 0.8055, 0.5853, 0.3726, 0.5781],
        [0.9834, 0.9542, 0.9971, 0.9649, 0.4984]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9105, 0.7847, 0.7502, 0.5701, 0.5651],
        [0.8966, 0.7984, 0.6881, 0.5840, 0.6617],
        [0.8676, 0.6731, 0.5300, 0.5574, 0.6675],
        [0.8966, 0.6741, 0.7730, 0.6788, 0.5365]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9005, 0.7893, 0.6507, 0.4382, 0.5579],
        [0.8753, 0.7685, 0.6421, 0.6806, 0.7546],
        [0.8762, 0.7299, 0.5693, 0.5334, 0.6605],
        [0.8860, 0.7031, 0.7262, 0.7016, 0.6427]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8916, 0.7889, 0.6414, 0.4360, 0.5648],
        [0.8712, 0.7641, 0.6327, 0.6706, 0.7502],
        [0.8896, 0.7582, 0.6193, 0.5476, 0.6544],
        [0.8891, 0.7038, 0.7272, 0.6745, 0.6144]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.0000, 0.0000, 1.0000, 1.0000],
        [0.8713, 0.7642, 0.6312, 0.6641, 0.7462],
        [0.8904, 0.7563, 0.6227, 0.5566, 0.6572],
        [0.8939, 0.7132, 0.7388, 0.6619, 0.5966]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8984, 0.8506, 0.9898, 0.9994, 0.9821],
        [0.8948, 0.8468, 0.9884, 0.9992, 0.9810],
        [0.8933, 0.8394, 0.9998, 1.0000, 0.9789],
        [1.0000, 1.0000, 1.0000, 0.9812, 0.0000]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5371, 0.6209, 0.4713, 0.7533],
        [0.4655, 0.4208, 0.4273, 0.4981],
        [0.2558, 0.3226, 0.4553, 0.4154],
        [0.4546, 0.4675, 0.1936, 0.7055]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.5011,  0.2184,  0.2455],
        [-0.9838, -0.5087, -0.4179],
        [ 0.6562,  0.7662, -0.0807],
        [ 0.4170,  0.5647,  1.7400]], device='cuda:0')]
====================
schedule for this step is
[(7, 0), (6, 1)]
inputting microbatch 7 into partition 0
before moving to cuda:0: tensor([[ 0.5011,  0.2184,  0.2455],
        [-0.9838, -0.5087, -0.4179],
        [ 0.6562,  0.7662, -0.0807],
        [ 0.4170,  0.5647,  1.7400]], device='cuda:0')
after: tensor([[ 0.5011,  0.2184,  0.2455],
        [-0.9838, -0.5087, -0.4179],
        [ 0.6562,  0.7662, -0.0807],
        [ 0.4170,  0.5647,  1.7400]], device='cuda:0')
********************
observing microbatch 7
current batch shape is torch.Size([4, 3])
batch is tensor([[ 0.5011,  0.2184,  0.2455],
        [-0.9838, -0.5087, -0.4179],
        [ 0.6562,  0.7662, -0.0807],
        [ 0.4170,  0.5647,  1.7400]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 7
********************
inputting microbatch 6 into partition 1
before moving to cuda:1: tensor([[0.5371, 0.6209, 0.4713, 0.7533],
        [0.4655, 0.4208, 0.4273, 0.4981],
        [0.2558, 0.3226, 0.4553, 0.4154],
        [0.4546, 0.4675, 0.1936, 0.7055]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.8984, 0.8506, 0.9898, 0.9994],
        [0.9821, 0.8948, 0.8468, 0.9884],
        [0.9992, 0.9810, 0.8933, 0.8394],
        [0.9998, 1.0000, 0.9789, 1.0000]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 6
current batch shape is torch.Size([4, 4])
batch is tensor([[0.8984, 0.8506, 0.9898, 0.9994],
        [0.9821, 0.8948, 0.8468, 0.9884],
        [0.9992, 0.9810, 0.8933, 0.8394],
        [0.9998, 1.0000, 0.9789, 1.0000]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 6
********************
receiving microbatch 7 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 7, tensor([[0.4756, 0.5224, 0.4080, 0.6652],
        [0.2939, 0.3350, 0.3514, 0.4689],
        [0.6136, 0.5696, 0.4087, 0.6786],
        [0.3127, 0.5071, 0.2694, 0.7682]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 7
result is tensor([[0.4756, 0.5224, 0.4080, 0.6652],
        [0.2939, 0.3350, 0.3514, 0.4689],
        [0.6136, 0.5696, 0.4087, 0.6786],
        [0.3127, 0.5071, 0.2694, 0.7682]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4756, 0.5224, 0.4080, 0.6652],
        [0.2939, 0.3350, 0.3514, 0.4689],
        [0.6136, 0.5696, 0.4087, 0.6786],
        [0.3127, 0.5071, 0.2694, 0.7682]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 6 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 6, tensor([[0.9220, 0.7980, 0.7202, 0.5453, 0.5926],
        [0.9271, 0.8130, 0.7850, 0.5890, 0.5711],
        [0.9215, 0.8264, 0.7316, 0.5300, 0.6001],
        [0.9288, 0.8290, 0.7561, 0.5668, 0.6080]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 6
result is tensor([[0.9220, 0.7980, 0.7202, 0.5453, 0.5926],
        [0.9271, 0.8130, 0.7850, 0.5890, 0.5711],
        [0.9215, 0.8264, 0.7316, 0.5300, 0.6001],
        [0.9288, 0.8290, 0.7561, 0.5668, 0.6080]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9220, 0.7980, 0.7202, 0.5453, 0.5926],
        [0.9271, 0.8130, 0.7850, 0.5890, 0.5711],
        [0.9215, 0.8264, 0.7316, 0.5300, 0.6001],
        [0.9288, 0.8290, 0.7561, 0.5668, 0.6080]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.8852, 0.7727, 0.6931, 0.7869, 0.8090],
        [0.9163, 0.7507, 0.6543, 0.4134, 0.4884],
        [0.9533, 0.8055, 0.5853, 0.3726, 0.5781],
        [0.9834, 0.9542, 0.9971, 0.9649, 0.4984]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9105, 0.7847, 0.7502, 0.5701, 0.5651],
        [0.8966, 0.7984, 0.6881, 0.5840, 0.6617],
        [0.8676, 0.6731, 0.5300, 0.5574, 0.6675],
        [0.8966, 0.6741, 0.7730, 0.6788, 0.5365]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9005, 0.7893, 0.6507, 0.4382, 0.5579],
        [0.8753, 0.7685, 0.6421, 0.6806, 0.7546],
        [0.8762, 0.7299, 0.5693, 0.5334, 0.6605],
        [0.8860, 0.7031, 0.7262, 0.7016, 0.6427]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8916, 0.7889, 0.6414, 0.4360, 0.5648],
        [0.8712, 0.7641, 0.6327, 0.6706, 0.7502],
        [0.8896, 0.7582, 0.6193, 0.5476, 0.6544],
        [0.8891, 0.7038, 0.7272, 0.6745, 0.6144]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.0000, 0.0000, 1.0000, 1.0000],
        [0.8713, 0.7642, 0.6312, 0.6641, 0.7462],
        [0.8904, 0.7563, 0.6227, 0.5566, 0.6572],
        [0.8939, 0.7132, 0.7388, 0.6619, 0.5966]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8984, 0.8506, 0.9898, 0.9994, 0.9821],
        [0.8948, 0.8468, 0.9884, 0.9992, 0.9810],
        [0.8933, 0.8394, 0.9998, 1.0000, 0.9789],
        [1.0000, 1.0000, 1.0000, 0.9812, 0.0000]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9220, 0.7980, 0.7202, 0.5453, 0.5926],
        [0.9271, 0.8130, 0.7850, 0.5890, 0.5711],
        [0.9215, 0.8264, 0.7316, 0.5300, 0.6001],
        [0.9288, 0.8290, 0.7561, 0.5668, 0.6080]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4756, 0.5224, 0.4080, 0.6652],
        [0.2939, 0.3350, 0.3514, 0.4689],
        [0.6136, 0.5696, 0.4087, 0.6786],
        [0.3127, 0.5071, 0.2694, 0.7682]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)]
====================
schedule for this step is
[(7, 1)]
inputting microbatch 7 into partition 1
before moving to cuda:1: tensor([[0.4756, 0.5224, 0.4080, 0.6652],
        [0.2939, 0.3350, 0.3514, 0.4689],
        [0.6136, 0.5696, 0.4087, 0.6786],
        [0.3127, 0.5071, 0.2694, 0.7682]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9220, 0.7980, 0.7202, 0.5453],
        [0.5926, 0.9271, 0.8130, 0.7850],
        [0.5890, 0.5711, 0.9215, 0.8264],
        [0.7316, 0.5300, 0.6001, 0.9288]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 7
current batch shape is torch.Size([4, 4])
batch is tensor([[0.9220, 0.7980, 0.7202, 0.5453],
        [0.5926, 0.9271, 0.8130, 0.7850],
        [0.5890, 0.5711, 0.9215, 0.8264],
        [0.7316, 0.5300, 0.6001, 0.9288]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 7
********************
receiving microbatch 7 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 7, tensor([[0.9000, 0.7926, 0.6579, 0.4422, 0.5581],
        [0.8847, 0.7766, 0.6827, 0.7110, 0.7546],
        [0.8871, 0.7136, 0.5961, 0.5320, 0.6210],
        [0.9059, 0.7247, 0.7826, 0.6445, 0.5337]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 7
result is tensor([[0.9000, 0.7926, 0.6579, 0.4422, 0.5581],
        [0.8847, 0.7766, 0.6827, 0.7110, 0.7546],
        [0.8871, 0.7136, 0.5961, 0.5320, 0.6210],
        [0.9059, 0.7247, 0.7826, 0.6445, 0.5337]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9000, 0.7926, 0.6579, 0.4422, 0.5581],
        [0.8847, 0.7766, 0.6827, 0.7110, 0.7546],
        [0.8871, 0.7136, 0.5961, 0.5320, 0.6210],
        [0.9059, 0.7247, 0.7826, 0.6445, 0.5337]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.8852, 0.7727, 0.6931, 0.7869, 0.8090],
        [0.9163, 0.7507, 0.6543, 0.4134, 0.4884],
        [0.9533, 0.8055, 0.5853, 0.3726, 0.5781],
        [0.9834, 0.9542, 0.9971, 0.9649, 0.4984]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9105, 0.7847, 0.7502, 0.5701, 0.5651],
        [0.8966, 0.7984, 0.6881, 0.5840, 0.6617],
        [0.8676, 0.6731, 0.5300, 0.5574, 0.6675],
        [0.8966, 0.6741, 0.7730, 0.6788, 0.5365]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9005, 0.7893, 0.6507, 0.4382, 0.5579],
        [0.8753, 0.7685, 0.6421, 0.6806, 0.7546],
        [0.8762, 0.7299, 0.5693, 0.5334, 0.6605],
        [0.8860, 0.7031, 0.7262, 0.7016, 0.6427]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8916, 0.7889, 0.6414, 0.4360, 0.5648],
        [0.8712, 0.7641, 0.6327, 0.6706, 0.7502],
        [0.8896, 0.7582, 0.6193, 0.5476, 0.6544],
        [0.8891, 0.7038, 0.7272, 0.6745, 0.6144]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.0000, 0.0000, 1.0000, 1.0000],
        [0.8713, 0.7642, 0.6312, 0.6641, 0.7462],
        [0.8904, 0.7563, 0.6227, 0.5566, 0.6572],
        [0.8939, 0.7132, 0.7388, 0.6619, 0.5966]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8984, 0.8506, 0.9898, 0.9994, 0.9821],
        [0.8948, 0.8468, 0.9884, 0.9992, 0.9810],
        [0.8933, 0.8394, 0.9998, 1.0000, 0.9789],
        [1.0000, 1.0000, 1.0000, 0.9812, 0.0000]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9220, 0.7980, 0.7202, 0.5453, 0.5926],
        [0.9271, 0.8130, 0.7850, 0.5890, 0.5711],
        [0.9215, 0.8264, 0.7316, 0.5300, 0.6001],
        [0.9288, 0.8290, 0.7561, 0.5668, 0.6080]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9000, 0.7926, 0.6579, 0.4422, 0.5581],
        [0.8847, 0.7766, 0.6827, 0.7110, 0.7546],
        [0.8871, 0.7136, 0.5961, 0.5320, 0.6210],
        [0.9059, 0.7247, 0.7826, 0.6445, 0.5337]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)]
finished running cycle
_____________________________ test_forward_0[4-64] _____________________________

batch_size = 64, split_size = 4

    @pytest.mark.a4_2_2
    @pytest.mark.parametrize("batch_size", [1, 16, 32, 64])
    @pytest.mark.parametrize("split_size", [1, 2, 4, 8, 16])
    # @pytest.mark.parametrize("batch_size", [1])
    # @pytest.mark.parametrize("split_size", [1, 2])
    def test_forward_0(batch_size, split_size):
        model = nn.Sequential(
            nn.Linear(3, 4).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
            nn.Linear(4, 5).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
        )
    
        x = torch.randn(batch_size, 3).to('cuda:0')
        y0 = model(x).to('cpu')
    
        # move the last two layer to another device
        model[-2] = model[-2].to('cuda:1')
        model[-1] = WithDevice(nn.Sigmoid(), 'cuda:1')
        pipe = Pipe(model, split_size=split_size)
        y1 = pipe(x).to('cpu')
>       assert torch.allclose(y0, y1)
E       assert False
E        +  where False = <built-in method allclose of type object at 0x153da78cb760>(tensor([[0.5485, 0.4449, 0.4226, 0.5627, 0.4636],\n        [0.5505, 0.4315, 0.4489, 0.5202, 0.4644],\n        [0.5484, 0.4402, 0.4312, 0.5494, 0.4638],\n        [0.5602, 0.4455, 0.4255, 0.5663, 0.4664],\n        [0.5106, 0.4259, 0.4394, 0.5065, 0.4564],\n        [0.5831, 0.4501, 0.4264, 0.5763, 0.4711],\n        [0.5365, 0.4474, 0.4138, 0.5652, 0.4602],\n        [0.5745, 0.4411, 0.4385, 0.5503, 0.4697],\n        [0.5724, 0.4330, 0.4538, 0.5496, 0.4719],\n        [0.5097, 0.4500, 0.3992, 0.5410, 0.4507],\n        [0.5563, 0.4448, 0.4253, 0.5480, 0.4639],\n        [0.5705, 0.4358, 0.4479, 0.5587, 0.4714],\n        [0.5637, 0.4481, 0.4218, 0.5573, 0.4654],\n        [0.5583, 0.4477, 0.4208, 0.5652, 0.4651],\n        [0.5702, 0.4431, 0.4336, 0.5594, 0.4689],\n        [0.5071, 0.4526, 0.3938, 0.5484, 0.4499],\n        [0.5789, 0.4208, 0.4800, 0.5201, 0.4748],\n        [0.5278, 0.4418, 0.4212, 0.5216, 0.4556],\n        [0.5153, 0.4354, 0.4310, 0.4934, 0.4520],\n        [0.5187, 0.4393, 0.4218, 0.5325, 0.4556],\n        [0.5178, 0.4464, 0.4091, 0.5497, 0.4546],\n        [0.5351, 0.4442, 0.4192, 0.5496, 0.4594],\n        [0.5602, 0.4301, 0.4546, 0.5358, 0.4687],\n        [0.5487, 0.4438, 0.4246, 0.5547, 0.4632]...441, 0.5393, 0.4691],\n        [0.5466, 0.4498, 0.4128, 0.5647, 0.4616],\n        [0.5261, 0.4457, 0.4131, 0.5350, 0.4553],\n        [0.5677, 0.4305, 0.4566, 0.5237, 0.4691],\n        [0.4963, 0.4397, 0.4112, 0.5214, 0.4495],\n        [0.5656, 0.4551, 0.4104, 0.5754, 0.4652],\n        [0.5453, 0.4540, 0.4046, 0.5687, 0.4602],\n        [0.5513, 0.4592, 0.3958, 0.5961, 0.4628],\n        [0.5366, 0.4483, 0.4121, 0.5506, 0.4584],\n        [0.5212, 0.4425, 0.4175, 0.5216, 0.4539],\n        [0.5485, 0.4311, 0.4482, 0.5296, 0.4650],\n        [0.5768, 0.4410, 0.4396, 0.5530, 0.4705],\n        [0.5220, 0.4274, 0.4471, 0.4899, 0.4561],\n        [0.5728, 0.4255, 0.4686, 0.5125, 0.4709],\n        [0.5456, 0.4537, 0.4051, 0.5731, 0.4609],\n        [0.5323, 0.4380, 0.4295, 0.5410, 0.4601],\n        [0.5332, 0.4283, 0.4498, 0.4994, 0.4593],\n        [0.5492, 0.4343, 0.4435, 0.5158, 0.4626],\n        [0.5690, 0.4514, 0.4178, 0.5577, 0.4655],\n        [0.5565, 0.4490, 0.4178, 0.5644, 0.4641],\n        [0.5346, 0.4501, 0.4081, 0.5678, 0.4590],\n        [0.5680, 0.4462, 0.4270, 0.5597, 0.4673],\n        [0.5726, 0.4403, 0.4395, 0.5530, 0.4698],\n        [0.5394, 0.4474, 0.4147, 0.5555, 0.4598]], grad_fn=<ToCopyBackward0>), tensor([[0.9448, 0.9472, 0.9503, 0.9406, 0.9381],\n        [0.9410, 0.9399, 0.9431, 0.9381, 0.9332],\n        [0.9465, 0.9404, 0.9421, 0.9300, 0.9365],\n        [0.9503, 0.9421, 0.9443, 0.9279, 0.9394],\n        [0.9715, 0.9712, 0.9732, 0.9643, 0.9661],\n        [0.9710, 0.9708, 0.9728, 0.9639, 0.9656],\n        [0.9709, 0.9707, 0.9727, 0.9638, 0.9655],\n        [0.9709, 0.9706, 0.9727, 0.9639, 0.9655],\n        [0.9733, 0.9731, 0.9751, 0.9665, 0.9682],\n        [0.9732, 0.9731, 0.9751, 0.9665, 0.9681],\n        [0.9730, 0.9729, 0.9749, 0.9663, 0.9679],\n        [0.9731, 0.9729, 0.9749, 0.9664, 0.9680],\n        [0.9734, 0.9732, 0.9753, 0.9666, 0.9683],\n        [0.9734, 0.9733, 0.9753, 0.9667, 0.9683],\n        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],\n        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682],\n        [0.9734, 0.9733, 0.9753, 0.9666, 0.9683],\n        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],\n        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],\n        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682],\n        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.9932, 0.9920, 0.9923, 0.9907, 0.9911],\n        [0.9375, 0.9843, 0.9779, 0.9714, 0.9572]...753, 0.9666, 0.9683],\n        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],\n        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],\n        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682],\n        [0.9734, 0.9733, 0.9753, 0.9666, 0.9683],\n        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],\n        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],\n        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682],\n        [0.9734, 0.9733, 0.9753, 0.9666, 0.9683],\n        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],\n        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],\n        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682],\n        [0.9734, 0.9733, 0.9753, 0.9666, 0.9683],\n        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],\n        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],\n        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682],\n        [0.9734, 0.9733, 0.9753, 0.9666, 0.9683],\n        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],\n        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],\n        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682],\n        [0.9734, 0.9733, 0.9753, 0.9666, 0.9683],\n        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],\n        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],\n        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], grad_fn=<ToCopyBackward0>))
E        +    where <built-in method allclose of type object at 0x153da78cb760> = torch.allclose

batch_size = 64
model      = Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
  (2): Linear(in_features=4, out_features=5, bias=True)
  (3): WithDevice(
    (_module): Sigmoid()
  )
)
pipe       = Pipe(
  (partitions): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=3, out_features=4, bias=True)
   ...inear(in_features=4, out_features=5, bias=True)
      (1): WithDevice(
        (_module): Sigmoid()
      )
    )
  )
)
split_size = 4
x          = tensor([[-0.6424, -0.9590,  0.0238],
        [ 1.1133,  0.3146,  0.3763],
        [-0.0758, -0.6715,  0.0766],
       ...-0.7282,  0.3135,  1.2061],
        [-0.0195, -0.0362,  1.3275],
        [-0.9153, -0.2002, -0.2004]], device='cuda:0')
y0         = tensor([[0.5485, 0.4449, 0.4226, 0.5627, 0.4636],
        [0.5505, 0.4315, 0.4489, 0.5202, 0.4644],
        [0.5484, 0...[0.5726, 0.4403, 0.4395, 0.5530, 0.4698],
        [0.5394, 0.4474, 0.4147, 0.5555, 0.4598]], grad_fn=<ToCopyBackward0>)
y1         = tensor([[0.9448, 0.9472, 0.9503, 0.9406, 0.9381],
        [0.9410, 0.9399, 0.9431, 0.9381, 0.9332],
        [0.9465, 0...[0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], grad_fn=<ToCopyBackward0>)

tests/test_pipeline.py:83: AssertionError
----------------------------- Captured stdout call -----------------------------
in init
ModuleList(
  (0): Sequential(
    (0): Linear(in_features=3, out_features=4, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
  (1): Sequential(
    (0): Linear(in_features=4, out_features=5, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
)
[device(type='cuda', index=0), device(type='cuda', index=1)]
before running the cycle
[tensor([[-0.6424, -0.9590,  0.0238],
        [ 1.1133,  0.3146,  0.3763],
        [-0.0758, -0.6715,  0.0766],
        [-0.6854, -0.7171,  0.6136]], device='cuda:0'), tensor([[ 1.9451, -1.2806, -2.5202],
        [-1.1854,  0.1370,  1.9947],
        [-0.9706, -1.1769, -0.5604],
        [-0.1166,  0.4733,  1.5325]], device='cuda:0'), tensor([[ 0.9948, -1.3726,  1.1132],
        [-1.3585,  0.5914, -1.4143],
        [-0.5605,  0.6331,  0.7424],
        [ 0.6689, -1.7510,  0.9586]], device='cuda:0'), tensor([[-0.9524,  0.7377,  1.1179],
        [-0.9370, -0.2675,  0.6341],
        [-0.3577, -0.1804,  1.1922],
        [-1.6772,  0.3181, -1.5878]], device='cuda:0'), tensor([[ 2.8154, -0.9979,  1.6711],
        [-0.1764,  1.3011, -0.4159],
        [ 1.1769,  1.8420, -1.2100],
        [ 0.0242, -0.5054, -1.2763]], device='cuda:0'), tensor([[-0.8394, -0.7112, -1.2919],
        [-0.5520, -0.4516, -0.4546],
        [ 1.2171, -1.0212,  0.5491],
        [-0.4934, -0.4480,  0.1458]], device='cuda:0'), tensor([[-0.2548, -1.3461, -0.8878],
        [ 1.1111, -0.7347,  0.8447],
        [ 0.2399, -1.0283,  0.4258],
        [-0.8746,  0.9923,  0.2034]], device='cuda:0'), tensor([[ 0.7326, -0.6715,  1.7661],
        [-0.6693, -1.7338,  0.3703],
        [-1.8851,  0.6470, -0.7668],
        [ 1.7409,  0.6931, -1.5818]], device='cuda:0'), tensor([[-0.2513,  0.6103, -0.1242],
        [-0.1741, -0.4948,  0.3420],
        [-0.4873,  1.0306, -0.1387],
        [ 0.0624, -0.2377, -1.1928]], device='cuda:0'), tensor([[ 0.2147, -1.7384, -0.5185],
        [ 0.4374,  0.2843, -0.5562],
        [ 0.2163,  1.0030, -0.1813],
        [ 0.4046,  0.2900, -0.1435]], device='cuda:0'), tensor([[ 0.3273,  0.6742,  1.4397],
        [-1.2014, -0.2807,  0.1094],
        [-0.7194,  0.8177, -0.5809],
        [ 1.2515,  0.5807,  1.2761]], device='cuda:0'), tensor([[-0.0676, -0.2095, -2.4560],
        [-1.8055,  0.5198,  1.2424],
        [-1.6933,  0.1127,  0.1598],
        [-2.6395, -1.0274,  0.2296]], device='cuda:0'), tensor([[-1.0079,  0.3010, -0.2049],
        [-0.2963,  1.1941, -0.7664],
        [ 1.0457, -0.6663,  0.0405],
        [-0.0975,  0.2815,  1.5998]], device='cuda:0'), tensor([[ 2.4996,  0.5638, -1.5046],
        [ 2.0686,  0.9217,  1.6783],
        [-1.6940, -0.3129,  0.0719],
        [ 0.1603, -0.9724, -0.7393]], device='cuda:0'), tensor([[ 1.9289,  0.6337, -0.5543],
        [ 0.8780,  1.2082,  0.5716],
        [-1.3254,  1.5558,  1.6003],
        [-1.0849, -0.0165,  0.6126]], device='cuda:0'), tensor([[-1.2905, -0.9718, -0.5923],
        [-0.7282,  0.3135,  1.2061],
        [-0.0195, -0.0362,  1.3275],
        [-0.9153, -0.2002, -0.2004]], device='cuda:0')]
====================
schedule for this step is
[(0, 0)]
inputting microbatch 0 into partition 0
before moving to cuda:0: tensor([[-0.6424, -0.9590,  0.0238],
        [ 1.1133,  0.3146,  0.3763],
        [-0.0758, -0.6715,  0.0766],
        [-0.6854, -0.7171,  0.6136]], device='cuda:0')
after: tensor([[-0.6424, -0.9590,  0.0238],
        [ 1.1133,  0.3146,  0.3763],
        [-0.0758, -0.6715,  0.0766],
        [-0.6854, -0.7171,  0.6136]], device='cuda:0')
********************
observing microbatch 0
current batch shape is torch.Size([4, 3])
batch is tensor([[-0.6424, -0.9590,  0.0238],
        [ 1.1133,  0.3146,  0.3763],
        [-0.0758, -0.6715,  0.0766],
        [-0.6854, -0.7171,  0.6136]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 0
********************
receiving microbatch 0 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 0, tensor([[0.4268, 0.4040, 0.5188, 0.4447],
        [0.7108, 0.4303, 0.3193, 0.6765],
        [0.5125, 0.4057, 0.4474, 0.5116],
        [0.4346, 0.3652, 0.5978, 0.4741]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.4268, 0.4040, 0.5188, 0.4447],
        [0.7108, 0.4303, 0.3193, 0.6765],
        [0.5125, 0.4057, 0.4474, 0.5116],
        [0.4346, 0.3652, 0.5978, 0.4741]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4268, 0.4040, 0.5188, 0.4447],
        [0.7108, 0.4303, 0.3193, 0.6765],
        [0.5125, 0.4057, 0.4474, 0.5116],
        [0.4346, 0.3652, 0.5978, 0.4741]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.4268, 0.4040, 0.5188, 0.4447],
        [0.7108, 0.4303, 0.3193, 0.6765],
        [0.5125, 0.4057, 0.4474, 0.5116],
        [0.4346, 0.3652, 0.5978, 0.4741]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 1.9451, -1.2806, -2.5202],
        [-1.1854,  0.1370,  1.9947],
        [-0.9706, -1.1769, -0.5604],
        [-0.1166,  0.4733,  1.5325]], device='cuda:0'), tensor([[ 0.9948, -1.3726,  1.1132],
        [-1.3585,  0.5914, -1.4143],
        [-0.5605,  0.6331,  0.7424],
        [ 0.6689, -1.7510,  0.9586]], device='cuda:0'), tensor([[-0.9524,  0.7377,  1.1179],
        [-0.9370, -0.2675,  0.6341],
        [-0.3577, -0.1804,  1.1922],
        [-1.6772,  0.3181, -1.5878]], device='cuda:0'), tensor([[ 2.8154, -0.9979,  1.6711],
        [-0.1764,  1.3011, -0.4159],
        [ 1.1769,  1.8420, -1.2100],
        [ 0.0242, -0.5054, -1.2763]], device='cuda:0'), tensor([[-0.8394, -0.7112, -1.2919],
        [-0.5520, -0.4516, -0.4546],
        [ 1.2171, -1.0212,  0.5491],
        [-0.4934, -0.4480,  0.1458]], device='cuda:0'), tensor([[-0.2548, -1.3461, -0.8878],
        [ 1.1111, -0.7347,  0.8447],
        [ 0.2399, -1.0283,  0.4258],
        [-0.8746,  0.9923,  0.2034]], device='cuda:0'), tensor([[ 0.7326, -0.6715,  1.7661],
        [-0.6693, -1.7338,  0.3703],
        [-1.8851,  0.6470, -0.7668],
        [ 1.7409,  0.6931, -1.5818]], device='cuda:0'), tensor([[-0.2513,  0.6103, -0.1242],
        [-0.1741, -0.4948,  0.3420],
        [-0.4873,  1.0306, -0.1387],
        [ 0.0624, -0.2377, -1.1928]], device='cuda:0'), tensor([[ 0.2147, -1.7384, -0.5185],
        [ 0.4374,  0.2843, -0.5562],
        [ 0.2163,  1.0030, -0.1813],
        [ 0.4046,  0.2900, -0.1435]], device='cuda:0'), tensor([[ 0.3273,  0.6742,  1.4397],
        [-1.2014, -0.2807,  0.1094],
        [-0.7194,  0.8177, -0.5809],
        [ 1.2515,  0.5807,  1.2761]], device='cuda:0'), tensor([[-0.0676, -0.2095, -2.4560],
        [-1.8055,  0.5198,  1.2424],
        [-1.6933,  0.1127,  0.1598],
        [-2.6395, -1.0274,  0.2296]], device='cuda:0'), tensor([[-1.0079,  0.3010, -0.2049],
        [-0.2963,  1.1941, -0.7664],
        [ 1.0457, -0.6663,  0.0405],
        [-0.0975,  0.2815,  1.5998]], device='cuda:0'), tensor([[ 2.4996,  0.5638, -1.5046],
        [ 2.0686,  0.9217,  1.6783],
        [-1.6940, -0.3129,  0.0719],
        [ 0.1603, -0.9724, -0.7393]], device='cuda:0'), tensor([[ 1.9289,  0.6337, -0.5543],
        [ 0.8780,  1.2082,  0.5716],
        [-1.3254,  1.5558,  1.6003],
        [-1.0849, -0.0165,  0.6126]], device='cuda:0'), tensor([[-1.2905, -0.9718, -0.5923],
        [-0.7282,  0.3135,  1.2061],
        [-0.0195, -0.0362,  1.3275],
        [-0.9153, -0.2002, -0.2004]], device='cuda:0')]
====================
schedule for this step is
[(1, 0), (0, 1)]
inputting microbatch 1 into partition 0
before moving to cuda:0: tensor([[ 1.9451, -1.2806, -2.5202],
        [-1.1854,  0.1370,  1.9947],
        [-0.9706, -1.1769, -0.5604],
        [-0.1166,  0.4733,  1.5325]], device='cuda:0')
after: tensor([[ 1.9451, -1.2806, -2.5202],
        [-1.1854,  0.1370,  1.9947],
        [-0.9706, -1.1769, -0.5604],
        [-0.1166,  0.4733,  1.5325]], device='cuda:0')
********************
observing microbatch 1
current batch shape is torch.Size([4, 3])
batch is tensor([[ 1.9451, -1.2806, -2.5202],
        [-1.1854,  0.1370,  1.9947],
        [-0.9706, -1.1769, -0.5604],
        [-0.1166,  0.4733,  1.5325]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 1
********************
inputting microbatch 0 into partition 1
before moving to cuda:1: tensor([[0.4268, 0.4040, 0.5188, 0.4447],
        [0.7108, 0.4303, 0.3193, 0.6765],
        [0.5125, 0.4057, 0.4474, 0.5116],
        [0.4346, 0.3652, 0.5978, 0.4741]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.5890, 0.5711, 0.9215, 0.8264],
        [0.7316, 0.5300, 0.6001, 0.9288],
        [0.8290, 0.7561, 0.5668, 0.6080],
        [0.9000, 0.7926, 0.6579, 0.4422]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 0
current batch shape is torch.Size([4, 4])
batch is tensor([[0.5890, 0.5711, 0.9215, 0.8264],
        [0.7316, 0.5300, 0.6001, 0.9288],
        [0.8290, 0.7561, 0.5668, 0.6080],
        [0.9000, 0.7926, 0.6579, 0.4422]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 0
********************
receiving microbatch 1 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 1, tensor([[0.7037, 0.5567, 0.0702, 0.5600],
        [0.4359, 0.3244, 0.7868, 0.5315],
        [0.3826, 0.4622, 0.4841, 0.3888],
        [0.5874, 0.3669, 0.6169, 0.6256]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 1
result is tensor([[0.7037, 0.5567, 0.0702, 0.5600],
        [0.4359, 0.3244, 0.7868, 0.5315],
        [0.3826, 0.4622, 0.4841, 0.3888],
        [0.5874, 0.3669, 0.6169, 0.6256]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.7037, 0.5567, 0.0702, 0.5600],
        [0.4359, 0.3244, 0.7868, 0.5315],
        [0.3826, 0.4622, 0.4841, 0.3888],
        [0.5874, 0.3669, 0.6169, 0.6256]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 0 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 0, tensor([[0.9448, 0.9472, 0.9503, 0.9406, 0.9381],
        [0.9410, 0.9399, 0.9431, 0.9381, 0.9332],
        [0.9465, 0.9404, 0.9421, 0.9300, 0.9365],
        [0.9503, 0.9421, 0.9443, 0.9279, 0.9394]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.9448, 0.9472, 0.9503, 0.9406, 0.9381],
        [0.9410, 0.9399, 0.9431, 0.9381, 0.9332],
        [0.9465, 0.9404, 0.9421, 0.9300, 0.9365],
        [0.9503, 0.9421, 0.9443, 0.9279, 0.9394]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9448, 0.9472, 0.9503, 0.9406, 0.9381],
        [0.9410, 0.9399, 0.9431, 0.9381, 0.9332],
        [0.9465, 0.9404, 0.9421, 0.9300, 0.9365],
        [0.9503, 0.9421, 0.9443, 0.9279, 0.9394]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9448, 0.9472, 0.9503, 0.9406, 0.9381],
        [0.9410, 0.9399, 0.9431, 0.9381, 0.9332],
        [0.9465, 0.9404, 0.9421, 0.9300, 0.9365],
        [0.9503, 0.9421, 0.9443, 0.9279, 0.9394]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.7037, 0.5567, 0.0702, 0.5600],
        [0.4359, 0.3244, 0.7868, 0.5315],
        [0.3826, 0.4622, 0.4841, 0.3888],
        [0.5874, 0.3669, 0.6169, 0.6256]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.9948, -1.3726,  1.1132],
        [-1.3585,  0.5914, -1.4143],
        [-0.5605,  0.6331,  0.7424],
        [ 0.6689, -1.7510,  0.9586]], device='cuda:0'), tensor([[-0.9524,  0.7377,  1.1179],
        [-0.9370, -0.2675,  0.6341],
        [-0.3577, -0.1804,  1.1922],
        [-1.6772,  0.3181, -1.5878]], device='cuda:0'), tensor([[ 2.8154, -0.9979,  1.6711],
        [-0.1764,  1.3011, -0.4159],
        [ 1.1769,  1.8420, -1.2100],
        [ 0.0242, -0.5054, -1.2763]], device='cuda:0'), tensor([[-0.8394, -0.7112, -1.2919],
        [-0.5520, -0.4516, -0.4546],
        [ 1.2171, -1.0212,  0.5491],
        [-0.4934, -0.4480,  0.1458]], device='cuda:0'), tensor([[-0.2548, -1.3461, -0.8878],
        [ 1.1111, -0.7347,  0.8447],
        [ 0.2399, -1.0283,  0.4258],
        [-0.8746,  0.9923,  0.2034]], device='cuda:0'), tensor([[ 0.7326, -0.6715,  1.7661],
        [-0.6693, -1.7338,  0.3703],
        [-1.8851,  0.6470, -0.7668],
        [ 1.7409,  0.6931, -1.5818]], device='cuda:0'), tensor([[-0.2513,  0.6103, -0.1242],
        [-0.1741, -0.4948,  0.3420],
        [-0.4873,  1.0306, -0.1387],
        [ 0.0624, -0.2377, -1.1928]], device='cuda:0'), tensor([[ 0.2147, -1.7384, -0.5185],
        [ 0.4374,  0.2843, -0.5562],
        [ 0.2163,  1.0030, -0.1813],
        [ 0.4046,  0.2900, -0.1435]], device='cuda:0'), tensor([[ 0.3273,  0.6742,  1.4397],
        [-1.2014, -0.2807,  0.1094],
        [-0.7194,  0.8177, -0.5809],
        [ 1.2515,  0.5807,  1.2761]], device='cuda:0'), tensor([[-0.0676, -0.2095, -2.4560],
        [-1.8055,  0.5198,  1.2424],
        [-1.6933,  0.1127,  0.1598],
        [-2.6395, -1.0274,  0.2296]], device='cuda:0'), tensor([[-1.0079,  0.3010, -0.2049],
        [-0.2963,  1.1941, -0.7664],
        [ 1.0457, -0.6663,  0.0405],
        [-0.0975,  0.2815,  1.5998]], device='cuda:0'), tensor([[ 2.4996,  0.5638, -1.5046],
        [ 2.0686,  0.9217,  1.6783],
        [-1.6940, -0.3129,  0.0719],
        [ 0.1603, -0.9724, -0.7393]], device='cuda:0'), tensor([[ 1.9289,  0.6337, -0.5543],
        [ 0.8780,  1.2082,  0.5716],
        [-1.3254,  1.5558,  1.6003],
        [-1.0849, -0.0165,  0.6126]], device='cuda:0'), tensor([[-1.2905, -0.9718, -0.5923],
        [-0.7282,  0.3135,  1.2061],
        [-0.0195, -0.0362,  1.3275],
        [-0.9153, -0.2002, -0.2004]], device='cuda:0')]
====================
schedule for this step is
[(2, 0), (1, 1)]
inputting microbatch 2 into partition 0
before moving to cuda:0: tensor([[ 0.9948, -1.3726,  1.1132],
        [-1.3585,  0.5914, -1.4143],
        [-0.5605,  0.6331,  0.7424],
        [ 0.6689, -1.7510,  0.9586]], device='cuda:0')
after: tensor([[ 0.9948, -1.3726,  1.1132],
        [-1.3585,  0.5914, -1.4143],
        [-0.5605,  0.6331,  0.7424],
        [ 0.6689, -1.7510,  0.9586]], device='cuda:0')
********************
observing microbatch 2
current batch shape is torch.Size([4, 3])
batch is tensor([[ 0.9948, -1.3726,  1.1132],
        [-1.3585,  0.5914, -1.4143],
        [-0.5605,  0.6331,  0.7424],
        [ 0.6689, -1.7510,  0.9586]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 2
********************
inputting microbatch 1 into partition 1
before moving to cuda:1: tensor([[0.7037, 0.5567, 0.0702, 0.5600],
        [0.4359, 0.3244, 0.7868, 0.5315],
        [0.3826, 0.4622, 0.4841, 0.3888],
        [0.5874, 0.3669, 0.6169, 0.6256]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9448, 0.9472, 0.9503, 0.9406],
        [0.9381, 0.9410, 0.9399, 0.9431],
        [0.9381, 0.9332, 0.9465, 0.9404],
        [0.9421, 0.9300, 0.9365, 0.9503]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 1
current batch shape is torch.Size([4, 4])
batch is tensor([[0.9448, 0.9472, 0.9503, 0.9406],
        [0.9381, 0.9410, 0.9399, 0.9431],
        [0.9381, 0.9332, 0.9465, 0.9404],
        [0.9421, 0.9300, 0.9365, 0.9503]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 1
********************
receiving microbatch 2 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 2, tensor([[0.5486, 0.1994, 0.4851, 0.5758],
        [0.5109, 0.7578, 0.3567, 0.4570],
        [0.5667, 0.4963, 0.5567, 0.5818],
        [0.4820, 0.1927, 0.5159, 0.5194]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 2
result is tensor([[0.5486, 0.1994, 0.4851, 0.5758],
        [0.5109, 0.7578, 0.3567, 0.4570],
        [0.5667, 0.4963, 0.5567, 0.5818],
        [0.4820, 0.1927, 0.5159, 0.5194]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5486, 0.1994, 0.4851, 0.5758],
        [0.5109, 0.7578, 0.3567, 0.4570],
        [0.5667, 0.4963, 0.5567, 0.5818],
        [0.4820, 0.1927, 0.5159, 0.5194]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 1 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 1, tensor([[0.9715, 0.9712, 0.9732, 0.9643, 0.9661],
        [0.9710, 0.9708, 0.9728, 0.9639, 0.9656],
        [0.9709, 0.9707, 0.9727, 0.9638, 0.9655],
        [0.9709, 0.9706, 0.9727, 0.9639, 0.9655]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 1
result is tensor([[0.9715, 0.9712, 0.9732, 0.9643, 0.9661],
        [0.9710, 0.9708, 0.9728, 0.9639, 0.9656],
        [0.9709, 0.9707, 0.9727, 0.9638, 0.9655],
        [0.9709, 0.9706, 0.9727, 0.9639, 0.9655]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9715, 0.9712, 0.9732, 0.9643, 0.9661],
        [0.9710, 0.9708, 0.9728, 0.9639, 0.9656],
        [0.9709, 0.9707, 0.9727, 0.9638, 0.9655],
        [0.9709, 0.9706, 0.9727, 0.9639, 0.9655]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9448, 0.9472, 0.9503, 0.9406, 0.9381],
        [0.9410, 0.9399, 0.9431, 0.9381, 0.9332],
        [0.9465, 0.9404, 0.9421, 0.9300, 0.9365],
        [0.9503, 0.9421, 0.9443, 0.9279, 0.9394]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9715, 0.9712, 0.9732, 0.9643, 0.9661],
        [0.9710, 0.9708, 0.9728, 0.9639, 0.9656],
        [0.9709, 0.9707, 0.9727, 0.9638, 0.9655],
        [0.9709, 0.9706, 0.9727, 0.9639, 0.9655]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5486, 0.1994, 0.4851, 0.5758],
        [0.5109, 0.7578, 0.3567, 0.4570],
        [0.5667, 0.4963, 0.5567, 0.5818],
        [0.4820, 0.1927, 0.5159, 0.5194]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.9524,  0.7377,  1.1179],
        [-0.9370, -0.2675,  0.6341],
        [-0.3577, -0.1804,  1.1922],
        [-1.6772,  0.3181, -1.5878]], device='cuda:0'), tensor([[ 2.8154, -0.9979,  1.6711],
        [-0.1764,  1.3011, -0.4159],
        [ 1.1769,  1.8420, -1.2100],
        [ 0.0242, -0.5054, -1.2763]], device='cuda:0'), tensor([[-0.8394, -0.7112, -1.2919],
        [-0.5520, -0.4516, -0.4546],
        [ 1.2171, -1.0212,  0.5491],
        [-0.4934, -0.4480,  0.1458]], device='cuda:0'), tensor([[-0.2548, -1.3461, -0.8878],
        [ 1.1111, -0.7347,  0.8447],
        [ 0.2399, -1.0283,  0.4258],
        [-0.8746,  0.9923,  0.2034]], device='cuda:0'), tensor([[ 0.7326, -0.6715,  1.7661],
        [-0.6693, -1.7338,  0.3703],
        [-1.8851,  0.6470, -0.7668],
        [ 1.7409,  0.6931, -1.5818]], device='cuda:0'), tensor([[-0.2513,  0.6103, -0.1242],
        [-0.1741, -0.4948,  0.3420],
        [-0.4873,  1.0306, -0.1387],
        [ 0.0624, -0.2377, -1.1928]], device='cuda:0'), tensor([[ 0.2147, -1.7384, -0.5185],
        [ 0.4374,  0.2843, -0.5562],
        [ 0.2163,  1.0030, -0.1813],
        [ 0.4046,  0.2900, -0.1435]], device='cuda:0'), tensor([[ 0.3273,  0.6742,  1.4397],
        [-1.2014, -0.2807,  0.1094],
        [-0.7194,  0.8177, -0.5809],
        [ 1.2515,  0.5807,  1.2761]], device='cuda:0'), tensor([[-0.0676, -0.2095, -2.4560],
        [-1.8055,  0.5198,  1.2424],
        [-1.6933,  0.1127,  0.1598],
        [-2.6395, -1.0274,  0.2296]], device='cuda:0'), tensor([[-1.0079,  0.3010, -0.2049],
        [-0.2963,  1.1941, -0.7664],
        [ 1.0457, -0.6663,  0.0405],
        [-0.0975,  0.2815,  1.5998]], device='cuda:0'), tensor([[ 2.4996,  0.5638, -1.5046],
        [ 2.0686,  0.9217,  1.6783],
        [-1.6940, -0.3129,  0.0719],
        [ 0.1603, -0.9724, -0.7393]], device='cuda:0'), tensor([[ 1.9289,  0.6337, -0.5543],
        [ 0.8780,  1.2082,  0.5716],
        [-1.3254,  1.5558,  1.6003],
        [-1.0849, -0.0165,  0.6126]], device='cuda:0'), tensor([[-1.2905, -0.9718, -0.5923],
        [-0.7282,  0.3135,  1.2061],
        [-0.0195, -0.0362,  1.3275],
        [-0.9153, -0.2002, -0.2004]], device='cuda:0')]
====================
schedule for this step is
[(3, 0), (2, 1)]
inputting microbatch 3 into partition 0
before moving to cuda:0: tensor([[-0.9524,  0.7377,  1.1179],
        [-0.9370, -0.2675,  0.6341],
        [-0.3577, -0.1804,  1.1922],
        [-1.6772,  0.3181, -1.5878]], device='cuda:0')
after: tensor([[-0.9524,  0.7377,  1.1179],
        [-0.9370, -0.2675,  0.6341],
        [-0.3577, -0.1804,  1.1922],
        [-1.6772,  0.3181, -1.5878]], device='cuda:0')
********************
observing microbatch 3
current batch shape is torch.Size([4, 3])
batch is tensor([[-0.9524,  0.7377,  1.1179],
        [-0.9370, -0.2675,  0.6341],
        [-0.3577, -0.1804,  1.1922],
        [-1.6772,  0.3181, -1.5878]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 3
********************
inputting microbatch 2 into partition 1
before moving to cuda:1: tensor([[0.5486, 0.1994, 0.4851, 0.5758],
        [0.5109, 0.7578, 0.3567, 0.4570],
        [0.5667, 0.4963, 0.5567, 0.5818],
        [0.4820, 0.1927, 0.5159, 0.5194]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9715, 0.9712, 0.9732, 0.9643],
        [0.9661, 0.9710, 0.9708, 0.9728],
        [0.9639, 0.9656, 0.9709, 0.9707],
        [0.9727, 0.9638, 0.9655, 0.9709]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 2
current batch shape is torch.Size([4, 4])
batch is tensor([[0.9715, 0.9712, 0.9732, 0.9643],
        [0.9661, 0.9710, 0.9708, 0.9728],
        [0.9639, 0.9656, 0.9709, 0.9707],
        [0.9727, 0.9638, 0.9655, 0.9709]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 2
********************
receiving microbatch 3 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 3, tensor([[0.5284, 0.4806, 0.6497, 0.5682],
        [0.4474, 0.4228, 0.6157, 0.4860],
        [0.5088, 0.3451, 0.6201, 0.5536],
        [0.4547, 0.7602, 0.3788, 0.4084]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 3
result is tensor([[0.5284, 0.4806, 0.6497, 0.5682],
        [0.4474, 0.4228, 0.6157, 0.4860],
        [0.5088, 0.3451, 0.6201, 0.5536],
        [0.4547, 0.7602, 0.3788, 0.4084]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5284, 0.4806, 0.6497, 0.5682],
        [0.4474, 0.4228, 0.6157, 0.4860],
        [0.5088, 0.3451, 0.6201, 0.5536],
        [0.4547, 0.7602, 0.3788, 0.4084]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 2 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 2, tensor([[0.9733, 0.9731, 0.9751, 0.9665, 0.9682],
        [0.9732, 0.9731, 0.9751, 0.9665, 0.9681],
        [0.9730, 0.9729, 0.9749, 0.9663, 0.9679],
        [0.9731, 0.9729, 0.9749, 0.9664, 0.9680]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 2
result is tensor([[0.9733, 0.9731, 0.9751, 0.9665, 0.9682],
        [0.9732, 0.9731, 0.9751, 0.9665, 0.9681],
        [0.9730, 0.9729, 0.9749, 0.9663, 0.9679],
        [0.9731, 0.9729, 0.9749, 0.9664, 0.9680]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9733, 0.9731, 0.9751, 0.9665, 0.9682],
        [0.9732, 0.9731, 0.9751, 0.9665, 0.9681],
        [0.9730, 0.9729, 0.9749, 0.9663, 0.9679],
        [0.9731, 0.9729, 0.9749, 0.9664, 0.9680]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9448, 0.9472, 0.9503, 0.9406, 0.9381],
        [0.9410, 0.9399, 0.9431, 0.9381, 0.9332],
        [0.9465, 0.9404, 0.9421, 0.9300, 0.9365],
        [0.9503, 0.9421, 0.9443, 0.9279, 0.9394]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9715, 0.9712, 0.9732, 0.9643, 0.9661],
        [0.9710, 0.9708, 0.9728, 0.9639, 0.9656],
        [0.9709, 0.9707, 0.9727, 0.9638, 0.9655],
        [0.9709, 0.9706, 0.9727, 0.9639, 0.9655]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9733, 0.9731, 0.9751, 0.9665, 0.9682],
        [0.9732, 0.9731, 0.9751, 0.9665, 0.9681],
        [0.9730, 0.9729, 0.9749, 0.9663, 0.9679],
        [0.9731, 0.9729, 0.9749, 0.9664, 0.9680]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5284, 0.4806, 0.6497, 0.5682],
        [0.4474, 0.4228, 0.6157, 0.4860],
        [0.5088, 0.3451, 0.6201, 0.5536],
        [0.4547, 0.7602, 0.3788, 0.4084]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 2.8154, -0.9979,  1.6711],
        [-0.1764,  1.3011, -0.4159],
        [ 1.1769,  1.8420, -1.2100],
        [ 0.0242, -0.5054, -1.2763]], device='cuda:0'), tensor([[-0.8394, -0.7112, -1.2919],
        [-0.5520, -0.4516, -0.4546],
        [ 1.2171, -1.0212,  0.5491],
        [-0.4934, -0.4480,  0.1458]], device='cuda:0'), tensor([[-0.2548, -1.3461, -0.8878],
        [ 1.1111, -0.7347,  0.8447],
        [ 0.2399, -1.0283,  0.4258],
        [-0.8746,  0.9923,  0.2034]], device='cuda:0'), tensor([[ 0.7326, -0.6715,  1.7661],
        [-0.6693, -1.7338,  0.3703],
        [-1.8851,  0.6470, -0.7668],
        [ 1.7409,  0.6931, -1.5818]], device='cuda:0'), tensor([[-0.2513,  0.6103, -0.1242],
        [-0.1741, -0.4948,  0.3420],
        [-0.4873,  1.0306, -0.1387],
        [ 0.0624, -0.2377, -1.1928]], device='cuda:0'), tensor([[ 0.2147, -1.7384, -0.5185],
        [ 0.4374,  0.2843, -0.5562],
        [ 0.2163,  1.0030, -0.1813],
        [ 0.4046,  0.2900, -0.1435]], device='cuda:0'), tensor([[ 0.3273,  0.6742,  1.4397],
        [-1.2014, -0.2807,  0.1094],
        [-0.7194,  0.8177, -0.5809],
        [ 1.2515,  0.5807,  1.2761]], device='cuda:0'), tensor([[-0.0676, -0.2095, -2.4560],
        [-1.8055,  0.5198,  1.2424],
        [-1.6933,  0.1127,  0.1598],
        [-2.6395, -1.0274,  0.2296]], device='cuda:0'), tensor([[-1.0079,  0.3010, -0.2049],
        [-0.2963,  1.1941, -0.7664],
        [ 1.0457, -0.6663,  0.0405],
        [-0.0975,  0.2815,  1.5998]], device='cuda:0'), tensor([[ 2.4996,  0.5638, -1.5046],
        [ 2.0686,  0.9217,  1.6783],
        [-1.6940, -0.3129,  0.0719],
        [ 0.1603, -0.9724, -0.7393]], device='cuda:0'), tensor([[ 1.9289,  0.6337, -0.5543],
        [ 0.8780,  1.2082,  0.5716],
        [-1.3254,  1.5558,  1.6003],
        [-1.0849, -0.0165,  0.6126]], device='cuda:0'), tensor([[-1.2905, -0.9718, -0.5923],
        [-0.7282,  0.3135,  1.2061],
        [-0.0195, -0.0362,  1.3275],
        [-0.9153, -0.2002, -0.2004]], device='cuda:0')]
====================
schedule for this step is
[(4, 0), (3, 1)]
inputting microbatch 4 into partition 0
before moving to cuda:0: tensor([[ 2.8154, -0.9979,  1.6711],
        [-0.1764,  1.3011, -0.4159],
        [ 1.1769,  1.8420, -1.2100],
        [ 0.0242, -0.5054, -1.2763]], device='cuda:0')
after: tensor([[ 2.8154, -0.9979,  1.6711],
        [-0.1764,  1.3011, -0.4159],
        [ 1.1769,  1.8420, -1.2100],
        [ 0.0242, -0.5054, -1.2763]], device='cuda:0')
********************
observing microbatch 4
current batch shape is torch.Size([4, 3])
batch is tensor([[ 2.8154, -0.9979,  1.6711],
        [-0.1764,  1.3011, -0.4159],
        [ 1.1769,  1.8420, -1.2100],
        [ 0.0242, -0.5054, -1.2763]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 4
********************
inputting microbatch 3 into partition 1
before moving to cuda:1: tensor([[0.5284, 0.4806, 0.6497, 0.5682],
        [0.4474, 0.4228, 0.6157, 0.4860],
        [0.5088, 0.3451, 0.6201, 0.5536],
        [0.4547, 0.7602, 0.3788, 0.4084]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9733, 0.9731, 0.9751, 0.9665],
        [0.9682, 0.9732, 0.9731, 0.9751],
        [0.9665, 0.9681, 0.9730, 0.9729],
        [0.9749, 0.9663, 0.9679, 0.9731]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 3
current batch shape is torch.Size([4, 4])
batch is tensor([[0.9733, 0.9731, 0.9751, 0.9665],
        [0.9682, 0.9732, 0.9731, 0.9751],
        [0.9665, 0.9681, 0.9730, 0.9729],
        [0.9749, 0.9663, 0.9679, 0.9731]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 3
********************
receiving microbatch 4 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 4, tensor([[0.7467, 0.1412, 0.3344, 0.7419],
        [0.6785, 0.6858, 0.3314, 0.6260],
        [0.8285, 0.7635, 0.1318, 0.7340],
        [0.5590, 0.5808, 0.2603, 0.4945]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 4
result is tensor([[0.7467, 0.1412, 0.3344, 0.7419],
        [0.6785, 0.6858, 0.3314, 0.6260],
        [0.8285, 0.7635, 0.1318, 0.7340],
        [0.5590, 0.5808, 0.2603, 0.4945]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.7467, 0.1412, 0.3344, 0.7419],
        [0.6785, 0.6858, 0.3314, 0.6260],
        [0.8285, 0.7635, 0.1318, 0.7340],
        [0.5590, 0.5808, 0.2603, 0.4945]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 3 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 3, tensor([[0.9734, 0.9732, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9667, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 3
result is tensor([[0.9734, 0.9732, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9667, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9734, 0.9732, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9667, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9448, 0.9472, 0.9503, 0.9406, 0.9381],
        [0.9410, 0.9399, 0.9431, 0.9381, 0.9332],
        [0.9465, 0.9404, 0.9421, 0.9300, 0.9365],
        [0.9503, 0.9421, 0.9443, 0.9279, 0.9394]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9715, 0.9712, 0.9732, 0.9643, 0.9661],
        [0.9710, 0.9708, 0.9728, 0.9639, 0.9656],
        [0.9709, 0.9707, 0.9727, 0.9638, 0.9655],
        [0.9709, 0.9706, 0.9727, 0.9639, 0.9655]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9733, 0.9731, 0.9751, 0.9665, 0.9682],
        [0.9732, 0.9731, 0.9751, 0.9665, 0.9681],
        [0.9730, 0.9729, 0.9749, 0.9663, 0.9679],
        [0.9731, 0.9729, 0.9749, 0.9664, 0.9680]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9732, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9667, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.7467, 0.1412, 0.3344, 0.7419],
        [0.6785, 0.6858, 0.3314, 0.6260],
        [0.8285, 0.7635, 0.1318, 0.7340],
        [0.5590, 0.5808, 0.2603, 0.4945]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.8394, -0.7112, -1.2919],
        [-0.5520, -0.4516, -0.4546],
        [ 1.2171, -1.0212,  0.5491],
        [-0.4934, -0.4480,  0.1458]], device='cuda:0'), tensor([[-0.2548, -1.3461, -0.8878],
        [ 1.1111, -0.7347,  0.8447],
        [ 0.2399, -1.0283,  0.4258],
        [-0.8746,  0.9923,  0.2034]], device='cuda:0'), tensor([[ 0.7326, -0.6715,  1.7661],
        [-0.6693, -1.7338,  0.3703],
        [-1.8851,  0.6470, -0.7668],
        [ 1.7409,  0.6931, -1.5818]], device='cuda:0'), tensor([[-0.2513,  0.6103, -0.1242],
        [-0.1741, -0.4948,  0.3420],
        [-0.4873,  1.0306, -0.1387],
        [ 0.0624, -0.2377, -1.1928]], device='cuda:0'), tensor([[ 0.2147, -1.7384, -0.5185],
        [ 0.4374,  0.2843, -0.5562],
        [ 0.2163,  1.0030, -0.1813],
        [ 0.4046,  0.2900, -0.1435]], device='cuda:0'), tensor([[ 0.3273,  0.6742,  1.4397],
        [-1.2014, -0.2807,  0.1094],
        [-0.7194,  0.8177, -0.5809],
        [ 1.2515,  0.5807,  1.2761]], device='cuda:0'), tensor([[-0.0676, -0.2095, -2.4560],
        [-1.8055,  0.5198,  1.2424],
        [-1.6933,  0.1127,  0.1598],
        [-2.6395, -1.0274,  0.2296]], device='cuda:0'), tensor([[-1.0079,  0.3010, -0.2049],
        [-0.2963,  1.1941, -0.7664],
        [ 1.0457, -0.6663,  0.0405],
        [-0.0975,  0.2815,  1.5998]], device='cuda:0'), tensor([[ 2.4996,  0.5638, -1.5046],
        [ 2.0686,  0.9217,  1.6783],
        [-1.6940, -0.3129,  0.0719],
        [ 0.1603, -0.9724, -0.7393]], device='cuda:0'), tensor([[ 1.9289,  0.6337, -0.5543],
        [ 0.8780,  1.2082,  0.5716],
        [-1.3254,  1.5558,  1.6003],
        [-1.0849, -0.0165,  0.6126]], device='cuda:0'), tensor([[-1.2905, -0.9718, -0.5923],
        [-0.7282,  0.3135,  1.2061],
        [-0.0195, -0.0362,  1.3275],
        [-0.9153, -0.2002, -0.2004]], device='cuda:0')]
====================
schedule for this step is
[(5, 0), (4, 1)]
inputting microbatch 5 into partition 0
before moving to cuda:0: tensor([[-0.8394, -0.7112, -1.2919],
        [-0.5520, -0.4516, -0.4546],
        [ 1.2171, -1.0212,  0.5491],
        [-0.4934, -0.4480,  0.1458]], device='cuda:0')
after: tensor([[-0.8394, -0.7112, -1.2919],
        [-0.5520, -0.4516, -0.4546],
        [ 1.2171, -1.0212,  0.5491],
        [-0.4934, -0.4480,  0.1458]], device='cuda:0')
********************
observing microbatch 5
current batch shape is torch.Size([4, 3])
batch is tensor([[-0.8394, -0.7112, -1.2919],
        [-0.5520, -0.4516, -0.4546],
        [ 1.2171, -1.0212,  0.5491],
        [-0.4934, -0.4480,  0.1458]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 5
********************
inputting microbatch 4 into partition 1
before moving to cuda:1: tensor([[0.7467, 0.1412, 0.3344, 0.7419],
        [0.6785, 0.6858, 0.3314, 0.6260],
        [0.8285, 0.7635, 0.1318, 0.7340],
        [0.5590, 0.5808, 0.2603, 0.4945]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9734, 0.9732, 0.9753, 0.9666],
        [0.9683, 0.9734, 0.9733, 0.9753],
        [0.9667, 0.9683, 0.9732, 0.9731],
        [0.9751, 0.9666, 0.9681, 0.9733]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 4
current batch shape is torch.Size([4, 4])
batch is tensor([[0.9734, 0.9732, 0.9753, 0.9666],
        [0.9683, 0.9734, 0.9733, 0.9753],
        [0.9667, 0.9683, 0.9732, 0.9731],
        [0.9751, 0.9666, 0.9681, 0.9733]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 4
********************
receiving microbatch 5 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 5, tensor([[0.4480, 0.5965, 0.3539, 0.4112],
        [0.4891, 0.5146, 0.4241, 0.4744],
        [0.6113, 0.2696, 0.3698, 0.6021],
        [0.4864, 0.4407, 0.5007, 0.4957]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 5
result is tensor([[0.4480, 0.5965, 0.3539, 0.4112],
        [0.4891, 0.5146, 0.4241, 0.4744],
        [0.6113, 0.2696, 0.3698, 0.6021],
        [0.4864, 0.4407, 0.5007, 0.4957]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4480, 0.5965, 0.3539, 0.4112],
        [0.4891, 0.5146, 0.4241, 0.4744],
        [0.6113, 0.2696, 0.3698, 0.6021],
        [0.4864, 0.4407, 0.5007, 0.4957]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 4 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 4, tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 4
result is tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9448, 0.9472, 0.9503, 0.9406, 0.9381],
        [0.9410, 0.9399, 0.9431, 0.9381, 0.9332],
        [0.9465, 0.9404, 0.9421, 0.9300, 0.9365],
        [0.9503, 0.9421, 0.9443, 0.9279, 0.9394]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9715, 0.9712, 0.9732, 0.9643, 0.9661],
        [0.9710, 0.9708, 0.9728, 0.9639, 0.9656],
        [0.9709, 0.9707, 0.9727, 0.9638, 0.9655],
        [0.9709, 0.9706, 0.9727, 0.9639, 0.9655]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9733, 0.9731, 0.9751, 0.9665, 0.9682],
        [0.9732, 0.9731, 0.9751, 0.9665, 0.9681],
        [0.9730, 0.9729, 0.9749, 0.9663, 0.9679],
        [0.9731, 0.9729, 0.9749, 0.9664, 0.9680]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9732, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9667, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4480, 0.5965, 0.3539, 0.4112],
        [0.4891, 0.5146, 0.4241, 0.4744],
        [0.6113, 0.2696, 0.3698, 0.6021],
        [0.4864, 0.4407, 0.5007, 0.4957]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.2548, -1.3461, -0.8878],
        [ 1.1111, -0.7347,  0.8447],
        [ 0.2399, -1.0283,  0.4258],
        [-0.8746,  0.9923,  0.2034]], device='cuda:0'), tensor([[ 0.7326, -0.6715,  1.7661],
        [-0.6693, -1.7338,  0.3703],
        [-1.8851,  0.6470, -0.7668],
        [ 1.7409,  0.6931, -1.5818]], device='cuda:0'), tensor([[-0.2513,  0.6103, -0.1242],
        [-0.1741, -0.4948,  0.3420],
        [-0.4873,  1.0306, -0.1387],
        [ 0.0624, -0.2377, -1.1928]], device='cuda:0'), tensor([[ 0.2147, -1.7384, -0.5185],
        [ 0.4374,  0.2843, -0.5562],
        [ 0.2163,  1.0030, -0.1813],
        [ 0.4046,  0.2900, -0.1435]], device='cuda:0'), tensor([[ 0.3273,  0.6742,  1.4397],
        [-1.2014, -0.2807,  0.1094],
        [-0.7194,  0.8177, -0.5809],
        [ 1.2515,  0.5807,  1.2761]], device='cuda:0'), tensor([[-0.0676, -0.2095, -2.4560],
        [-1.8055,  0.5198,  1.2424],
        [-1.6933,  0.1127,  0.1598],
        [-2.6395, -1.0274,  0.2296]], device='cuda:0'), tensor([[-1.0079,  0.3010, -0.2049],
        [-0.2963,  1.1941, -0.7664],
        [ 1.0457, -0.6663,  0.0405],
        [-0.0975,  0.2815,  1.5998]], device='cuda:0'), tensor([[ 2.4996,  0.5638, -1.5046],
        [ 2.0686,  0.9217,  1.6783],
        [-1.6940, -0.3129,  0.0719],
        [ 0.1603, -0.9724, -0.7393]], device='cuda:0'), tensor([[ 1.9289,  0.6337, -0.5543],
        [ 0.8780,  1.2082,  0.5716],
        [-1.3254,  1.5558,  1.6003],
        [-1.0849, -0.0165,  0.6126]], device='cuda:0'), tensor([[-1.2905, -0.9718, -0.5923],
        [-0.7282,  0.3135,  1.2061],
        [-0.0195, -0.0362,  1.3275],
        [-0.9153, -0.2002, -0.2004]], device='cuda:0')]
====================
schedule for this step is
[(6, 0), (5, 1)]
inputting microbatch 6 into partition 0
before moving to cuda:0: tensor([[-0.2548, -1.3461, -0.8878],
        [ 1.1111, -0.7347,  0.8447],
        [ 0.2399, -1.0283,  0.4258],
        [-0.8746,  0.9923,  0.2034]], device='cuda:0')
after: tensor([[-0.2548, -1.3461, -0.8878],
        [ 1.1111, -0.7347,  0.8447],
        [ 0.2399, -1.0283,  0.4258],
        [-0.8746,  0.9923,  0.2034]], device='cuda:0')
********************
observing microbatch 6
current batch shape is torch.Size([4, 3])
batch is tensor([[-0.2548, -1.3461, -0.8878],
        [ 1.1111, -0.7347,  0.8447],
        [ 0.2399, -1.0283,  0.4258],
        [-0.8746,  0.9923,  0.2034]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 6
********************
inputting microbatch 5 into partition 1
before moving to cuda:1: tensor([[0.4480, 0.5965, 0.3539, 0.4112],
        [0.4891, 0.5146, 0.4241, 0.4744],
        [0.6113, 0.2696, 0.3698, 0.6021],
        [0.4864, 0.4407, 0.5007, 0.4957]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[ 3.6893e+19,  1.8684e+00, -0.0000e+00,  1.8683e+00],
        [-0.0000e+00,  1.8688e+00, -3.6893e+19,  1.8667e+00],
        [ 2.0000e+00,  1.8671e+00, -0.0000e+00,  1.8683e+00],
        [-2.0000e+00,  1.8683e+00,  2.0000e+00,  1.8688e+00]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 5
current batch shape is torch.Size([4, 4])
batch is tensor([[ 3.6893e+19,  1.8684e+00, -0.0000e+00,  1.8683e+00],
        [-0.0000e+00,  1.8688e+00, -3.6893e+19,  1.8667e+00],
        [ 2.0000e+00,  1.8671e+00, -0.0000e+00,  1.8683e+00],
        [-2.0000e+00,  1.8683e+00,  2.0000e+00,  1.8688e+00]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 5
********************
receiving microbatch 6 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 6, tensor([[0.4477, 0.4506, 0.3597, 0.4241],
        [0.6205, 0.2713, 0.4127, 0.6208],
        [0.5090, 0.3167, 0.4695, 0.5213],
        [0.5734, 0.6140, 0.5077, 0.5675]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 6
result is tensor([[0.4477, 0.4506, 0.3597, 0.4241],
        [0.6205, 0.2713, 0.4127, 0.6208],
        [0.5090, 0.3167, 0.4695, 0.5213],
        [0.5734, 0.6140, 0.5077, 0.5675]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4477, 0.4506, 0.3597, 0.4241],
        [0.6205, 0.2713, 0.4127, 0.6208],
        [0.5090, 0.3167, 0.4695, 0.5213],
        [0.5734, 0.6140, 0.5077, 0.5675]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 5 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 5, tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.9932, 0.9920, 0.9923, 0.9907, 0.9911],
        [0.9375, 0.9843, 0.9779, 0.9714, 0.9572]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 5
result is tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.9932, 0.9920, 0.9923, 0.9907, 0.9911],
        [0.9375, 0.9843, 0.9779, 0.9714, 0.9572]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.9932, 0.9920, 0.9923, 0.9907, 0.9911],
        [0.9375, 0.9843, 0.9779, 0.9714, 0.9572]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9448, 0.9472, 0.9503, 0.9406, 0.9381],
        [0.9410, 0.9399, 0.9431, 0.9381, 0.9332],
        [0.9465, 0.9404, 0.9421, 0.9300, 0.9365],
        [0.9503, 0.9421, 0.9443, 0.9279, 0.9394]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9715, 0.9712, 0.9732, 0.9643, 0.9661],
        [0.9710, 0.9708, 0.9728, 0.9639, 0.9656],
        [0.9709, 0.9707, 0.9727, 0.9638, 0.9655],
        [0.9709, 0.9706, 0.9727, 0.9639, 0.9655]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9733, 0.9731, 0.9751, 0.9665, 0.9682],
        [0.9732, 0.9731, 0.9751, 0.9665, 0.9681],
        [0.9730, 0.9729, 0.9749, 0.9663, 0.9679],
        [0.9731, 0.9729, 0.9749, 0.9664, 0.9680]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9732, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9667, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.9932, 0.9920, 0.9923, 0.9907, 0.9911],
        [0.9375, 0.9843, 0.9779, 0.9714, 0.9572]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4477, 0.4506, 0.3597, 0.4241],
        [0.6205, 0.2713, 0.4127, 0.6208],
        [0.5090, 0.3167, 0.4695, 0.5213],
        [0.5734, 0.6140, 0.5077, 0.5675]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.7326, -0.6715,  1.7661],
        [-0.6693, -1.7338,  0.3703],
        [-1.8851,  0.6470, -0.7668],
        [ 1.7409,  0.6931, -1.5818]], device='cuda:0'), tensor([[-0.2513,  0.6103, -0.1242],
        [-0.1741, -0.4948,  0.3420],
        [-0.4873,  1.0306, -0.1387],
        [ 0.0624, -0.2377, -1.1928]], device='cuda:0'), tensor([[ 0.2147, -1.7384, -0.5185],
        [ 0.4374,  0.2843, -0.5562],
        [ 0.2163,  1.0030, -0.1813],
        [ 0.4046,  0.2900, -0.1435]], device='cuda:0'), tensor([[ 0.3273,  0.6742,  1.4397],
        [-1.2014, -0.2807,  0.1094],
        [-0.7194,  0.8177, -0.5809],
        [ 1.2515,  0.5807,  1.2761]], device='cuda:0'), tensor([[-0.0676, -0.2095, -2.4560],
        [-1.8055,  0.5198,  1.2424],
        [-1.6933,  0.1127,  0.1598],
        [-2.6395, -1.0274,  0.2296]], device='cuda:0'), tensor([[-1.0079,  0.3010, -0.2049],
        [-0.2963,  1.1941, -0.7664],
        [ 1.0457, -0.6663,  0.0405],
        [-0.0975,  0.2815,  1.5998]], device='cuda:0'), tensor([[ 2.4996,  0.5638, -1.5046],
        [ 2.0686,  0.9217,  1.6783],
        [-1.6940, -0.3129,  0.0719],
        [ 0.1603, -0.9724, -0.7393]], device='cuda:0'), tensor([[ 1.9289,  0.6337, -0.5543],
        [ 0.8780,  1.2082,  0.5716],
        [-1.3254,  1.5558,  1.6003],
        [-1.0849, -0.0165,  0.6126]], device='cuda:0'), tensor([[-1.2905, -0.9718, -0.5923],
        [-0.7282,  0.3135,  1.2061],
        [-0.0195, -0.0362,  1.3275],
        [-0.9153, -0.2002, -0.2004]], device='cuda:0')]
====================
schedule for this step is
[(7, 0), (6, 1)]
inputting microbatch 7 into partition 0
before moving to cuda:0: tensor([[ 0.7326, -0.6715,  1.7661],
        [-0.6693, -1.7338,  0.3703],
        [-1.8851,  0.6470, -0.7668],
        [ 1.7409,  0.6931, -1.5818]], device='cuda:0')
after: tensor([[ 0.7326, -0.6715,  1.7661],
        [-0.6693, -1.7338,  0.3703],
        [-1.8851,  0.6470, -0.7668],
        [ 1.7409,  0.6931, -1.5818]], device='cuda:0')
********************
observing microbatch 7
current batch shape is torch.Size([4, 3])
batch is tensor([[ 0.7326, -0.6715,  1.7661],
        [-0.6693, -1.7338,  0.3703],
        [-1.8851,  0.6470, -0.7668],
        [ 1.7409,  0.6931, -1.5818]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 7
********************
inputting microbatch 6 into partition 1
before moving to cuda:1: tensor([[0.4477, 0.4506, 0.3597, 0.4241],
        [0.6205, 0.2713, 0.4127, 0.6208],
        [0.5090, 0.3167, 0.4695, 0.5213],
        [0.5734, 0.6140, 0.5077, 0.5675]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[1.0000, 1.0000, 1.0000, 1.0000],
        [1.0000, 0.9932, 0.9920, 0.9923],
        [0.9907, 0.9911, 0.9375, 0.9843],
        [0.9779, 0.9714, 0.9572, 0.9375]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 6
current batch shape is torch.Size([4, 4])
batch is tensor([[1.0000, 1.0000, 1.0000, 1.0000],
        [1.0000, 0.9932, 0.9920, 0.9923],
        [0.9907, 0.9911, 0.9375, 0.9843],
        [0.9779, 0.9714, 0.9572, 0.9375]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 6
********************
receiving microbatch 7 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 7, tensor([[0.5728, 0.2085, 0.5850, 0.6200],
        [0.3528, 0.2876, 0.5955, 0.3987],
        [0.4492, 0.7208, 0.5065, 0.4365],
        [0.8067, 0.6731, 0.0981, 0.6969]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 7
result is tensor([[0.5728, 0.2085, 0.5850, 0.6200],
        [0.3528, 0.2876, 0.5955, 0.3987],
        [0.4492, 0.7208, 0.5065, 0.4365],
        [0.8067, 0.6731, 0.0981, 0.6969]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5728, 0.2085, 0.5850, 0.6200],
        [0.3528, 0.2876, 0.5955, 0.3987],
        [0.4492, 0.7208, 0.5065, 0.4365],
        [0.8067, 0.6731, 0.0981, 0.6969]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 6 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 6, tensor([[0.9753, 0.9753, 0.9772, 0.9690, 0.9705],
        [0.9750, 0.9748, 0.9768, 0.9685, 0.9701],
        [0.9738, 0.9735, 0.9754, 0.9670, 0.9687],
        [0.9729, 0.9724, 0.9744, 0.9655, 0.9676]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 6
result is tensor([[0.9753, 0.9753, 0.9772, 0.9690, 0.9705],
        [0.9750, 0.9748, 0.9768, 0.9685, 0.9701],
        [0.9738, 0.9735, 0.9754, 0.9670, 0.9687],
        [0.9729, 0.9724, 0.9744, 0.9655, 0.9676]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9753, 0.9753, 0.9772, 0.9690, 0.9705],
        [0.9750, 0.9748, 0.9768, 0.9685, 0.9701],
        [0.9738, 0.9735, 0.9754, 0.9670, 0.9687],
        [0.9729, 0.9724, 0.9744, 0.9655, 0.9676]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9448, 0.9472, 0.9503, 0.9406, 0.9381],
        [0.9410, 0.9399, 0.9431, 0.9381, 0.9332],
        [0.9465, 0.9404, 0.9421, 0.9300, 0.9365],
        [0.9503, 0.9421, 0.9443, 0.9279, 0.9394]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9715, 0.9712, 0.9732, 0.9643, 0.9661],
        [0.9710, 0.9708, 0.9728, 0.9639, 0.9656],
        [0.9709, 0.9707, 0.9727, 0.9638, 0.9655],
        [0.9709, 0.9706, 0.9727, 0.9639, 0.9655]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9733, 0.9731, 0.9751, 0.9665, 0.9682],
        [0.9732, 0.9731, 0.9751, 0.9665, 0.9681],
        [0.9730, 0.9729, 0.9749, 0.9663, 0.9679],
        [0.9731, 0.9729, 0.9749, 0.9664, 0.9680]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9732, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9667, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.9932, 0.9920, 0.9923, 0.9907, 0.9911],
        [0.9375, 0.9843, 0.9779, 0.9714, 0.9572]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9753, 0.9753, 0.9772, 0.9690, 0.9705],
        [0.9750, 0.9748, 0.9768, 0.9685, 0.9701],
        [0.9738, 0.9735, 0.9754, 0.9670, 0.9687],
        [0.9729, 0.9724, 0.9744, 0.9655, 0.9676]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5728, 0.2085, 0.5850, 0.6200],
        [0.3528, 0.2876, 0.5955, 0.3987],
        [0.4492, 0.7208, 0.5065, 0.4365],
        [0.8067, 0.6731, 0.0981, 0.6969]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.2513,  0.6103, -0.1242],
        [-0.1741, -0.4948,  0.3420],
        [-0.4873,  1.0306, -0.1387],
        [ 0.0624, -0.2377, -1.1928]], device='cuda:0'), tensor([[ 0.2147, -1.7384, -0.5185],
        [ 0.4374,  0.2843, -0.5562],
        [ 0.2163,  1.0030, -0.1813],
        [ 0.4046,  0.2900, -0.1435]], device='cuda:0'), tensor([[ 0.3273,  0.6742,  1.4397],
        [-1.2014, -0.2807,  0.1094],
        [-0.7194,  0.8177, -0.5809],
        [ 1.2515,  0.5807,  1.2761]], device='cuda:0'), tensor([[-0.0676, -0.2095, -2.4560],
        [-1.8055,  0.5198,  1.2424],
        [-1.6933,  0.1127,  0.1598],
        [-2.6395, -1.0274,  0.2296]], device='cuda:0'), tensor([[-1.0079,  0.3010, -0.2049],
        [-0.2963,  1.1941, -0.7664],
        [ 1.0457, -0.6663,  0.0405],
        [-0.0975,  0.2815,  1.5998]], device='cuda:0'), tensor([[ 2.4996,  0.5638, -1.5046],
        [ 2.0686,  0.9217,  1.6783],
        [-1.6940, -0.3129,  0.0719],
        [ 0.1603, -0.9724, -0.7393]], device='cuda:0'), tensor([[ 1.9289,  0.6337, -0.5543],
        [ 0.8780,  1.2082,  0.5716],
        [-1.3254,  1.5558,  1.6003],
        [-1.0849, -0.0165,  0.6126]], device='cuda:0'), tensor([[-1.2905, -0.9718, -0.5923],
        [-0.7282,  0.3135,  1.2061],
        [-0.0195, -0.0362,  1.3275],
        [-0.9153, -0.2002, -0.2004]], device='cuda:0')]
====================
schedule for this step is
[(8, 0), (7, 1)]
inputting microbatch 8 into partition 0
before moving to cuda:0: tensor([[-0.2513,  0.6103, -0.1242],
        [-0.1741, -0.4948,  0.3420],
        [-0.4873,  1.0306, -0.1387],
        [ 0.0624, -0.2377, -1.1928]], device='cuda:0')
after: tensor([[-0.2513,  0.6103, -0.1242],
        [-0.1741, -0.4948,  0.3420],
        [-0.4873,  1.0306, -0.1387],
        [ 0.0624, -0.2377, -1.1928]], device='cuda:0')
********************
observing microbatch 8
current batch shape is torch.Size([4, 3])
batch is tensor([[-0.2513,  0.6103, -0.1242],
        [-0.1741, -0.4948,  0.3420],
        [-0.4873,  1.0306, -0.1387],
        [ 0.0624, -0.2377, -1.1928]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 8
********************
inputting microbatch 7 into partition 1
before moving to cuda:1: tensor([[0.5728, 0.2085, 0.5850, 0.6200],
        [0.3528, 0.2876, 0.5955, 0.3987],
        [0.4492, 0.7208, 0.5065, 0.4365],
        [0.8067, 0.6731, 0.0981, 0.6969]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9753, 0.9753, 0.9772, 0.9690],
        [0.9705, 0.9750, 0.9748, 0.9768],
        [0.9685, 0.9701, 0.9738, 0.9735],
        [0.9754, 0.9670, 0.9687, 0.9729]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 7
current batch shape is torch.Size([4, 4])
batch is tensor([[0.9753, 0.9753, 0.9772, 0.9690],
        [0.9705, 0.9750, 0.9748, 0.9768],
        [0.9685, 0.9701, 0.9738, 0.9735],
        [0.9754, 0.9670, 0.9687, 0.9729]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 7
********************
receiving microbatch 8 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 8, tensor([[0.6101, 0.5832, 0.3992, 0.5814],
        [0.5136, 0.3989, 0.4907, 0.5233],
        [0.6220, 0.6406, 0.4117, 0.5913],
        [0.5854, 0.5993, 0.2589, 0.5186]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 8
result is tensor([[0.6101, 0.5832, 0.3992, 0.5814],
        [0.5136, 0.3989, 0.4907, 0.5233],
        [0.6220, 0.6406, 0.4117, 0.5913],
        [0.5854, 0.5993, 0.2589, 0.5186]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6101, 0.5832, 0.3992, 0.5814],
        [0.5136, 0.3989, 0.4907, 0.5233],
        [0.6220, 0.6406, 0.4117, 0.5913],
        [0.5854, 0.5993, 0.2589, 0.5186]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 7 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 7, tensor([[0.9736, 0.9734, 0.9754, 0.9668, 0.9685],
        [0.9735, 0.9734, 0.9754, 0.9669, 0.9685],
        [0.9733, 0.9732, 0.9752, 0.9667, 0.9682],
        [0.9733, 0.9731, 0.9752, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 7
result is tensor([[0.9736, 0.9734, 0.9754, 0.9668, 0.9685],
        [0.9735, 0.9734, 0.9754, 0.9669, 0.9685],
        [0.9733, 0.9732, 0.9752, 0.9667, 0.9682],
        [0.9733, 0.9731, 0.9752, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9736, 0.9734, 0.9754, 0.9668, 0.9685],
        [0.9735, 0.9734, 0.9754, 0.9669, 0.9685],
        [0.9733, 0.9732, 0.9752, 0.9667, 0.9682],
        [0.9733, 0.9731, 0.9752, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9448, 0.9472, 0.9503, 0.9406, 0.9381],
        [0.9410, 0.9399, 0.9431, 0.9381, 0.9332],
        [0.9465, 0.9404, 0.9421, 0.9300, 0.9365],
        [0.9503, 0.9421, 0.9443, 0.9279, 0.9394]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9715, 0.9712, 0.9732, 0.9643, 0.9661],
        [0.9710, 0.9708, 0.9728, 0.9639, 0.9656],
        [0.9709, 0.9707, 0.9727, 0.9638, 0.9655],
        [0.9709, 0.9706, 0.9727, 0.9639, 0.9655]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9733, 0.9731, 0.9751, 0.9665, 0.9682],
        [0.9732, 0.9731, 0.9751, 0.9665, 0.9681],
        [0.9730, 0.9729, 0.9749, 0.9663, 0.9679],
        [0.9731, 0.9729, 0.9749, 0.9664, 0.9680]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9732, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9667, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.9932, 0.9920, 0.9923, 0.9907, 0.9911],
        [0.9375, 0.9843, 0.9779, 0.9714, 0.9572]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9753, 0.9753, 0.9772, 0.9690, 0.9705],
        [0.9750, 0.9748, 0.9768, 0.9685, 0.9701],
        [0.9738, 0.9735, 0.9754, 0.9670, 0.9687],
        [0.9729, 0.9724, 0.9744, 0.9655, 0.9676]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9736, 0.9734, 0.9754, 0.9668, 0.9685],
        [0.9735, 0.9734, 0.9754, 0.9669, 0.9685],
        [0.9733, 0.9732, 0.9752, 0.9667, 0.9682],
        [0.9733, 0.9731, 0.9752, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6101, 0.5832, 0.3992, 0.5814],
        [0.5136, 0.3989, 0.4907, 0.5233],
        [0.6220, 0.6406, 0.4117, 0.5913],
        [0.5854, 0.5993, 0.2589, 0.5186]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.2147, -1.7384, -0.5185],
        [ 0.4374,  0.2843, -0.5562],
        [ 0.2163,  1.0030, -0.1813],
        [ 0.4046,  0.2900, -0.1435]], device='cuda:0'), tensor([[ 0.3273,  0.6742,  1.4397],
        [-1.2014, -0.2807,  0.1094],
        [-0.7194,  0.8177, -0.5809],
        [ 1.2515,  0.5807,  1.2761]], device='cuda:0'), tensor([[-0.0676, -0.2095, -2.4560],
        [-1.8055,  0.5198,  1.2424],
        [-1.6933,  0.1127,  0.1598],
        [-2.6395, -1.0274,  0.2296]], device='cuda:0'), tensor([[-1.0079,  0.3010, -0.2049],
        [-0.2963,  1.1941, -0.7664],
        [ 1.0457, -0.6663,  0.0405],
        [-0.0975,  0.2815,  1.5998]], device='cuda:0'), tensor([[ 2.4996,  0.5638, -1.5046],
        [ 2.0686,  0.9217,  1.6783],
        [-1.6940, -0.3129,  0.0719],
        [ 0.1603, -0.9724, -0.7393]], device='cuda:0'), tensor([[ 1.9289,  0.6337, -0.5543],
        [ 0.8780,  1.2082,  0.5716],
        [-1.3254,  1.5558,  1.6003],
        [-1.0849, -0.0165,  0.6126]], device='cuda:0'), tensor([[-1.2905, -0.9718, -0.5923],
        [-0.7282,  0.3135,  1.2061],
        [-0.0195, -0.0362,  1.3275],
        [-0.9153, -0.2002, -0.2004]], device='cuda:0')]
====================
schedule for this step is
[(9, 0), (8, 1)]
inputting microbatch 9 into partition 0
before moving to cuda:0: tensor([[ 0.2147, -1.7384, -0.5185],
        [ 0.4374,  0.2843, -0.5562],
        [ 0.2163,  1.0030, -0.1813],
        [ 0.4046,  0.2900, -0.1435]], device='cuda:0')
after: tensor([[ 0.2147, -1.7384, -0.5185],
        [ 0.4374,  0.2843, -0.5562],
        [ 0.2163,  1.0030, -0.1813],
        [ 0.4046,  0.2900, -0.1435]], device='cuda:0')
********************
observing microbatch 9
current batch shape is torch.Size([4, 3])
batch is tensor([[ 0.2147, -1.7384, -0.5185],
        [ 0.4374,  0.2843, -0.5562],
        [ 0.2163,  1.0030, -0.1813],
        [ 0.4046,  0.2900, -0.1435]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 9
********************
inputting microbatch 8 into partition 1
before moving to cuda:1: tensor([[0.6101, 0.5832, 0.3992, 0.5814],
        [0.5136, 0.3989, 0.4907, 0.5233],
        [0.6220, 0.6406, 0.4117, 0.5913],
        [0.5854, 0.5993, 0.2589, 0.5186]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9736, 0.9734, 0.9754, 0.9668],
        [0.9685, 0.9735, 0.9734, 0.9754],
        [0.9669, 0.9685, 0.9733, 0.9732],
        [0.9752, 0.9667, 0.9682, 0.9733]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 8
current batch shape is torch.Size([4, 4])
batch is tensor([[0.9736, 0.9734, 0.9754, 0.9668],
        [0.9685, 0.9735, 0.9734, 0.9754],
        [0.9669, 0.9685, 0.9733, 0.9732],
        [0.9752, 0.9667, 0.9682, 0.9733]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 8
********************
receiving microbatch 9 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 9, tensor([[0.4572, 0.3456, 0.3666, 0.4439],
        [0.6581, 0.5675, 0.2796, 0.6013],
        [0.6888, 0.6134, 0.3268, 0.6417],
        [0.6495, 0.5205, 0.3321, 0.6099]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 9
result is tensor([[0.4572, 0.3456, 0.3666, 0.4439],
        [0.6581, 0.5675, 0.2796, 0.6013],
        [0.6888, 0.6134, 0.3268, 0.6417],
        [0.6495, 0.5205, 0.3321, 0.6099]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4572, 0.3456, 0.3666, 0.4439],
        [0.6581, 0.5675, 0.2796, 0.6013],
        [0.6888, 0.6134, 0.3268, 0.6417],
        [0.6495, 0.5205, 0.3321, 0.6099]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 8 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 8, tensor([[0.9735, 0.9733, 0.9753, 0.9666, 0.9684],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9682],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 8
result is tensor([[0.9735, 0.9733, 0.9753, 0.9666, 0.9684],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9682],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9735, 0.9733, 0.9753, 0.9666, 0.9684],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9682],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9448, 0.9472, 0.9503, 0.9406, 0.9381],
        [0.9410, 0.9399, 0.9431, 0.9381, 0.9332],
        [0.9465, 0.9404, 0.9421, 0.9300, 0.9365],
        [0.9503, 0.9421, 0.9443, 0.9279, 0.9394]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9715, 0.9712, 0.9732, 0.9643, 0.9661],
        [0.9710, 0.9708, 0.9728, 0.9639, 0.9656],
        [0.9709, 0.9707, 0.9727, 0.9638, 0.9655],
        [0.9709, 0.9706, 0.9727, 0.9639, 0.9655]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9733, 0.9731, 0.9751, 0.9665, 0.9682],
        [0.9732, 0.9731, 0.9751, 0.9665, 0.9681],
        [0.9730, 0.9729, 0.9749, 0.9663, 0.9679],
        [0.9731, 0.9729, 0.9749, 0.9664, 0.9680]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9732, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9667, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.9932, 0.9920, 0.9923, 0.9907, 0.9911],
        [0.9375, 0.9843, 0.9779, 0.9714, 0.9572]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9753, 0.9753, 0.9772, 0.9690, 0.9705],
        [0.9750, 0.9748, 0.9768, 0.9685, 0.9701],
        [0.9738, 0.9735, 0.9754, 0.9670, 0.9687],
        [0.9729, 0.9724, 0.9744, 0.9655, 0.9676]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9736, 0.9734, 0.9754, 0.9668, 0.9685],
        [0.9735, 0.9734, 0.9754, 0.9669, 0.9685],
        [0.9733, 0.9732, 0.9752, 0.9667, 0.9682],
        [0.9733, 0.9731, 0.9752, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9735, 0.9733, 0.9753, 0.9666, 0.9684],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9682],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4572, 0.3456, 0.3666, 0.4439],
        [0.6581, 0.5675, 0.2796, 0.6013],
        [0.6888, 0.6134, 0.3268, 0.6417],
        [0.6495, 0.5205, 0.3321, 0.6099]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.3273,  0.6742,  1.4397],
        [-1.2014, -0.2807,  0.1094],
        [-0.7194,  0.8177, -0.5809],
        [ 1.2515,  0.5807,  1.2761]], device='cuda:0'), tensor([[-0.0676, -0.2095, -2.4560],
        [-1.8055,  0.5198,  1.2424],
        [-1.6933,  0.1127,  0.1598],
        [-2.6395, -1.0274,  0.2296]], device='cuda:0'), tensor([[-1.0079,  0.3010, -0.2049],
        [-0.2963,  1.1941, -0.7664],
        [ 1.0457, -0.6663,  0.0405],
        [-0.0975,  0.2815,  1.5998]], device='cuda:0'), tensor([[ 2.4996,  0.5638, -1.5046],
        [ 2.0686,  0.9217,  1.6783],
        [-1.6940, -0.3129,  0.0719],
        [ 0.1603, -0.9724, -0.7393]], device='cuda:0'), tensor([[ 1.9289,  0.6337, -0.5543],
        [ 0.8780,  1.2082,  0.5716],
        [-1.3254,  1.5558,  1.6003],
        [-1.0849, -0.0165,  0.6126]], device='cuda:0'), tensor([[-1.2905, -0.9718, -0.5923],
        [-0.7282,  0.3135,  1.2061],
        [-0.0195, -0.0362,  1.3275],
        [-0.9153, -0.2002, -0.2004]], device='cuda:0')]
====================
schedule for this step is
[(10, 0), (9, 1)]
inputting microbatch 10 into partition 0
before moving to cuda:0: tensor([[ 0.3273,  0.6742,  1.4397],
        [-1.2014, -0.2807,  0.1094],
        [-0.7194,  0.8177, -0.5809],
        [ 1.2515,  0.5807,  1.2761]], device='cuda:0')
after: tensor([[ 0.3273,  0.6742,  1.4397],
        [-1.2014, -0.2807,  0.1094],
        [-0.7194,  0.8177, -0.5809],
        [ 1.2515,  0.5807,  1.2761]], device='cuda:0')
********************
observing microbatch 10
current batch shape is torch.Size([4, 3])
batch is tensor([[ 0.3273,  0.6742,  1.4397],
        [-1.2014, -0.2807,  0.1094],
        [-0.7194,  0.8177, -0.5809],
        [ 1.2515,  0.5807,  1.2761]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 10
********************
inputting microbatch 9 into partition 1
before moving to cuda:1: tensor([[0.4572, 0.3456, 0.3666, 0.4439],
        [0.6581, 0.5675, 0.2796, 0.6013],
        [0.6888, 0.6134, 0.3268, 0.6417],
        [0.6495, 0.5205, 0.3321, 0.6099]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9735, 0.9733, 0.9753, 0.9666],
        [0.9684, 0.9734, 0.9733, 0.9753],
        [0.9668, 0.9683, 0.9732, 0.9731],
        [0.9751, 0.9666, 0.9682, 0.9733]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 9
current batch shape is torch.Size([4, 4])
batch is tensor([[0.9735, 0.9733, 0.9753, 0.9666],
        [0.9684, 0.9734, 0.9733, 0.9753],
        [0.9668, 0.9683, 0.9732, 0.9731],
        [0.9751, 0.9666, 0.9682, 0.9733]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 9
********************
receiving microbatch 10 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 10, tensor([[0.6509, 0.3807, 0.5448, 0.6687],
        [0.4262, 0.4952, 0.5763, 0.4498],
        [0.5864, 0.6754, 0.3861, 0.5466],
        [0.7308, 0.3512, 0.4125, 0.7221]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 10
result is tensor([[0.6509, 0.3807, 0.5448, 0.6687],
        [0.4262, 0.4952, 0.5763, 0.4498],
        [0.5864, 0.6754, 0.3861, 0.5466],
        [0.7308, 0.3512, 0.4125, 0.7221]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6509, 0.3807, 0.5448, 0.6687],
        [0.4262, 0.4952, 0.5763, 0.4498],
        [0.5864, 0.6754, 0.3861, 0.5466],
        [0.7308, 0.3512, 0.4125, 0.7221]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 9 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 9, tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 9
result is tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9448, 0.9472, 0.9503, 0.9406, 0.9381],
        [0.9410, 0.9399, 0.9431, 0.9381, 0.9332],
        [0.9465, 0.9404, 0.9421, 0.9300, 0.9365],
        [0.9503, 0.9421, 0.9443, 0.9279, 0.9394]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9715, 0.9712, 0.9732, 0.9643, 0.9661],
        [0.9710, 0.9708, 0.9728, 0.9639, 0.9656],
        [0.9709, 0.9707, 0.9727, 0.9638, 0.9655],
        [0.9709, 0.9706, 0.9727, 0.9639, 0.9655]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9733, 0.9731, 0.9751, 0.9665, 0.9682],
        [0.9732, 0.9731, 0.9751, 0.9665, 0.9681],
        [0.9730, 0.9729, 0.9749, 0.9663, 0.9679],
        [0.9731, 0.9729, 0.9749, 0.9664, 0.9680]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9732, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9667, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.9932, 0.9920, 0.9923, 0.9907, 0.9911],
        [0.9375, 0.9843, 0.9779, 0.9714, 0.9572]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9753, 0.9753, 0.9772, 0.9690, 0.9705],
        [0.9750, 0.9748, 0.9768, 0.9685, 0.9701],
        [0.9738, 0.9735, 0.9754, 0.9670, 0.9687],
        [0.9729, 0.9724, 0.9744, 0.9655, 0.9676]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9736, 0.9734, 0.9754, 0.9668, 0.9685],
        [0.9735, 0.9734, 0.9754, 0.9669, 0.9685],
        [0.9733, 0.9732, 0.9752, 0.9667, 0.9682],
        [0.9733, 0.9731, 0.9752, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9735, 0.9733, 0.9753, 0.9666, 0.9684],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9682],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6509, 0.3807, 0.5448, 0.6687],
        [0.4262, 0.4952, 0.5763, 0.4498],
        [0.5864, 0.6754, 0.3861, 0.5466],
        [0.7308, 0.3512, 0.4125, 0.7221]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.0676, -0.2095, -2.4560],
        [-1.8055,  0.5198,  1.2424],
        [-1.6933,  0.1127,  0.1598],
        [-2.6395, -1.0274,  0.2296]], device='cuda:0'), tensor([[-1.0079,  0.3010, -0.2049],
        [-0.2963,  1.1941, -0.7664],
        [ 1.0457, -0.6663,  0.0405],
        [-0.0975,  0.2815,  1.5998]], device='cuda:0'), tensor([[ 2.4996,  0.5638, -1.5046],
        [ 2.0686,  0.9217,  1.6783],
        [-1.6940, -0.3129,  0.0719],
        [ 0.1603, -0.9724, -0.7393]], device='cuda:0'), tensor([[ 1.9289,  0.6337, -0.5543],
        [ 0.8780,  1.2082,  0.5716],
        [-1.3254,  1.5558,  1.6003],
        [-1.0849, -0.0165,  0.6126]], device='cuda:0'), tensor([[-1.2905, -0.9718, -0.5923],
        [-0.7282,  0.3135,  1.2061],
        [-0.0195, -0.0362,  1.3275],
        [-0.9153, -0.2002, -0.2004]], device='cuda:0')]
====================
schedule for this step is
[(11, 0), (10, 1)]
inputting microbatch 11 into partition 0
before moving to cuda:0: tensor([[-0.0676, -0.2095, -2.4560],
        [-1.8055,  0.5198,  1.2424],
        [-1.6933,  0.1127,  0.1598],
        [-2.6395, -1.0274,  0.2296]], device='cuda:0')
after: tensor([[-0.0676, -0.2095, -2.4560],
        [-1.8055,  0.5198,  1.2424],
        [-1.6933,  0.1127,  0.1598],
        [-2.6395, -1.0274,  0.2296]], device='cuda:0')
********************
observing microbatch 11
current batch shape is torch.Size([4, 3])
batch is tensor([[-0.0676, -0.2095, -2.4560],
        [-1.8055,  0.5198,  1.2424],
        [-1.6933,  0.1127,  0.1598],
        [-2.6395, -1.0274,  0.2296]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 11
********************
inputting microbatch 10 into partition 1
before moving to cuda:1: tensor([[0.6509, 0.3807, 0.5448, 0.6687],
        [0.4262, 0.4952, 0.5763, 0.4498],
        [0.5864, 0.6754, 0.3861, 0.5466],
        [0.7308, 0.3512, 0.4125, 0.7221]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9734, 0.9733, 0.9753, 0.9666],
        [0.9683, 0.9734, 0.9733, 0.9753],
        [0.9668, 0.9683, 0.9732, 0.9731],
        [0.9751, 0.9666, 0.9681, 0.9733]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 10
current batch shape is torch.Size([4, 4])
batch is tensor([[0.9734, 0.9733, 0.9753, 0.9666],
        [0.9683, 0.9734, 0.9733, 0.9753],
        [0.9668, 0.9683, 0.9732, 0.9731],
        [0.9751, 0.9666, 0.9681, 0.9733]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 10
********************
receiving microbatch 11 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 11, tensor([[0.5933, 0.7397, 0.1541, 0.4758],
        [0.4156, 0.4781, 0.7568, 0.4882],
        [0.4082, 0.5558, 0.6283, 0.4395],
        [0.2325, 0.4582, 0.7650, 0.2974]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 11
result is tensor([[0.5933, 0.7397, 0.1541, 0.4758],
        [0.4156, 0.4781, 0.7568, 0.4882],
        [0.4082, 0.5558, 0.6283, 0.4395],
        [0.2325, 0.4582, 0.7650, 0.2974]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5933, 0.7397, 0.1541, 0.4758],
        [0.4156, 0.4781, 0.7568, 0.4882],
        [0.4082, 0.5558, 0.6283, 0.4395],
        [0.2325, 0.4582, 0.7650, 0.2974]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 10 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 10, tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 10
result is tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9448, 0.9472, 0.9503, 0.9406, 0.9381],
        [0.9410, 0.9399, 0.9431, 0.9381, 0.9332],
        [0.9465, 0.9404, 0.9421, 0.9300, 0.9365],
        [0.9503, 0.9421, 0.9443, 0.9279, 0.9394]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9715, 0.9712, 0.9732, 0.9643, 0.9661],
        [0.9710, 0.9708, 0.9728, 0.9639, 0.9656],
        [0.9709, 0.9707, 0.9727, 0.9638, 0.9655],
        [0.9709, 0.9706, 0.9727, 0.9639, 0.9655]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9733, 0.9731, 0.9751, 0.9665, 0.9682],
        [0.9732, 0.9731, 0.9751, 0.9665, 0.9681],
        [0.9730, 0.9729, 0.9749, 0.9663, 0.9679],
        [0.9731, 0.9729, 0.9749, 0.9664, 0.9680]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9732, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9667, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.9932, 0.9920, 0.9923, 0.9907, 0.9911],
        [0.9375, 0.9843, 0.9779, 0.9714, 0.9572]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9753, 0.9753, 0.9772, 0.9690, 0.9705],
        [0.9750, 0.9748, 0.9768, 0.9685, 0.9701],
        [0.9738, 0.9735, 0.9754, 0.9670, 0.9687],
        [0.9729, 0.9724, 0.9744, 0.9655, 0.9676]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9736, 0.9734, 0.9754, 0.9668, 0.9685],
        [0.9735, 0.9734, 0.9754, 0.9669, 0.9685],
        [0.9733, 0.9732, 0.9752, 0.9667, 0.9682],
        [0.9733, 0.9731, 0.9752, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9735, 0.9733, 0.9753, 0.9666, 0.9684],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9682],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5933, 0.7397, 0.1541, 0.4758],
        [0.4156, 0.4781, 0.7568, 0.4882],
        [0.4082, 0.5558, 0.6283, 0.4395],
        [0.2325, 0.4582, 0.7650, 0.2974]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-1.0079,  0.3010, -0.2049],
        [-0.2963,  1.1941, -0.7664],
        [ 1.0457, -0.6663,  0.0405],
        [-0.0975,  0.2815,  1.5998]], device='cuda:0'), tensor([[ 2.4996,  0.5638, -1.5046],
        [ 2.0686,  0.9217,  1.6783],
        [-1.6940, -0.3129,  0.0719],
        [ 0.1603, -0.9724, -0.7393]], device='cuda:0'), tensor([[ 1.9289,  0.6337, -0.5543],
        [ 0.8780,  1.2082,  0.5716],
        [-1.3254,  1.5558,  1.6003],
        [-1.0849, -0.0165,  0.6126]], device='cuda:0'), tensor([[-1.2905, -0.9718, -0.5923],
        [-0.7282,  0.3135,  1.2061],
        [-0.0195, -0.0362,  1.3275],
        [-0.9153, -0.2002, -0.2004]], device='cuda:0')]
====================
schedule for this step is
[(12, 0), (11, 1)]
inputting microbatch 12 into partition 0
before moving to cuda:0: tensor([[-1.0079,  0.3010, -0.2049],
        [-0.2963,  1.1941, -0.7664],
        [ 1.0457, -0.6663,  0.0405],
        [-0.0975,  0.2815,  1.5998]], device='cuda:0')
after: tensor([[-1.0079,  0.3010, -0.2049],
        [-0.2963,  1.1941, -0.7664],
        [ 1.0457, -0.6663,  0.0405],
        [-0.0975,  0.2815,  1.5998]], device='cuda:0')
********************
observing microbatch 12
current batch shape is torch.Size([4, 3])
batch is tensor([[-1.0079,  0.3010, -0.2049],
        [-0.2963,  1.1941, -0.7664],
        [ 1.0457, -0.6663,  0.0405],
        [-0.0975,  0.2815,  1.5998]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 12
********************
inputting microbatch 11 into partition 1
before moving to cuda:1: tensor([[0.5933, 0.7397, 0.1541, 0.4758],
        [0.4156, 0.4781, 0.7568, 0.4882],
        [0.4082, 0.5558, 0.6283, 0.4395],
        [0.2325, 0.4582, 0.7650, 0.2974]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9734, 0.9733, 0.9753, 0.9666],
        [0.9683, 0.9734, 0.9733, 0.9753],
        [0.9668, 0.9683, 0.9732, 0.9731],
        [0.9751, 0.9666, 0.9681, 0.9733]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 11
current batch shape is torch.Size([4, 4])
batch is tensor([[0.9734, 0.9733, 0.9753, 0.9666],
        [0.9683, 0.9734, 0.9733, 0.9753],
        [0.9668, 0.9683, 0.9732, 0.9731],
        [0.9751, 0.9666, 0.9681, 0.9733]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 11
********************
receiving microbatch 12 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 12, tensor([[0.5038, 0.5903, 0.4895, 0.4975],
        [0.6634, 0.7151, 0.3045, 0.6007],
        [0.6314, 0.3637, 0.3138, 0.5996],
        [0.5716, 0.3386, 0.6296, 0.6161]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 12
result is tensor([[0.5038, 0.5903, 0.4895, 0.4975],
        [0.6634, 0.7151, 0.3045, 0.6007],
        [0.6314, 0.3637, 0.3138, 0.5996],
        [0.5716, 0.3386, 0.6296, 0.6161]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5038, 0.5903, 0.4895, 0.4975],
        [0.6634, 0.7151, 0.3045, 0.6007],
        [0.6314, 0.3637, 0.3138, 0.5996],
        [0.5716, 0.3386, 0.6296, 0.6161]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 11 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 11, tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 11
result is tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9448, 0.9472, 0.9503, 0.9406, 0.9381],
        [0.9410, 0.9399, 0.9431, 0.9381, 0.9332],
        [0.9465, 0.9404, 0.9421, 0.9300, 0.9365],
        [0.9503, 0.9421, 0.9443, 0.9279, 0.9394]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9715, 0.9712, 0.9732, 0.9643, 0.9661],
        [0.9710, 0.9708, 0.9728, 0.9639, 0.9656],
        [0.9709, 0.9707, 0.9727, 0.9638, 0.9655],
        [0.9709, 0.9706, 0.9727, 0.9639, 0.9655]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9733, 0.9731, 0.9751, 0.9665, 0.9682],
        [0.9732, 0.9731, 0.9751, 0.9665, 0.9681],
        [0.9730, 0.9729, 0.9749, 0.9663, 0.9679],
        [0.9731, 0.9729, 0.9749, 0.9664, 0.9680]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9732, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9667, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.9932, 0.9920, 0.9923, 0.9907, 0.9911],
        [0.9375, 0.9843, 0.9779, 0.9714, 0.9572]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9753, 0.9753, 0.9772, 0.9690, 0.9705],
        [0.9750, 0.9748, 0.9768, 0.9685, 0.9701],
        [0.9738, 0.9735, 0.9754, 0.9670, 0.9687],
        [0.9729, 0.9724, 0.9744, 0.9655, 0.9676]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9736, 0.9734, 0.9754, 0.9668, 0.9685],
        [0.9735, 0.9734, 0.9754, 0.9669, 0.9685],
        [0.9733, 0.9732, 0.9752, 0.9667, 0.9682],
        [0.9733, 0.9731, 0.9752, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9735, 0.9733, 0.9753, 0.9666, 0.9684],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9682],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5038, 0.5903, 0.4895, 0.4975],
        [0.6634, 0.7151, 0.3045, 0.6007],
        [0.6314, 0.3637, 0.3138, 0.5996],
        [0.5716, 0.3386, 0.6296, 0.6161]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 2.4996,  0.5638, -1.5046],
        [ 2.0686,  0.9217,  1.6783],
        [-1.6940, -0.3129,  0.0719],
        [ 0.1603, -0.9724, -0.7393]], device='cuda:0'), tensor([[ 1.9289,  0.6337, -0.5543],
        [ 0.8780,  1.2082,  0.5716],
        [-1.3254,  1.5558,  1.6003],
        [-1.0849, -0.0165,  0.6126]], device='cuda:0'), tensor([[-1.2905, -0.9718, -0.5923],
        [-0.7282,  0.3135,  1.2061],
        [-0.0195, -0.0362,  1.3275],
        [-0.9153, -0.2002, -0.2004]], device='cuda:0')]
====================
schedule for this step is
[(13, 0), (12, 1)]
inputting microbatch 13 into partition 0
before moving to cuda:0: tensor([[ 2.4996,  0.5638, -1.5046],
        [ 2.0686,  0.9217,  1.6783],
        [-1.6940, -0.3129,  0.0719],
        [ 0.1603, -0.9724, -0.7393]], device='cuda:0')
after: tensor([[ 2.4996,  0.5638, -1.5046],
        [ 2.0686,  0.9217,  1.6783],
        [-1.6940, -0.3129,  0.0719],
        [ 0.1603, -0.9724, -0.7393]], device='cuda:0')
********************
observing microbatch 13
current batch shape is torch.Size([4, 3])
batch is tensor([[ 2.4996,  0.5638, -1.5046],
        [ 2.0686,  0.9217,  1.6783],
        [-1.6940, -0.3129,  0.0719],
        [ 0.1603, -0.9724, -0.7393]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 13
********************
inputting microbatch 12 into partition 1
before moving to cuda:1: tensor([[0.5038, 0.5903, 0.4895, 0.4975],
        [0.6634, 0.7151, 0.3045, 0.6007],
        [0.6314, 0.3637, 0.3138, 0.5996],
        [0.5716, 0.3386, 0.6296, 0.6161]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9734, 0.9733, 0.9753, 0.9666],
        [0.9683, 0.9734, 0.9733, 0.9753],
        [0.9668, 0.9683, 0.9732, 0.9731],
        [0.9751, 0.9666, 0.9681, 0.9733]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 12
current batch shape is torch.Size([4, 4])
batch is tensor([[0.9734, 0.9733, 0.9753, 0.9666],
        [0.9683, 0.9734, 0.9733, 0.9753],
        [0.9668, 0.9683, 0.9732, 0.9731],
        [0.9751, 0.9666, 0.9681, 0.9733]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 12
********************
receiving microbatch 13 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 13, tensor([[0.8462, 0.6206, 0.0738, 0.7404],
        [0.8098, 0.3114, 0.3604, 0.7952],
        [0.3730, 0.5176, 0.6300, 0.4079],
        [0.5236, 0.4575, 0.3216, 0.4872]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 13
result is tensor([[0.8462, 0.6206, 0.0738, 0.7404],
        [0.8098, 0.3114, 0.3604, 0.7952],
        [0.3730, 0.5176, 0.6300, 0.4079],
        [0.5236, 0.4575, 0.3216, 0.4872]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.8462, 0.6206, 0.0738, 0.7404],
        [0.8098, 0.3114, 0.3604, 0.7952],
        [0.3730, 0.5176, 0.6300, 0.4079],
        [0.5236, 0.4575, 0.3216, 0.4872]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 12 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 12, tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 12
result is tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9448, 0.9472, 0.9503, 0.9406, 0.9381],
        [0.9410, 0.9399, 0.9431, 0.9381, 0.9332],
        [0.9465, 0.9404, 0.9421, 0.9300, 0.9365],
        [0.9503, 0.9421, 0.9443, 0.9279, 0.9394]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9715, 0.9712, 0.9732, 0.9643, 0.9661],
        [0.9710, 0.9708, 0.9728, 0.9639, 0.9656],
        [0.9709, 0.9707, 0.9727, 0.9638, 0.9655],
        [0.9709, 0.9706, 0.9727, 0.9639, 0.9655]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9733, 0.9731, 0.9751, 0.9665, 0.9682],
        [0.9732, 0.9731, 0.9751, 0.9665, 0.9681],
        [0.9730, 0.9729, 0.9749, 0.9663, 0.9679],
        [0.9731, 0.9729, 0.9749, 0.9664, 0.9680]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9732, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9667, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.9932, 0.9920, 0.9923, 0.9907, 0.9911],
        [0.9375, 0.9843, 0.9779, 0.9714, 0.9572]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9753, 0.9753, 0.9772, 0.9690, 0.9705],
        [0.9750, 0.9748, 0.9768, 0.9685, 0.9701],
        [0.9738, 0.9735, 0.9754, 0.9670, 0.9687],
        [0.9729, 0.9724, 0.9744, 0.9655, 0.9676]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9736, 0.9734, 0.9754, 0.9668, 0.9685],
        [0.9735, 0.9734, 0.9754, 0.9669, 0.9685],
        [0.9733, 0.9732, 0.9752, 0.9667, 0.9682],
        [0.9733, 0.9731, 0.9752, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9735, 0.9733, 0.9753, 0.9666, 0.9684],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9682],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8462, 0.6206, 0.0738, 0.7404],
        [0.8098, 0.3114, 0.3604, 0.7952],
        [0.3730, 0.5176, 0.6300, 0.4079],
        [0.5236, 0.4575, 0.3216, 0.4872]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 1.9289,  0.6337, -0.5543],
        [ 0.8780,  1.2082,  0.5716],
        [-1.3254,  1.5558,  1.6003],
        [-1.0849, -0.0165,  0.6126]], device='cuda:0'), tensor([[-1.2905, -0.9718, -0.5923],
        [-0.7282,  0.3135,  1.2061],
        [-0.0195, -0.0362,  1.3275],
        [-0.9153, -0.2002, -0.2004]], device='cuda:0')]
====================
schedule for this step is
[(14, 0), (13, 1)]
inputting microbatch 14 into partition 0
before moving to cuda:0: tensor([[ 1.9289,  0.6337, -0.5543],
        [ 0.8780,  1.2082,  0.5716],
        [-1.3254,  1.5558,  1.6003],
        [-1.0849, -0.0165,  0.6126]], device='cuda:0')
after: tensor([[ 1.9289,  0.6337, -0.5543],
        [ 0.8780,  1.2082,  0.5716],
        [-1.3254,  1.5558,  1.6003],
        [-1.0849, -0.0165,  0.6126]], device='cuda:0')
********************
observing microbatch 14
current batch shape is torch.Size([4, 3])
batch is tensor([[ 1.9289,  0.6337, -0.5543],
        [ 0.8780,  1.2082,  0.5716],
        [-1.3254,  1.5558,  1.6003],
        [-1.0849, -0.0165,  0.6126]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 14
********************
inputting microbatch 13 into partition 1
before moving to cuda:1: tensor([[0.8462, 0.6206, 0.0738, 0.7404],
        [0.8098, 0.3114, 0.3604, 0.7952],
        [0.3730, 0.5176, 0.6300, 0.4079],
        [0.5236, 0.4575, 0.3216, 0.4872]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9734, 0.9733, 0.9753, 0.9666],
        [0.9683, 0.9734, 0.9733, 0.9753],
        [0.9668, 0.9683, 0.9732, 0.9731],
        [0.9751, 0.9666, 0.9681, 0.9733]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 13
current batch shape is torch.Size([4, 4])
batch is tensor([[0.9734, 0.9733, 0.9753, 0.9666],
        [0.9683, 0.9734, 0.9733, 0.9753],
        [0.9668, 0.9683, 0.9732, 0.9731],
        [0.9751, 0.9666, 0.9681, 0.9733]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 13
********************
receiving microbatch 14 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 14, tensor([[0.8060, 0.5422, 0.1514, 0.7287],
        [0.7516, 0.5197, 0.3429, 0.7180],
        [0.5540, 0.5332, 0.7238, 0.6076],
        [0.4542, 0.4602, 0.6221, 0.4912]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 14
result is tensor([[0.8060, 0.5422, 0.1514, 0.7287],
        [0.7516, 0.5197, 0.3429, 0.7180],
        [0.5540, 0.5332, 0.7238, 0.6076],
        [0.4542, 0.4602, 0.6221, 0.4912]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.8060, 0.5422, 0.1514, 0.7287],
        [0.7516, 0.5197, 0.3429, 0.7180],
        [0.5540, 0.5332, 0.7238, 0.6076],
        [0.4542, 0.4602, 0.6221, 0.4912]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 13 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 13, tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 13
result is tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9448, 0.9472, 0.9503, 0.9406, 0.9381],
        [0.9410, 0.9399, 0.9431, 0.9381, 0.9332],
        [0.9465, 0.9404, 0.9421, 0.9300, 0.9365],
        [0.9503, 0.9421, 0.9443, 0.9279, 0.9394]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9715, 0.9712, 0.9732, 0.9643, 0.9661],
        [0.9710, 0.9708, 0.9728, 0.9639, 0.9656],
        [0.9709, 0.9707, 0.9727, 0.9638, 0.9655],
        [0.9709, 0.9706, 0.9727, 0.9639, 0.9655]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9733, 0.9731, 0.9751, 0.9665, 0.9682],
        [0.9732, 0.9731, 0.9751, 0.9665, 0.9681],
        [0.9730, 0.9729, 0.9749, 0.9663, 0.9679],
        [0.9731, 0.9729, 0.9749, 0.9664, 0.9680]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9732, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9667, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.9932, 0.9920, 0.9923, 0.9907, 0.9911],
        [0.9375, 0.9843, 0.9779, 0.9714, 0.9572]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9753, 0.9753, 0.9772, 0.9690, 0.9705],
        [0.9750, 0.9748, 0.9768, 0.9685, 0.9701],
        [0.9738, 0.9735, 0.9754, 0.9670, 0.9687],
        [0.9729, 0.9724, 0.9744, 0.9655, 0.9676]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9736, 0.9734, 0.9754, 0.9668, 0.9685],
        [0.9735, 0.9734, 0.9754, 0.9669, 0.9685],
        [0.9733, 0.9732, 0.9752, 0.9667, 0.9682],
        [0.9733, 0.9731, 0.9752, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9735, 0.9733, 0.9753, 0.9666, 0.9684],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9682],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.8060, 0.5422, 0.1514, 0.7287],
        [0.7516, 0.5197, 0.3429, 0.7180],
        [0.5540, 0.5332, 0.7238, 0.6076],
        [0.4542, 0.4602, 0.6221, 0.4912]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-1.2905, -0.9718, -0.5923],
        [-0.7282,  0.3135,  1.2061],
        [-0.0195, -0.0362,  1.3275],
        [-0.9153, -0.2002, -0.2004]], device='cuda:0')]
====================
schedule for this step is
[(15, 0), (14, 1)]
inputting microbatch 15 into partition 0
before moving to cuda:0: tensor([[-1.2905, -0.9718, -0.5923],
        [-0.7282,  0.3135,  1.2061],
        [-0.0195, -0.0362,  1.3275],
        [-0.9153, -0.2002, -0.2004]], device='cuda:0')
after: tensor([[-1.2905, -0.9718, -0.5923],
        [-0.7282,  0.3135,  1.2061],
        [-0.0195, -0.0362,  1.3275],
        [-0.9153, -0.2002, -0.2004]], device='cuda:0')
********************
observing microbatch 15
current batch shape is torch.Size([4, 3])
batch is tensor([[-1.2905, -0.9718, -0.5923],
        [-0.7282,  0.3135,  1.2061],
        [-0.0195, -0.0362,  1.3275],
        [-0.9153, -0.2002, -0.2004]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 15
********************
inputting microbatch 14 into partition 1
before moving to cuda:1: tensor([[0.8060, 0.5422, 0.1514, 0.7287],
        [0.7516, 0.5197, 0.3429, 0.7180],
        [0.5540, 0.5332, 0.7238, 0.6076],
        [0.4542, 0.4602, 0.6221, 0.4912]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9734, 0.9733, 0.9753, 0.9666],
        [0.9683, 0.9734, 0.9733, 0.9753],
        [0.9668, 0.9683, 0.9732, 0.9731],
        [0.9751, 0.9666, 0.9681, 0.9733]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 14
current batch shape is torch.Size([4, 4])
batch is tensor([[0.9734, 0.9733, 0.9753, 0.9666],
        [0.9683, 0.9734, 0.9733, 0.9753],
        [0.9668, 0.9683, 0.9732, 0.9731],
        [0.9751, 0.9666, 0.9681, 0.9733]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 14
********************
receiving microbatch 15 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 15, tensor([[0.3680, 0.5037, 0.5120, 0.3774],
        [0.5130, 0.4122, 0.6488, 0.5589],
        [0.5559, 0.3321, 0.5941, 0.5937],
        [0.4686, 0.5290, 0.4955, 0.4698]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 15
result is tensor([[0.3680, 0.5037, 0.5120, 0.3774],
        [0.5130, 0.4122, 0.6488, 0.5589],
        [0.5559, 0.3321, 0.5941, 0.5937],
        [0.4686, 0.5290, 0.4955, 0.4698]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3680, 0.5037, 0.5120, 0.3774],
        [0.5130, 0.4122, 0.6488, 0.5589],
        [0.5559, 0.3321, 0.5941, 0.5937],
        [0.4686, 0.5290, 0.4955, 0.4698]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 14 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 14, tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 14
result is tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9448, 0.9472, 0.9503, 0.9406, 0.9381],
        [0.9410, 0.9399, 0.9431, 0.9381, 0.9332],
        [0.9465, 0.9404, 0.9421, 0.9300, 0.9365],
        [0.9503, 0.9421, 0.9443, 0.9279, 0.9394]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9715, 0.9712, 0.9732, 0.9643, 0.9661],
        [0.9710, 0.9708, 0.9728, 0.9639, 0.9656],
        [0.9709, 0.9707, 0.9727, 0.9638, 0.9655],
        [0.9709, 0.9706, 0.9727, 0.9639, 0.9655]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9733, 0.9731, 0.9751, 0.9665, 0.9682],
        [0.9732, 0.9731, 0.9751, 0.9665, 0.9681],
        [0.9730, 0.9729, 0.9749, 0.9663, 0.9679],
        [0.9731, 0.9729, 0.9749, 0.9664, 0.9680]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9732, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9667, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.9932, 0.9920, 0.9923, 0.9907, 0.9911],
        [0.9375, 0.9843, 0.9779, 0.9714, 0.9572]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9753, 0.9753, 0.9772, 0.9690, 0.9705],
        [0.9750, 0.9748, 0.9768, 0.9685, 0.9701],
        [0.9738, 0.9735, 0.9754, 0.9670, 0.9687],
        [0.9729, 0.9724, 0.9744, 0.9655, 0.9676]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9736, 0.9734, 0.9754, 0.9668, 0.9685],
        [0.9735, 0.9734, 0.9754, 0.9669, 0.9685],
        [0.9733, 0.9732, 0.9752, 0.9667, 0.9682],
        [0.9733, 0.9731, 0.9752, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9735, 0.9733, 0.9753, 0.9666, 0.9684],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9682],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3680, 0.5037, 0.5120, 0.3774],
        [0.5130, 0.4122, 0.6488, 0.5589],
        [0.5559, 0.3321, 0.5941, 0.5937],
        [0.4686, 0.5290, 0.4955, 0.4698]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)]
====================
schedule for this step is
[(15, 1)]
inputting microbatch 15 into partition 1
before moving to cuda:1: tensor([[0.3680, 0.5037, 0.5120, 0.3774],
        [0.5130, 0.4122, 0.6488, 0.5589],
        [0.5559, 0.3321, 0.5941, 0.5937],
        [0.4686, 0.5290, 0.4955, 0.4698]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9734, 0.9733, 0.9753, 0.9666],
        [0.9683, 0.9734, 0.9733, 0.9753],
        [0.9668, 0.9683, 0.9732, 0.9731],
        [0.9751, 0.9666, 0.9681, 0.9733]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 15
current batch shape is torch.Size([4, 4])
batch is tensor([[0.9734, 0.9733, 0.9753, 0.9666],
        [0.9683, 0.9734, 0.9733, 0.9753],
        [0.9668, 0.9683, 0.9732, 0.9731],
        [0.9751, 0.9666, 0.9681, 0.9733]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 15
********************
receiving microbatch 15 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 15, tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 15
result is tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9448, 0.9472, 0.9503, 0.9406, 0.9381],
        [0.9410, 0.9399, 0.9431, 0.9381, 0.9332],
        [0.9465, 0.9404, 0.9421, 0.9300, 0.9365],
        [0.9503, 0.9421, 0.9443, 0.9279, 0.9394]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9715, 0.9712, 0.9732, 0.9643, 0.9661],
        [0.9710, 0.9708, 0.9728, 0.9639, 0.9656],
        [0.9709, 0.9707, 0.9727, 0.9638, 0.9655],
        [0.9709, 0.9706, 0.9727, 0.9639, 0.9655]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9733, 0.9731, 0.9751, 0.9665, 0.9682],
        [0.9732, 0.9731, 0.9751, 0.9665, 0.9681],
        [0.9730, 0.9729, 0.9749, 0.9663, 0.9679],
        [0.9731, 0.9729, 0.9749, 0.9664, 0.9680]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9732, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9667, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.9932, 0.9920, 0.9923, 0.9907, 0.9911],
        [0.9375, 0.9843, 0.9779, 0.9714, 0.9572]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9753, 0.9753, 0.9772, 0.9690, 0.9705],
        [0.9750, 0.9748, 0.9768, 0.9685, 0.9701],
        [0.9738, 0.9735, 0.9754, 0.9670, 0.9687],
        [0.9729, 0.9724, 0.9744, 0.9655, 0.9676]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9736, 0.9734, 0.9754, 0.9668, 0.9685],
        [0.9735, 0.9734, 0.9754, 0.9669, 0.9685],
        [0.9733, 0.9732, 0.9752, 0.9667, 0.9682],
        [0.9733, 0.9731, 0.9752, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9735, 0.9733, 0.9753, 0.9666, 0.9684],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9682],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9734, 0.9733, 0.9753, 0.9666, 0.9683],
        [0.9734, 0.9733, 0.9753, 0.9668, 0.9683],
        [0.9732, 0.9731, 0.9751, 0.9666, 0.9681],
        [0.9733, 0.9731, 0.9751, 0.9666, 0.9682]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)]
finished running cycle
_____________________________ test_forward_0[8-1] ______________________________

batch_size = 1, split_size = 8

    @pytest.mark.a4_2_2
    @pytest.mark.parametrize("batch_size", [1, 16, 32, 64])
    @pytest.mark.parametrize("split_size", [1, 2, 4, 8, 16])
    # @pytest.mark.parametrize("batch_size", [1])
    # @pytest.mark.parametrize("split_size", [1, 2])
    def test_forward_0(batch_size, split_size):
        model = nn.Sequential(
            nn.Linear(3, 4).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
            nn.Linear(4, 5).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
        )
    
        x = torch.randn(batch_size, 3).to('cuda:0')
        y0 = model(x).to('cpu')
    
        # move the last two layer to another device
        model[-2] = model[-2].to('cuda:1')
        model[-1] = WithDevice(nn.Sigmoid(), 'cuda:1')
        pipe = Pipe(model, split_size=split_size)
        y1 = pipe(x).to('cpu')
>       assert torch.allclose(y0, y1)
E       assert False
E        +  where False = <built-in method allclose of type object at 0x153da78cb760>(tensor([[0.3918, 0.3877, 0.4631, 0.3642, 0.6121]], grad_fn=<ToCopyBackward0>), tensor([[0.9912, 0.9912, 0.9912, 0.9911, 0.9911]], grad_fn=<ToCopyBackward0>))
E        +    where <built-in method allclose of type object at 0x153da78cb760> = torch.allclose

batch_size = 1
model      = Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
  (2): Linear(in_features=4, out_features=5, bias=True)
  (3): WithDevice(
    (_module): Sigmoid()
  )
)
pipe       = Pipe(
  (partitions): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=3, out_features=4, bias=True)
   ...inear(in_features=4, out_features=5, bias=True)
      (1): WithDevice(
        (_module): Sigmoid()
      )
    )
  )
)
split_size = 8
x          = tensor([[-0.4150, -0.1010, -2.4771]], device='cuda:0')
y0         = tensor([[0.3918, 0.3877, 0.4631, 0.3642, 0.6121]], grad_fn=<ToCopyBackward0>)
y1         = tensor([[0.9912, 0.9912, 0.9912, 0.9911, 0.9911]], grad_fn=<ToCopyBackward0>)

tests/test_pipeline.py:83: AssertionError
----------------------------- Captured stdout call -----------------------------
in init
ModuleList(
  (0): Sequential(
    (0): Linear(in_features=3, out_features=4, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
  (1): Sequential(
    (0): Linear(in_features=4, out_features=5, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
)
[device(type='cuda', index=0), device(type='cuda', index=1)]
before running the cycle
[tensor([[-0.4150, -0.1010, -2.4771]], device='cuda:0')]
====================
schedule for this step is
[(0, 0)]
inputting microbatch 0 into partition 0
before moving to cuda:0: tensor([[-0.4150, -0.1010, -2.4771]], device='cuda:0')
after: tensor([[-0.4150, -0.1010, -2.4771]], device='cuda:0')
********************
observing microbatch 0
current batch shape is torch.Size([1, 3])
batch is tensor([[-0.4150, -0.1010, -2.4771]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 0
********************
receiving microbatch 0 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 0, tensor([[0.5535, 0.7143, 0.3156, 0.1755]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.5535, 0.7143, 0.3156, 0.1755]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5535, 0.7143, 0.3156, 0.1755]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.5535, 0.7143, 0.3156, 0.1755]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)]
====================
schedule for this step is
[(0, 1)]
inputting microbatch 0 into partition 1
before moving to cuda:1: tensor([[0.5535, 0.7143, 0.3156, 0.1755]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9685, 0.9701, 0.9738, 0.9735]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 0
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9685, 0.9701, 0.9738, 0.9735]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 0
********************
receiving microbatch 0 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 0, tensor([[0.9912, 0.9912, 0.9912, 0.9911, 0.9911]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.9912, 0.9912, 0.9912, 0.9911, 0.9911]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9912, 0.9912, 0.9912, 0.9911, 0.9911]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9912, 0.9912, 0.9912, 0.9911, 0.9911]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)]
finished running cycle
_____________________________ test_forward_0[8-16] _____________________________

batch_size = 16, split_size = 8

    @pytest.mark.a4_2_2
    @pytest.mark.parametrize("batch_size", [1, 16, 32, 64])
    @pytest.mark.parametrize("split_size", [1, 2, 4, 8, 16])
    # @pytest.mark.parametrize("batch_size", [1])
    # @pytest.mark.parametrize("split_size", [1, 2])
    def test_forward_0(batch_size, split_size):
        model = nn.Sequential(
            nn.Linear(3, 4).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
            nn.Linear(4, 5).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
        )
    
        x = torch.randn(batch_size, 3).to('cuda:0')
        y0 = model(x).to('cpu')
    
        # move the last two layer to another device
        model[-2] = model[-2].to('cuda:1')
        model[-1] = WithDevice(nn.Sigmoid(), 'cuda:1')
        pipe = Pipe(model, split_size=split_size)
        y1 = pipe(x).to('cpu')
>       assert torch.allclose(y0, y1)
E       assert False
E        +  where False = <built-in method allclose of type object at 0x153da78cb760>(tensor([[0.5900, 0.5043, 0.4175, 0.6102, 0.4199],\n        [0.5664, 0.5448, 0.4212, 0.6057, 0.4159],\n        [0.5605, 0.5394, 0.4370, 0.6190, 0.4221],\n        [0.5726, 0.5312, 0.4151, 0.5935, 0.4169],\n        [0.5794, 0.5267, 0.4068, 0.5875, 0.4147],\n        [0.5628, 0.5268, 0.4300, 0.5975, 0.4238],\n        [0.5294, 0.5392, 0.4629, 0.6022, 0.4360],\n        [0.5920, 0.5055, 0.4172, 0.6152, 0.4189],\n        [0.5587, 0.5326, 0.4341, 0.6023, 0.4240],\n        [0.5648, 0.5677, 0.3989, 0.5811, 0.4058],\n        [0.5625, 0.5328, 0.4355, 0.6137, 0.4231],\n        [0.5928, 0.4962, 0.4140, 0.6018, 0.4208],\n        [0.5562, 0.5304, 0.4408, 0.6081, 0.4266],\n        [0.5769, 0.5340, 0.4168, 0.6095, 0.4151],\n        [0.5411, 0.5446, 0.4433, 0.5945, 0.4274],\n        [0.5469, 0.5517, 0.4329, 0.5938, 0.4218]], grad_fn=<ToCopyBackward0>), tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n        [1.0000, 0.9959, 0.9904, 0.9902, 0.9902],\n        [1.0000, 0.9700, 0.9317, 0.9308, 0.9313],\n        [1.0000, 0.8668, 0.7293, 0.7293, 0.7293],\n        [1.0000, 0.8668, 0.7293, 0.7293, 0.7293],\n        [1.0000, 0.8668, 0.7293, 0.7293, 0.7293],\n        [1.0000, 0.9969, 0.9925, 0.9924, 0.9924],\n        [1.0000, 0.9969, 0.9925, 0.9924, 0.9924],\n        [1.0000, 0.9969, 0.9925, 0.9924, 0.9924],\n        [1.0000, 0.9968, 0.9922, 0.9922, 0.9922],\n        [1.0000, 0.9964, 0.9911, 0.9911, 0.9911],\n        [1.0000, 0.9951, 0.9881, 0.9880, 0.9880],\n        [1.0000, 0.9941, 0.9856, 0.9855, 0.9855],\n        [1.0000, 0.9932, 0.9836, 0.9835, 0.9835]], grad_fn=<ToCopyBackward0>))
E        +    where <built-in method allclose of type object at 0x153da78cb760> = torch.allclose

batch_size = 16
model      = Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
  (2): Linear(in_features=4, out_features=5, bias=True)
  (3): WithDevice(
    (_module): Sigmoid()
  )
)
pipe       = Pipe(
  (partitions): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=3, out_features=4, bias=True)
   ...inear(in_features=4, out_features=5, bias=True)
      (1): WithDevice(
        (_module): Sigmoid()
      )
    )
  )
)
split_size = 8
x          = tensor([[-1.5940,  0.1417, -1.1487],
        [-0.9376, -0.3906,  1.1768],
        [-1.6480, -1.4723,  0.7225],
       ...-1.5249, -0.2342,  0.8203],
        [ 1.2828, -0.7455, -0.2576],
        [ 0.8965, -0.4163,  0.6373]], device='cuda:0')
y0         = tensor([[0.5900, 0.5043, 0.4175, 0.6102, 0.4199],
        [0.5664, 0.5448, 0.4212, 0.6057, 0.4159],
        [0.5605, 0...[0.5411, 0.5446, 0.4433, 0.5945, 0.4274],
        [0.5469, 0.5517, 0.4329, 0.5938, 0.4218]], grad_fn=<ToCopyBackward0>)
y1         = tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 1...[1.0000, 0.9941, 0.9856, 0.9855, 0.9855],
        [1.0000, 0.9932, 0.9836, 0.9835, 0.9835]], grad_fn=<ToCopyBackward0>)

tests/test_pipeline.py:83: AssertionError
----------------------------- Captured stdout call -----------------------------
in init
ModuleList(
  (0): Sequential(
    (0): Linear(in_features=3, out_features=4, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
  (1): Sequential(
    (0): Linear(in_features=4, out_features=5, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
)
[device(type='cuda', index=0), device(type='cuda', index=1)]
before running the cycle
[tensor([[-1.5940,  0.1417, -1.1487],
        [-0.9376, -0.3906,  1.1768],
        [-1.6480, -1.4723,  0.7225],
        [ 0.0811,  0.5526,  0.0309],
        [ 0.4233,  1.2825, -0.2112],
        [ 0.2831, -0.1728, -0.6886],
        [ 1.4738, -1.6943, -1.2739],
        [-2.1722, -0.0961, -0.8458]], device='cuda:0'), tensor([[-0.0407, -0.5962, -0.2791],
        [ 0.8230,  1.2474,  2.5904],
        [-1.1772, -1.1254,  0.1522],
        [-0.7693,  0.8680, -1.9455],
        [-0.3535, -1.0630, -0.4670],
        [-1.5249, -0.2342,  0.8203],
        [ 1.2828, -0.7455, -0.2576],
        [ 0.8965, -0.4163,  0.6373]], device='cuda:0')]
====================
schedule for this step is
[(0, 0)]
inputting microbatch 0 into partition 0
before moving to cuda:0: tensor([[-1.5940,  0.1417, -1.1487],
        [-0.9376, -0.3906,  1.1768],
        [-1.6480, -1.4723,  0.7225],
        [ 0.0811,  0.5526,  0.0309],
        [ 0.4233,  1.2825, -0.2112],
        [ 0.2831, -0.1728, -0.6886],
        [ 1.4738, -1.6943, -1.2739],
        [-2.1722, -0.0961, -0.8458]], device='cuda:0')
after: tensor([[-1.5940,  0.1417, -1.1487],
        [-0.9376, -0.3906,  1.1768],
        [-1.6480, -1.4723,  0.7225],
        [ 0.0811,  0.5526,  0.0309],
        [ 0.4233,  1.2825, -0.2112],
        [ 0.2831, -0.1728, -0.6886],
        [ 1.4738, -1.6943, -1.2739],
        [-2.1722, -0.0961, -0.8458]], device='cuda:0')
********************
observing microbatch 0
current batch shape is torch.Size([8, 3])
batch is tensor([[-1.5940,  0.1417, -1.1487],
        [-0.9376, -0.3906,  1.1768],
        [-1.6480, -1.4723,  0.7225],
        [ 0.0811,  0.5526,  0.0309],
        [ 0.4233,  1.2825, -0.2112],
        [ 0.2831, -0.1728, -0.6886],
        [ 1.4738, -1.6943, -1.2739],
        [-2.1722, -0.0961, -0.8458]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 0
********************
receiving microbatch 0 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 0, tensor([[0.3486, 0.5335, 0.6683, 0.2718],
        [0.4600, 0.4400, 0.4463, 0.4014],
        [0.3980, 0.4899, 0.4927, 0.4809],
        [0.4647, 0.5140, 0.4725, 0.3164],
        [0.4787, 0.5094, 0.4885, 0.2559],
        [0.4308, 0.6041, 0.4733, 0.3820],
        [0.4279, 0.7490, 0.3539, 0.5860],
        [0.3370, 0.4992, 0.6855, 0.2799]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.3486, 0.5335, 0.6683, 0.2718],
        [0.4600, 0.4400, 0.4463, 0.4014],
        [0.3980, 0.4899, 0.4927, 0.4809],
        [0.4647, 0.5140, 0.4725, 0.3164],
        [0.4787, 0.5094, 0.4885, 0.2559],
        [0.4308, 0.6041, 0.4733, 0.3820],
        [0.4279, 0.7490, 0.3539, 0.5860],
        [0.3370, 0.4992, 0.6855, 0.2799]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3486, 0.5335, 0.6683, 0.2718],
        [0.4600, 0.4400, 0.4463, 0.4014],
        [0.3980, 0.4899, 0.4927, 0.4809],
        [0.4647, 0.5140, 0.4725, 0.3164],
        [0.4787, 0.5094, 0.4885, 0.2559],
        [0.4308, 0.6041, 0.4733, 0.3820],
        [0.4279, 0.7490, 0.3539, 0.5860],
        [0.3370, 0.4992, 0.6855, 0.2799]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.3486, 0.5335, 0.6683, 0.2718],
        [0.4600, 0.4400, 0.4463, 0.4014],
        [0.3980, 0.4899, 0.4927, 0.4809],
        [0.4647, 0.5140, 0.4725, 0.3164],
        [0.4787, 0.5094, 0.4885, 0.2559],
        [0.4308, 0.6041, 0.4733, 0.3820],
        [0.4279, 0.7490, 0.3539, 0.5860],
        [0.3370, 0.4992, 0.6855, 0.2799]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.0407, -0.5962, -0.2791],
        [ 0.8230,  1.2474,  2.5904],
        [-1.1772, -1.1254,  0.1522],
        [-0.7693,  0.8680, -1.9455],
        [-0.3535, -1.0630, -0.4670],
        [-1.5249, -0.2342,  0.8203],
        [ 1.2828, -0.7455, -0.2576],
        [ 0.8965, -0.4163,  0.6373]], device='cuda:0')]
====================
schedule for this step is
[(1, 0), (0, 1)]
inputting microbatch 1 into partition 0
before moving to cuda:0: tensor([[-0.0407, -0.5962, -0.2791],
        [ 0.8230,  1.2474,  2.5904],
        [-1.1772, -1.1254,  0.1522],
        [-0.7693,  0.8680, -1.9455],
        [-0.3535, -1.0630, -0.4670],
        [-1.5249, -0.2342,  0.8203],
        [ 1.2828, -0.7455, -0.2576],
        [ 0.8965, -0.4163,  0.6373]], device='cuda:0')
after: tensor([[-0.0407, -0.5962, -0.2791],
        [ 0.8230,  1.2474,  2.5904],
        [-1.1772, -1.1254,  0.1522],
        [-0.7693,  0.8680, -1.9455],
        [-0.3535, -1.0630, -0.4670],
        [-1.5249, -0.2342,  0.8203],
        [ 1.2828, -0.7455, -0.2576],
        [ 0.8965, -0.4163,  0.6373]], device='cuda:0')
********************
observing microbatch 1
current batch shape is torch.Size([8, 3])
batch is tensor([[-0.0407, -0.5962, -0.2791],
        [ 0.8230,  1.2474,  2.5904],
        [-1.1772, -1.1254,  0.1522],
        [-0.7693,  0.8680, -1.9455],
        [-0.3535, -1.0630, -0.4670],
        [-1.5249, -0.2342,  0.8203],
        [ 1.2828, -0.7455, -0.2576],
        [ 0.8965, -0.4163,  0.6373]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 1
********************
inputting microbatch 0 into partition 1
before moving to cuda:1: tensor([[0.3486, 0.5335, 0.6683, 0.2718],
        [0.4600, 0.4400, 0.4463, 0.4014],
        [0.3980, 0.4899, 0.4927, 0.4809],
        [0.4647, 0.5140, 0.4725, 0.3164],
        [0.4787, 0.5094, 0.4885, 0.2559],
        [0.4308, 0.6041, 0.4733, 0.3820],
        [0.4279, 0.7490, 0.3539, 0.5860],
        [0.3370, 0.4992, 0.6855, 0.2799]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[ 3.6893e+19,  1.8728e+00,  3.6893e+19,  1.8728e+00],
        [-3.6893e+19,  1.8728e+00,  2.0000e+00,  1.8728e+00],
        [ 3.6893e+19,  1.8728e+00,  1.0842e-19,  1.8640e+00],
        [-0.0000e+00,  1.8628e+00,  0.0000e+00,  1.8735e+00],
        [ 1.0842e-19,  1.8743e+00, -2.0000e+00,  1.7879e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 0
current batch shape is torch.Size([8, 4])
batch is tensor([[ 3.6893e+19,  1.8728e+00,  3.6893e+19,  1.8728e+00],
        [-3.6893e+19,  1.8728e+00,  2.0000e+00,  1.8728e+00],
        [ 3.6893e+19,  1.8728e+00,  1.0842e-19,  1.8640e+00],
        [-0.0000e+00,  1.8628e+00,  0.0000e+00,  1.8735e+00],
        [ 1.0842e-19,  1.8743e+00, -2.0000e+00,  1.7879e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 0
********************
receiving microbatch 1 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 1, tensor([[0.4294, 0.5844, 0.4571, 0.4248],
        [0.6113, 0.3529, 0.2850, 0.3316],
        [0.3971, 0.5312, 0.5039, 0.4473],
        [0.3566, 0.5862, 0.6782, 0.2217],
        [0.4029, 0.6044, 0.4771, 0.4592],
        [0.4252, 0.4286, 0.5255, 0.3532],
        [0.4782, 0.6475, 0.3429, 0.4973],
        [0.5068, 0.5599, 0.3316, 0.4665]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 1
result is tensor([[0.4294, 0.5844, 0.4571, 0.4248],
        [0.6113, 0.3529, 0.2850, 0.3316],
        [0.3971, 0.5312, 0.5039, 0.4473],
        [0.3566, 0.5862, 0.6782, 0.2217],
        [0.4029, 0.6044, 0.4771, 0.4592],
        [0.4252, 0.4286, 0.5255, 0.3532],
        [0.4782, 0.6475, 0.3429, 0.4973],
        [0.5068, 0.5599, 0.3316, 0.4665]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4294, 0.5844, 0.4571, 0.4248],
        [0.6113, 0.3529, 0.2850, 0.3316],
        [0.3971, 0.5312, 0.5039, 0.4473],
        [0.3566, 0.5862, 0.6782, 0.2217],
        [0.4029, 0.6044, 0.4771, 0.4592],
        [0.4252, 0.4286, 0.5255, 0.3532],
        [0.4782, 0.6475, 0.3429, 0.4973],
        [0.5068, 0.5599, 0.3316, 0.4665]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 0 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 0, tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
        [1.0000, 0.9959, 0.9904, 0.9902, 0.9902],
        [1.0000, 0.9700, 0.9317, 0.9308, 0.9313],
        [1.0000, 0.8668, 0.7293, 0.7293, 0.7293],
        [1.0000, 0.8668, 0.7293, 0.7293, 0.7293],
        [1.0000, 0.8668, 0.7293, 0.7293, 0.7293]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
        [1.0000, 0.9959, 0.9904, 0.9902, 0.9902],
        [1.0000, 0.9700, 0.9317, 0.9308, 0.9313],
        [1.0000, 0.8668, 0.7293, 0.7293, 0.7293],
        [1.0000, 0.8668, 0.7293, 0.7293, 0.7293],
        [1.0000, 0.8668, 0.7293, 0.7293, 0.7293]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
        [1.0000, 0.9959, 0.9904, 0.9902, 0.9902],
        [1.0000, 0.9700, 0.9317, 0.9308, 0.9313],
        [1.0000, 0.8668, 0.7293, 0.7293, 0.7293],
        [1.0000, 0.8668, 0.7293, 0.7293, 0.7293],
        [1.0000, 0.8668, 0.7293, 0.7293, 0.7293]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
        [1.0000, 0.9959, 0.9904, 0.9902, 0.9902],
        [1.0000, 0.9700, 0.9317, 0.9308, 0.9313],
        [1.0000, 0.8668, 0.7293, 0.7293, 0.7293],
        [1.0000, 0.8668, 0.7293, 0.7293, 0.7293],
        [1.0000, 0.8668, 0.7293, 0.7293, 0.7293]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4294, 0.5844, 0.4571, 0.4248],
        [0.6113, 0.3529, 0.2850, 0.3316],
        [0.3971, 0.5312, 0.5039, 0.4473],
        [0.3566, 0.5862, 0.6782, 0.2217],
        [0.4029, 0.6044, 0.4771, 0.4592],
        [0.4252, 0.4286, 0.5255, 0.3532],
        [0.4782, 0.6475, 0.3429, 0.4973],
        [0.5068, 0.5599, 0.3316, 0.4665]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)]
====================
schedule for this step is
[(1, 1)]
inputting microbatch 1 into partition 1
before moving to cuda:1: tensor([[0.4294, 0.5844, 0.4571, 0.4248],
        [0.6113, 0.3529, 0.2850, 0.3316],
        [0.3971, 0.5312, 0.5039, 0.4473],
        [0.3566, 0.5862, 0.6782, 0.2217],
        [0.4029, 0.6044, 0.4771, 0.4592],
        [0.4252, 0.4286, 0.5255, 0.3532],
        [0.4782, 0.6475, 0.3429, 0.4973],
        [0.5068, 0.5599, 0.3316, 0.4665]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[1.0000, 1.0000, 1.0000, 1.0000],
        [1.0000, 1.0000, 1.0000, 1.0000],
        [1.0000, 1.0000, 1.0000, 1.0000],
        [0.9959, 0.9904, 0.9902, 0.9902],
        [1.0000, 0.9700, 0.9317, 0.9308],
        [0.9313, 1.0000, 0.8668, 0.7293],
        [0.7293, 0.7293, 1.0000, 0.8668],
        [0.7293, 0.7293, 0.7293, 1.0000]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 1
current batch shape is torch.Size([8, 4])
batch is tensor([[1.0000, 1.0000, 1.0000, 1.0000],
        [1.0000, 1.0000, 1.0000, 1.0000],
        [1.0000, 1.0000, 1.0000, 1.0000],
        [0.9959, 0.9904, 0.9902, 0.9902],
        [1.0000, 0.9700, 0.9317, 0.9308],
        [0.9313, 1.0000, 0.8668, 0.7293],
        [0.7293, 0.7293, 1.0000, 0.8668],
        [0.7293, 0.7293, 0.7293, 1.0000]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 1
********************
receiving microbatch 1 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 1, tensor([[1.0000, 0.9969, 0.9925, 0.9924, 0.9924],
        [1.0000, 0.9969, 0.9925, 0.9924, 0.9924],
        [1.0000, 0.9969, 0.9925, 0.9924, 0.9924],
        [1.0000, 0.9968, 0.9922, 0.9922, 0.9922],
        [1.0000, 0.9964, 0.9911, 0.9911, 0.9911],
        [1.0000, 0.9951, 0.9881, 0.9880, 0.9880],
        [1.0000, 0.9941, 0.9856, 0.9855, 0.9855],
        [1.0000, 0.9932, 0.9836, 0.9835, 0.9835]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 1
result is tensor([[1.0000, 0.9969, 0.9925, 0.9924, 0.9924],
        [1.0000, 0.9969, 0.9925, 0.9924, 0.9924],
        [1.0000, 0.9969, 0.9925, 0.9924, 0.9924],
        [1.0000, 0.9968, 0.9922, 0.9922, 0.9922],
        [1.0000, 0.9964, 0.9911, 0.9911, 0.9911],
        [1.0000, 0.9951, 0.9881, 0.9880, 0.9880],
        [1.0000, 0.9941, 0.9856, 0.9855, 0.9855],
        [1.0000, 0.9932, 0.9836, 0.9835, 0.9835]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[1.0000, 0.9969, 0.9925, 0.9924, 0.9924],
        [1.0000, 0.9969, 0.9925, 0.9924, 0.9924],
        [1.0000, 0.9969, 0.9925, 0.9924, 0.9924],
        [1.0000, 0.9968, 0.9922, 0.9922, 0.9922],
        [1.0000, 0.9964, 0.9911, 0.9911, 0.9911],
        [1.0000, 0.9951, 0.9881, 0.9880, 0.9880],
        [1.0000, 0.9941, 0.9856, 0.9855, 0.9855],
        [1.0000, 0.9932, 0.9836, 0.9835, 0.9835]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
        [1.0000, 0.9959, 0.9904, 0.9902, 0.9902],
        [1.0000, 0.9700, 0.9317, 0.9308, 0.9313],
        [1.0000, 0.8668, 0.7293, 0.7293, 0.7293],
        [1.0000, 0.8668, 0.7293, 0.7293, 0.7293],
        [1.0000, 0.8668, 0.7293, 0.7293, 0.7293]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[1.0000, 0.9969, 0.9925, 0.9924, 0.9924],
        [1.0000, 0.9969, 0.9925, 0.9924, 0.9924],
        [1.0000, 0.9969, 0.9925, 0.9924, 0.9924],
        [1.0000, 0.9968, 0.9922, 0.9922, 0.9922],
        [1.0000, 0.9964, 0.9911, 0.9911, 0.9911],
        [1.0000, 0.9951, 0.9881, 0.9880, 0.9880],
        [1.0000, 0.9941, 0.9856, 0.9855, 0.9855],
        [1.0000, 0.9932, 0.9836, 0.9835, 0.9835]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)]
finished running cycle
_____________________________ test_forward_0[8-32] _____________________________

batch_size = 32, split_size = 8

    @pytest.mark.a4_2_2
    @pytest.mark.parametrize("batch_size", [1, 16, 32, 64])
    @pytest.mark.parametrize("split_size", [1, 2, 4, 8, 16])
    # @pytest.mark.parametrize("batch_size", [1])
    # @pytest.mark.parametrize("split_size", [1, 2])
    def test_forward_0(batch_size, split_size):
        model = nn.Sequential(
            nn.Linear(3, 4).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
            nn.Linear(4, 5).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
        )
    
        x = torch.randn(batch_size, 3).to('cuda:0')
        y0 = model(x).to('cpu')
    
        # move the last two layer to another device
        model[-2] = model[-2].to('cuda:1')
        model[-1] = WithDevice(nn.Sigmoid(), 'cuda:1')
        pipe = Pipe(model, split_size=split_size)
        y1 = pipe(x).to('cpu')
>       assert torch.allclose(y0, y1)
E       assert False
E        +  where False = <built-in method allclose of type object at 0x153da78cb760>(tensor([[0.5655, 0.3866, 0.5149, 0.3043, 0.6026],\n        [0.4788, 0.4209, 0.4949, 0.3004, 0.5383],\n        [0.5512, 0.4018, 0.4963, 0.3510, 0.5614],\n        [0.5739, 0.3747, 0.5340, 0.2984, 0.6011],\n        [0.5210, 0.4096, 0.4949, 0.3251, 0.5555],\n        [0.5601, 0.3864, 0.5185, 0.3073, 0.5914],\n        [0.5573, 0.3955, 0.5029, 0.3189, 0.5910],\n        [0.5664, 0.3718, 0.5438, 0.2857, 0.5985],\n        [0.5166, 0.4077, 0.5002, 0.3135, 0.5577],\n        [0.5129, 0.4118, 0.4951, 0.3225, 0.5501],\n        [0.5463, 0.3851, 0.5293, 0.3004, 0.5767],\n        [0.5311, 0.3998, 0.5079, 0.3118, 0.5673],\n        [0.5465, 0.4059, 0.4907, 0.3496, 0.5627],\n        [0.5187, 0.4110, 0.4954, 0.3360, 0.5440],\n        [0.5663, 0.3836, 0.5214, 0.3140, 0.5893],\n        [0.5463, 0.3891, 0.5204, 0.3029, 0.5806],\n        [0.5220, 0.3942, 0.5236, 0.2885, 0.5693],\n        [0.5261, 0.3977, 0.5161, 0.3112, 0.5575],\n        [0.5772, 0.3842, 0.5162, 0.3301, 0.5904],\n        [0.4990, 0.4104, 0.5052, 0.3040, 0.5466],\n        [0.5761, 0.3818, 0.5207, 0.3207, 0.5938],\n        [0.5449, 0.3900, 0.5216, 0.3217, 0.5612],\n        [0.5840, 0.3844, 0.5136, 0.3439, 0.5875],\n        [0.5570, 0.3837, 0.5252, 0.2996, 0.5904],\n        [0.5554, 0.3940, 0.5074, 0.3271, 0.5778],\n        [0.5078, 0.4079, 0.5059, 0.3120, 0.5471],\n        [0.5501, 0.3920, 0.5118, 0.3011, 0.5925],\n        [0.5581, 0.3925, 0.5092, 0.3286, 0.5776],\n        [0.5254, 0.4029, 0.5038, 0.3044, 0.5716],\n        [0.5720, 0.3813, 0.5234, 0.3188, 0.5889],\n        [0.5272, 0.4114, 0.4886, 0.3351, 0.5576],\n        [0.5364, 0.3912, 0.5220, 0.3000, 0.5729]], grad_fn=<ToCopyBackward0>), tensor([[0.9770, 0.9769, 0.9462, 0.9914, 0.9910],\n        [1.0000, 0.9769, 1.0000, 1.0000, 1.0000],\n        [0.9968, 0.9968, 0.9462, 0.9988, 0.9988],\n        [0.9770, 0.9769, 0.9461, 0.9913, 0.9910],\n        [1.0000, 1.0000, 0.9924, 1.0000, 1.0000],\n        [0.9770, 0.9769, 0.9462, 0.9914, 0.9910],\n        [1.0000, 0.9769, 1.0000, 1.0000, 1.0000],\n        [0.9968, 0.9968, 0.9462, 0.9988, 0.9988],\n        [0.9800, 0.9786, 0.9493, 0.9925, 0.9922],\n        [0.9814, 0.9794, 0.9512, 0.9930, 0.9928],\n        [0.9819, 0.9796, 0.9519, 0.9932, 0.9930],\n        [0.9806, 0.9785, 0.9511, 0.9927, 0.9925],\n        [0.9803, 0.9780, 0.9514, 0.9926, 0.9923],\n        [0.9819, 0.9796, 0.9519, 0.9932, 0.9930],\n        [0.9802, 0.9791, 0.9486, 0.9926, 0.9923],\n        [0.9813, 0.9792, 0.9512, 0.9930, 0.9927],\n        [0.9802, 0.9787, 0.9495, 0.9926, 0.9923],\n        [0.9802, 0.9791, 0.9490, 0.9926, 0.9923],\n        [0.9811, 0.9793, 0.9505, 0.9929, 0.9927],\n        [0.9805, 0.9785, 0.9510, 0.9927, 0.9924],\n        [0.9804, 0.9781, 0.9516, 0.9927, 0.9924],\n        [0.9802, 0.9787, 0.9496, 0.9926, 0.9923],\n        [0.9803, 0.9791, 0.9490, 0.9926, 0.9924],\n        [0.9810, 0.9793, 0.9503, 0.9929, 0.9926],\n        [0.9802, 0.9787, 0.9495, 0.9926, 0.9923],\n        [0.9802, 0.9790, 0.9489, 0.9926, 0.9923],\n        [0.9810, 0.9793, 0.9504, 0.9929, 0.9926],\n        [0.9805, 0.9784, 0.9510, 0.9927, 0.9924],\n        [0.9804, 0.9781, 0.9516, 0.9927, 0.9924],\n        [0.9802, 0.9787, 0.9496, 0.9926, 0.9923],\n        [0.9802, 0.9790, 0.9489, 0.9926, 0.9923],\n        [0.9810, 0.9793, 0.9503, 0.9929, 0.9926]], grad_fn=<ToCopyBackward0>))
E        +    where <built-in method allclose of type object at 0x153da78cb760> = torch.allclose

batch_size = 32
model      = Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
  (2): Linear(in_features=4, out_features=5, bias=True)
  (3): WithDevice(
    (_module): Sigmoid()
  )
)
pipe       = Pipe(
  (partitions): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=3, out_features=4, bias=True)
   ...inear(in_features=4, out_features=5, bias=True)
      (1): WithDevice(
        (_module): Sigmoid()
      )
    )
  )
)
split_size = 8
x          = tensor([[ 2.0490, -1.3601, -0.0137],
        [ 0.3198,  1.7181, -1.8082],
        [-2.1263, -1.5490, -0.9167],
       ...-0.7655, -0.6347,  0.9851],
        [-0.4316, -1.0162, -1.7684],
        [-0.1329,  1.0769,  0.5670]], device='cuda:0')
y0         = tensor([[0.5655, 0.3866, 0.5149, 0.3043, 0.6026],
        [0.4788, 0.4209, 0.4949, 0.3004, 0.5383],
        [0.5512, 0...[0.5272, 0.4114, 0.4886, 0.3351, 0.5576],
        [0.5364, 0.3912, 0.5220, 0.3000, 0.5729]], grad_fn=<ToCopyBackward0>)
y1         = tensor([[0.9770, 0.9769, 0.9462, 0.9914, 0.9910],
        [1.0000, 0.9769, 1.0000, 1.0000, 1.0000],
        [0.9968, 0...[0.9802, 0.9790, 0.9489, 0.9926, 0.9923],
        [0.9810, 0.9793, 0.9503, 0.9929, 0.9926]], grad_fn=<ToCopyBackward0>)

tests/test_pipeline.py:83: AssertionError
----------------------------- Captured stdout call -----------------------------
in init
ModuleList(
  (0): Sequential(
    (0): Linear(in_features=3, out_features=4, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
  (1): Sequential(
    (0): Linear(in_features=4, out_features=5, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
)
[device(type='cuda', index=0), device(type='cuda', index=1)]
before running the cycle
[tensor([[ 2.0490, -1.3601, -0.0137],
        [ 0.3198,  1.7181, -1.8082],
        [-2.1263, -1.5490, -0.9167],
        [ 0.5205, -0.0290,  1.5473],
        [-0.4502, -0.1931, -1.3564],
        [ 0.7441, -0.5584,  0.3394],
        [ 1.4444, -1.8264, -0.8451],
        [ 0.3840,  1.2019,  2.2311]], device='cuda:0'), tensor([[ 0.0195,  0.3921, -1.0261],
        [-0.6169,  0.2099, -1.4376],
        [-0.6465,  1.2048,  1.1929],
        [-0.0106,  0.2675, -0.3990],
        [-1.2853, -1.8543, -1.4496],
        [-2.1453, -0.0304, -1.5709],
        [-0.1477, -0.5632,  0.6938],
        [ 0.2286,  0.4114,  0.4694]], device='cuda:0'), tensor([[ 0.3843,  1.8223,  0.5087],
        [-1.2779,  1.2813,  0.2524],
        [-0.6761, -1.6585,  0.4575],
        [-0.3703,  1.6241, -0.8280],
        [-0.2497, -1.2311,  0.6891],
        [-2.5869,  0.9905,  1.0545],
        [-1.6680, -2.2586,  0.5339],
        [ 0.5953,  0.1727,  0.8009]], device='cuda:0'), tensor([[-0.3851, -1.0455, -0.2624],
        [-1.0019,  1.3147, -0.6903],
        [ 1.8642, -0.6791, -0.2830],
        [-0.6980, -1.0039, -0.0714],
        [ 1.1256,  0.0997, -0.8433],
        [-0.7655, -0.6347,  0.9851],
        [-0.4316, -1.0162, -1.7684],
        [-0.1329,  1.0769,  0.5670]], device='cuda:0')]
====================
schedule for this step is
[(0, 0)]
inputting microbatch 0 into partition 0
before moving to cuda:0: tensor([[ 2.0490, -1.3601, -0.0137],
        [ 0.3198,  1.7181, -1.8082],
        [-2.1263, -1.5490, -0.9167],
        [ 0.5205, -0.0290,  1.5473],
        [-0.4502, -0.1931, -1.3564],
        [ 0.7441, -0.5584,  0.3394],
        [ 1.4444, -1.8264, -0.8451],
        [ 0.3840,  1.2019,  2.2311]], device='cuda:0')
after: tensor([[ 2.0490, -1.3601, -0.0137],
        [ 0.3198,  1.7181, -1.8082],
        [-2.1263, -1.5490, -0.9167],
        [ 0.5205, -0.0290,  1.5473],
        [-0.4502, -0.1931, -1.3564],
        [ 0.7441, -0.5584,  0.3394],
        [ 1.4444, -1.8264, -0.8451],
        [ 0.3840,  1.2019,  2.2311]], device='cuda:0')
********************
observing microbatch 0
current batch shape is torch.Size([8, 3])
batch is tensor([[ 2.0490, -1.3601, -0.0137],
        [ 0.3198,  1.7181, -1.8082],
        [-2.1263, -1.5490, -0.9167],
        [ 0.5205, -0.0290,  1.5473],
        [-0.4502, -0.1931, -1.3564],
        [ 0.7441, -0.5584,  0.3394],
        [ 1.4444, -1.8264, -0.8451],
        [ 0.3840,  1.2019,  2.2311]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 0
********************
receiving microbatch 0 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 0, tensor([[0.7164, 0.5111, 0.2604, 0.5249],
        [0.2178, 0.4044, 0.5974, 0.7103],
        [0.2111, 0.7267, 0.2696, 0.5668],
        [0.7171, 0.6463, 0.3086, 0.6467],
        [0.2574, 0.5546, 0.4005, 0.6189],
        [0.6086, 0.5862, 0.3083, 0.5986],
        [0.5703, 0.5218, 0.2519, 0.4950],
        [0.7423, 0.6466, 0.3881, 0.7223]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.7164, 0.5111, 0.2604, 0.5249],
        [0.2178, 0.4044, 0.5974, 0.7103],
        [0.2111, 0.7267, 0.2696, 0.5668],
        [0.7171, 0.6463, 0.3086, 0.6467],
        [0.2574, 0.5546, 0.4005, 0.6189],
        [0.6086, 0.5862, 0.3083, 0.5986],
        [0.5703, 0.5218, 0.2519, 0.4950],
        [0.7423, 0.6466, 0.3881, 0.7223]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.7164, 0.5111, 0.2604, 0.5249],
        [0.2178, 0.4044, 0.5974, 0.7103],
        [0.2111, 0.7267, 0.2696, 0.5668],
        [0.7171, 0.6463, 0.3086, 0.6467],
        [0.2574, 0.5546, 0.4005, 0.6189],
        [0.6086, 0.5862, 0.3083, 0.5986],
        [0.5703, 0.5218, 0.2519, 0.4950],
        [0.7423, 0.6466, 0.3881, 0.7223]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.7164, 0.5111, 0.2604, 0.5249],
        [0.2178, 0.4044, 0.5974, 0.7103],
        [0.2111, 0.7267, 0.2696, 0.5668],
        [0.7171, 0.6463, 0.3086, 0.6467],
        [0.2574, 0.5546, 0.4005, 0.6189],
        [0.6086, 0.5862, 0.3083, 0.5986],
        [0.5703, 0.5218, 0.2519, 0.4950],
        [0.7423, 0.6466, 0.3881, 0.7223]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.0195,  0.3921, -1.0261],
        [-0.6169,  0.2099, -1.4376],
        [-0.6465,  1.2048,  1.1929],
        [-0.0106,  0.2675, -0.3990],
        [-1.2853, -1.8543, -1.4496],
        [-2.1453, -0.0304, -1.5709],
        [-0.1477, -0.5632,  0.6938],
        [ 0.2286,  0.4114,  0.4694]], device='cuda:0'), tensor([[ 0.3843,  1.8223,  0.5087],
        [-1.2779,  1.2813,  0.2524],
        [-0.6761, -1.6585,  0.4575],
        [-0.3703,  1.6241, -0.8280],
        [-0.2497, -1.2311,  0.6891],
        [-2.5869,  0.9905,  1.0545],
        [-1.6680, -2.2586,  0.5339],
        [ 0.5953,  0.1727,  0.8009]], device='cuda:0'), tensor([[-0.3851, -1.0455, -0.2624],
        [-1.0019,  1.3147, -0.6903],
        [ 1.8642, -0.6791, -0.2830],
        [-0.6980, -1.0039, -0.0714],
        [ 1.1256,  0.0997, -0.8433],
        [-0.7655, -0.6347,  0.9851],
        [-0.4316, -1.0162, -1.7684],
        [-0.1329,  1.0769,  0.5670]], device='cuda:0')]
====================
schedule for this step is
[(1, 0), (0, 1)]
inputting microbatch 1 into partition 0
before moving to cuda:0: tensor([[ 0.0195,  0.3921, -1.0261],
        [-0.6169,  0.2099, -1.4376],
        [-0.6465,  1.2048,  1.1929],
        [-0.0106,  0.2675, -0.3990],
        [-1.2853, -1.8543, -1.4496],
        [-2.1453, -0.0304, -1.5709],
        [-0.1477, -0.5632,  0.6938],
        [ 0.2286,  0.4114,  0.4694]], device='cuda:0')
after: tensor([[ 0.0195,  0.3921, -1.0261],
        [-0.6169,  0.2099, -1.4376],
        [-0.6465,  1.2048,  1.1929],
        [-0.0106,  0.2675, -0.3990],
        [-1.2853, -1.8543, -1.4496],
        [-2.1453, -0.0304, -1.5709],
        [-0.1477, -0.5632,  0.6938],
        [ 0.2286,  0.4114,  0.4694]], device='cuda:0')
********************
observing microbatch 1
current batch shape is torch.Size([8, 3])
batch is tensor([[ 0.0195,  0.3921, -1.0261],
        [-0.6169,  0.2099, -1.4376],
        [-0.6465,  1.2048,  1.1929],
        [-0.0106,  0.2675, -0.3990],
        [-1.2853, -1.8543, -1.4496],
        [-2.1453, -0.0304, -1.5709],
        [-0.1477, -0.5632,  0.6938],
        [ 0.2286,  0.4114,  0.4694]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 1
********************
inputting microbatch 0 into partition 1
before moving to cuda:1: tensor([[0.7164, 0.5111, 0.2604, 0.5249],
        [0.2178, 0.4044, 0.5974, 0.7103],
        [0.2111, 0.7267, 0.2696, 0.5668],
        [0.7171, 0.6463, 0.3086, 0.6467],
        [0.2574, 0.5546, 0.4005, 0.6189],
        [0.6086, 0.5862, 0.3083, 0.5986],
        [0.5703, 0.5218, 0.2519, 0.4950],
        [0.7423, 0.6466, 0.3881, 0.7223]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.0000e+00, 1.8750e+00, 0.0000e+00, 1.8742e+00],
        [-0.0000e+00, 1.8731e+00, 3.6893e+19, 1.8731e+00],
        [2.0000e+00, 1.8731e+00, 0.0000e+00, 1.8750e+00],
        [0.0000e+00, 1.8742e+00, -0.0000e+00, 1.8731e+00],
        [3.6893e+19, 1.8731e+00, 2.0000e+00, 1.8731e+00],
        [0.0000e+00, 1.8750e+00, 0.0000e+00, 1.8742e+00],
        [-0.0000e+00, 1.8731e+00, 3.6893e+19, 1.8731e+00],
        [2.0000e+00, 1.8731e+00, 0.0000e+00, 1.8750e+00]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 0
current batch shape is torch.Size([8, 4])
batch is tensor([[0.0000e+00, 1.8750e+00, 0.0000e+00, 1.8742e+00],
        [-0.0000e+00, 1.8731e+00, 3.6893e+19, 1.8731e+00],
        [2.0000e+00, 1.8731e+00, 0.0000e+00, 1.8750e+00],
        [0.0000e+00, 1.8742e+00, -0.0000e+00, 1.8731e+00],
        [3.6893e+19, 1.8731e+00, 2.0000e+00, 1.8731e+00],
        [0.0000e+00, 1.8750e+00, 0.0000e+00, 1.8742e+00],
        [-0.0000e+00, 1.8731e+00, 3.6893e+19, 1.8731e+00],
        [2.0000e+00, 1.8731e+00, 0.0000e+00, 1.8750e+00]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 0
********************
receiving microbatch 1 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 1, tensor([[0.3168, 0.5198, 0.4419, 0.6500],
        [0.2248, 0.5455, 0.4404, 0.6441],
        [0.5115, 0.6547, 0.4274, 0.7254],
        [0.3975, 0.5623, 0.4055, 0.6501],
        [0.2302, 0.6636, 0.2652, 0.5285],
        [0.1274, 0.6447, 0.4210, 0.6505],
        [0.5642, 0.6610, 0.2945, 0.6156],
        [0.5356, 0.5900, 0.3852, 0.6643]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 1
result is tensor([[0.3168, 0.5198, 0.4419, 0.6500],
        [0.2248, 0.5455, 0.4404, 0.6441],
        [0.5115, 0.6547, 0.4274, 0.7254],
        [0.3975, 0.5623, 0.4055, 0.6501],
        [0.2302, 0.6636, 0.2652, 0.5285],
        [0.1274, 0.6447, 0.4210, 0.6505],
        [0.5642, 0.6610, 0.2945, 0.6156],
        [0.5356, 0.5900, 0.3852, 0.6643]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.3168, 0.5198, 0.4419, 0.6500],
        [0.2248, 0.5455, 0.4404, 0.6441],
        [0.5115, 0.6547, 0.4274, 0.7254],
        [0.3975, 0.5623, 0.4055, 0.6501],
        [0.2302, 0.6636, 0.2652, 0.5285],
        [0.1274, 0.6447, 0.4210, 0.6505],
        [0.5642, 0.6610, 0.2945, 0.6156],
        [0.5356, 0.5900, 0.3852, 0.6643]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 0 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 0, tensor([[0.9770, 0.9769, 0.9462, 0.9914, 0.9910],
        [1.0000, 0.9769, 1.0000, 1.0000, 1.0000],
        [0.9968, 0.9968, 0.9462, 0.9988, 0.9988],
        [0.9770, 0.9769, 0.9461, 0.9913, 0.9910],
        [1.0000, 1.0000, 0.9924, 1.0000, 1.0000],
        [0.9770, 0.9769, 0.9462, 0.9914, 0.9910],
        [1.0000, 0.9769, 1.0000, 1.0000, 1.0000],
        [0.9968, 0.9968, 0.9462, 0.9988, 0.9988]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.9770, 0.9769, 0.9462, 0.9914, 0.9910],
        [1.0000, 0.9769, 1.0000, 1.0000, 1.0000],
        [0.9968, 0.9968, 0.9462, 0.9988, 0.9988],
        [0.9770, 0.9769, 0.9461, 0.9913, 0.9910],
        [1.0000, 1.0000, 0.9924, 1.0000, 1.0000],
        [0.9770, 0.9769, 0.9462, 0.9914, 0.9910],
        [1.0000, 0.9769, 1.0000, 1.0000, 1.0000],
        [0.9968, 0.9968, 0.9462, 0.9988, 0.9988]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9770, 0.9769, 0.9462, 0.9914, 0.9910],
        [1.0000, 0.9769, 1.0000, 1.0000, 1.0000],
        [0.9968, 0.9968, 0.9462, 0.9988, 0.9988],
        [0.9770, 0.9769, 0.9461, 0.9913, 0.9910],
        [1.0000, 1.0000, 0.9924, 1.0000, 1.0000],
        [0.9770, 0.9769, 0.9462, 0.9914, 0.9910],
        [1.0000, 0.9769, 1.0000, 1.0000, 1.0000],
        [0.9968, 0.9968, 0.9462, 0.9988, 0.9988]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9770, 0.9769, 0.9462, 0.9914, 0.9910],
        [1.0000, 0.9769, 1.0000, 1.0000, 1.0000],
        [0.9968, 0.9968, 0.9462, 0.9988, 0.9988],
        [0.9770, 0.9769, 0.9461, 0.9913, 0.9910],
        [1.0000, 1.0000, 0.9924, 1.0000, 1.0000],
        [0.9770, 0.9769, 0.9462, 0.9914, 0.9910],
        [1.0000, 0.9769, 1.0000, 1.0000, 1.0000],
        [0.9968, 0.9968, 0.9462, 0.9988, 0.9988]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.3168, 0.5198, 0.4419, 0.6500],
        [0.2248, 0.5455, 0.4404, 0.6441],
        [0.5115, 0.6547, 0.4274, 0.7254],
        [0.3975, 0.5623, 0.4055, 0.6501],
        [0.2302, 0.6636, 0.2652, 0.5285],
        [0.1274, 0.6447, 0.4210, 0.6505],
        [0.5642, 0.6610, 0.2945, 0.6156],
        [0.5356, 0.5900, 0.3852, 0.6643]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.3843,  1.8223,  0.5087],
        [-1.2779,  1.2813,  0.2524],
        [-0.6761, -1.6585,  0.4575],
        [-0.3703,  1.6241, -0.8280],
        [-0.2497, -1.2311,  0.6891],
        [-2.5869,  0.9905,  1.0545],
        [-1.6680, -2.2586,  0.5339],
        [ 0.5953,  0.1727,  0.8009]], device='cuda:0'), tensor([[-0.3851, -1.0455, -0.2624],
        [-1.0019,  1.3147, -0.6903],
        [ 1.8642, -0.6791, -0.2830],
        [-0.6980, -1.0039, -0.0714],
        [ 1.1256,  0.0997, -0.8433],
        [-0.7655, -0.6347,  0.9851],
        [-0.4316, -1.0162, -1.7684],
        [-0.1329,  1.0769,  0.5670]], device='cuda:0')]
====================
schedule for this step is
[(2, 0), (1, 1)]
inputting microbatch 2 into partition 0
before moving to cuda:0: tensor([[ 0.3843,  1.8223,  0.5087],
        [-1.2779,  1.2813,  0.2524],
        [-0.6761, -1.6585,  0.4575],
        [-0.3703,  1.6241, -0.8280],
        [-0.2497, -1.2311,  0.6891],
        [-2.5869,  0.9905,  1.0545],
        [-1.6680, -2.2586,  0.5339],
        [ 0.5953,  0.1727,  0.8009]], device='cuda:0')
after: tensor([[ 0.3843,  1.8223,  0.5087],
        [-1.2779,  1.2813,  0.2524],
        [-0.6761, -1.6585,  0.4575],
        [-0.3703,  1.6241, -0.8280],
        [-0.2497, -1.2311,  0.6891],
        [-2.5869,  0.9905,  1.0545],
        [-1.6680, -2.2586,  0.5339],
        [ 0.5953,  0.1727,  0.8009]], device='cuda:0')
********************
observing microbatch 2
current batch shape is torch.Size([8, 3])
batch is tensor([[ 0.3843,  1.8223,  0.5087],
        [-1.2779,  1.2813,  0.2524],
        [-0.6761, -1.6585,  0.4575],
        [-0.3703,  1.6241, -0.8280],
        [-0.2497, -1.2311,  0.6891],
        [-2.5869,  0.9905,  1.0545],
        [-1.6680, -2.2586,  0.5339],
        [ 0.5953,  0.1727,  0.8009]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 2
********************
inputting microbatch 1 into partition 1
before moving to cuda:1: tensor([[0.3168, 0.5198, 0.4419, 0.6500],
        [0.2248, 0.5455, 0.4404, 0.6441],
        [0.5115, 0.6547, 0.4274, 0.7254],
        [0.3975, 0.5623, 0.4055, 0.6501],
        [0.2302, 0.6636, 0.2652, 0.5285],
        [0.1274, 0.6447, 0.4210, 0.6505],
        [0.5642, 0.6610, 0.2945, 0.6156],
        [0.5356, 0.5900, 0.3852, 0.6643]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9770, 0.9769, 0.9462, 0.9914],
        [0.9910, 1.0000, 0.9769, 1.0000],
        [1.0000, 1.0000, 0.9968, 0.9968],
        [0.9462, 0.9988, 0.9988, 0.9770],
        [0.9769, 0.9461, 0.9913, 0.9910],
        [1.0000, 1.0000, 0.9924, 1.0000],
        [1.0000, 0.9770, 0.9769, 0.9462],
        [0.9914, 0.9910, 1.0000, 0.9769]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 1
current batch shape is torch.Size([8, 4])
batch is tensor([[0.9770, 0.9769, 0.9462, 0.9914],
        [0.9910, 1.0000, 0.9769, 1.0000],
        [1.0000, 1.0000, 0.9968, 0.9968],
        [0.9462, 0.9988, 0.9988, 0.9770],
        [0.9769, 0.9461, 0.9913, 0.9910],
        [1.0000, 1.0000, 0.9924, 1.0000],
        [1.0000, 0.9770, 0.9769, 0.9462],
        [0.9914, 0.9910, 1.0000, 0.9769]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 1
********************
receiving microbatch 2 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 2, tensor([[0.5027, 0.5279, 0.5145, 0.7369],
        [0.3201, 0.6410, 0.4712, 0.7281],
        [0.5184, 0.7160, 0.2225, 0.5539],
        [0.2657, 0.5091, 0.5487, 0.7237],
        [0.5784, 0.6895, 0.2451, 0.5766],
        [0.3053, 0.7609, 0.4101, 0.7372],
        [0.4470, 0.7858, 0.1830, 0.5326],
        [0.6269, 0.5938, 0.3523, 0.6491]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 2
result is tensor([[0.5027, 0.5279, 0.5145, 0.7369],
        [0.3201, 0.6410, 0.4712, 0.7281],
        [0.5184, 0.7160, 0.2225, 0.5539],
        [0.2657, 0.5091, 0.5487, 0.7237],
        [0.5784, 0.6895, 0.2451, 0.5766],
        [0.3053, 0.7609, 0.4101, 0.7372],
        [0.4470, 0.7858, 0.1830, 0.5326],
        [0.6269, 0.5938, 0.3523, 0.6491]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5027, 0.5279, 0.5145, 0.7369],
        [0.3201, 0.6410, 0.4712, 0.7281],
        [0.5184, 0.7160, 0.2225, 0.5539],
        [0.2657, 0.5091, 0.5487, 0.7237],
        [0.5784, 0.6895, 0.2451, 0.5766],
        [0.3053, 0.7609, 0.4101, 0.7372],
        [0.4470, 0.7858, 0.1830, 0.5326],
        [0.6269, 0.5938, 0.3523, 0.6491]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 1 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 1, tensor([[0.9800, 0.9786, 0.9493, 0.9925, 0.9922],
        [0.9814, 0.9794, 0.9512, 0.9930, 0.9928],
        [0.9819, 0.9796, 0.9519, 0.9932, 0.9930],
        [0.9806, 0.9785, 0.9511, 0.9927, 0.9925],
        [0.9803, 0.9780, 0.9514, 0.9926, 0.9923],
        [0.9819, 0.9796, 0.9519, 0.9932, 0.9930],
        [0.9802, 0.9791, 0.9486, 0.9926, 0.9923],
        [0.9813, 0.9792, 0.9512, 0.9930, 0.9927]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 1
result is tensor([[0.9800, 0.9786, 0.9493, 0.9925, 0.9922],
        [0.9814, 0.9794, 0.9512, 0.9930, 0.9928],
        [0.9819, 0.9796, 0.9519, 0.9932, 0.9930],
        [0.9806, 0.9785, 0.9511, 0.9927, 0.9925],
        [0.9803, 0.9780, 0.9514, 0.9926, 0.9923],
        [0.9819, 0.9796, 0.9519, 0.9932, 0.9930],
        [0.9802, 0.9791, 0.9486, 0.9926, 0.9923],
        [0.9813, 0.9792, 0.9512, 0.9930, 0.9927]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9800, 0.9786, 0.9493, 0.9925, 0.9922],
        [0.9814, 0.9794, 0.9512, 0.9930, 0.9928],
        [0.9819, 0.9796, 0.9519, 0.9932, 0.9930],
        [0.9806, 0.9785, 0.9511, 0.9927, 0.9925],
        [0.9803, 0.9780, 0.9514, 0.9926, 0.9923],
        [0.9819, 0.9796, 0.9519, 0.9932, 0.9930],
        [0.9802, 0.9791, 0.9486, 0.9926, 0.9923],
        [0.9813, 0.9792, 0.9512, 0.9930, 0.9927]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9770, 0.9769, 0.9462, 0.9914, 0.9910],
        [1.0000, 0.9769, 1.0000, 1.0000, 1.0000],
        [0.9968, 0.9968, 0.9462, 0.9988, 0.9988],
        [0.9770, 0.9769, 0.9461, 0.9913, 0.9910],
        [1.0000, 1.0000, 0.9924, 1.0000, 1.0000],
        [0.9770, 0.9769, 0.9462, 0.9914, 0.9910],
        [1.0000, 0.9769, 1.0000, 1.0000, 1.0000],
        [0.9968, 0.9968, 0.9462, 0.9988, 0.9988]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9800, 0.9786, 0.9493, 0.9925, 0.9922],
        [0.9814, 0.9794, 0.9512, 0.9930, 0.9928],
        [0.9819, 0.9796, 0.9519, 0.9932, 0.9930],
        [0.9806, 0.9785, 0.9511, 0.9927, 0.9925],
        [0.9803, 0.9780, 0.9514, 0.9926, 0.9923],
        [0.9819, 0.9796, 0.9519, 0.9932, 0.9930],
        [0.9802, 0.9791, 0.9486, 0.9926, 0.9923],
        [0.9813, 0.9792, 0.9512, 0.9930, 0.9927]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5027, 0.5279, 0.5145, 0.7369],
        [0.3201, 0.6410, 0.4712, 0.7281],
        [0.5184, 0.7160, 0.2225, 0.5539],
        [0.2657, 0.5091, 0.5487, 0.7237],
        [0.5784, 0.6895, 0.2451, 0.5766],
        [0.3053, 0.7609, 0.4101, 0.7372],
        [0.4470, 0.7858, 0.1830, 0.5326],
        [0.6269, 0.5938, 0.3523, 0.6491]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.3851, -1.0455, -0.2624],
        [-1.0019,  1.3147, -0.6903],
        [ 1.8642, -0.6791, -0.2830],
        [-0.6980, -1.0039, -0.0714],
        [ 1.1256,  0.0997, -0.8433],
        [-0.7655, -0.6347,  0.9851],
        [-0.4316, -1.0162, -1.7684],
        [-0.1329,  1.0769,  0.5670]], device='cuda:0')]
====================
schedule for this step is
[(3, 0), (2, 1)]
inputting microbatch 3 into partition 0
before moving to cuda:0: tensor([[-0.3851, -1.0455, -0.2624],
        [-1.0019,  1.3147, -0.6903],
        [ 1.8642, -0.6791, -0.2830],
        [-0.6980, -1.0039, -0.0714],
        [ 1.1256,  0.0997, -0.8433],
        [-0.7655, -0.6347,  0.9851],
        [-0.4316, -1.0162, -1.7684],
        [-0.1329,  1.0769,  0.5670]], device='cuda:0')
after: tensor([[-0.3851, -1.0455, -0.2624],
        [-1.0019,  1.3147, -0.6903],
        [ 1.8642, -0.6791, -0.2830],
        [-0.6980, -1.0039, -0.0714],
        [ 1.1256,  0.0997, -0.8433],
        [-0.7655, -0.6347,  0.9851],
        [-0.4316, -1.0162, -1.7684],
        [-0.1329,  1.0769,  0.5670]], device='cuda:0')
********************
observing microbatch 3
current batch shape is torch.Size([8, 3])
batch is tensor([[-0.3851, -1.0455, -0.2624],
        [-1.0019,  1.3147, -0.6903],
        [ 1.8642, -0.6791, -0.2830],
        [-0.6980, -1.0039, -0.0714],
        [ 1.1256,  0.0997, -0.8433],
        [-0.7655, -0.6347,  0.9851],
        [-0.4316, -1.0162, -1.7684],
        [-0.1329,  1.0769,  0.5670]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 3
********************
inputting microbatch 2 into partition 1
before moving to cuda:1: tensor([[0.5027, 0.5279, 0.5145, 0.7369],
        [0.3201, 0.6410, 0.4712, 0.7281],
        [0.5184, 0.7160, 0.2225, 0.5539],
        [0.2657, 0.5091, 0.5487, 0.7237],
        [0.5784, 0.6895, 0.2451, 0.5766],
        [0.3053, 0.7609, 0.4101, 0.7372],
        [0.4470, 0.7858, 0.1830, 0.5326],
        [0.6269, 0.5938, 0.3523, 0.6491]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9800, 0.9786, 0.9493, 0.9925],
        [0.9922, 0.9814, 0.9794, 0.9512],
        [0.9930, 0.9928, 0.9819, 0.9796],
        [0.9519, 0.9932, 0.9930, 0.9806],
        [0.9785, 0.9511, 0.9927, 0.9925],
        [0.9803, 0.9780, 0.9514, 0.9926],
        [0.9923, 0.9819, 0.9796, 0.9519],
        [0.9932, 0.9930, 0.9802, 0.9791]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 2
current batch shape is torch.Size([8, 4])
batch is tensor([[0.9800, 0.9786, 0.9493, 0.9925],
        [0.9922, 0.9814, 0.9794, 0.9512],
        [0.9930, 0.9928, 0.9819, 0.9796],
        [0.9519, 0.9932, 0.9930, 0.9806],
        [0.9785, 0.9511, 0.9927, 0.9925],
        [0.9803, 0.9780, 0.9514, 0.9926],
        [0.9923, 0.9819, 0.9796, 0.9519],
        [0.9932, 0.9930, 0.9802, 0.9791]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 2
********************
receiving microbatch 3 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 3, tensor([[0.4267, 0.6425, 0.2888, 0.5790],
        [0.2384, 0.5706, 0.5131, 0.7173],
        [0.6441, 0.4814, 0.3219, 0.5671],
        [0.4184, 0.6697, 0.2852, 0.5885],
        [0.4622, 0.4681, 0.4096, 0.6190],
        [0.5417, 0.7130, 0.2785, 0.6237],
        [0.2404, 0.5617, 0.3439, 0.5639],
        [0.4848, 0.5936, 0.4414, 0.7062]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 3
result is tensor([[0.4267, 0.6425, 0.2888, 0.5790],
        [0.2384, 0.5706, 0.5131, 0.7173],
        [0.6441, 0.4814, 0.3219, 0.5671],
        [0.4184, 0.6697, 0.2852, 0.5885],
        [0.4622, 0.4681, 0.4096, 0.6190],
        [0.5417, 0.7130, 0.2785, 0.6237],
        [0.2404, 0.5617, 0.3439, 0.5639],
        [0.4848, 0.5936, 0.4414, 0.7062]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4267, 0.6425, 0.2888, 0.5790],
        [0.2384, 0.5706, 0.5131, 0.7173],
        [0.6441, 0.4814, 0.3219, 0.5671],
        [0.4184, 0.6697, 0.2852, 0.5885],
        [0.4622, 0.4681, 0.4096, 0.6190],
        [0.5417, 0.7130, 0.2785, 0.6237],
        [0.2404, 0.5617, 0.3439, 0.5639],
        [0.4848, 0.5936, 0.4414, 0.7062]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 2 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 2, tensor([[0.9802, 0.9787, 0.9495, 0.9926, 0.9923],
        [0.9802, 0.9791, 0.9490, 0.9926, 0.9923],
        [0.9811, 0.9793, 0.9505, 0.9929, 0.9927],
        [0.9805, 0.9785, 0.9510, 0.9927, 0.9924],
        [0.9804, 0.9781, 0.9516, 0.9927, 0.9924],
        [0.9802, 0.9787, 0.9496, 0.9926, 0.9923],
        [0.9803, 0.9791, 0.9490, 0.9926, 0.9924],
        [0.9810, 0.9793, 0.9503, 0.9929, 0.9926]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 2
result is tensor([[0.9802, 0.9787, 0.9495, 0.9926, 0.9923],
        [0.9802, 0.9791, 0.9490, 0.9926, 0.9923],
        [0.9811, 0.9793, 0.9505, 0.9929, 0.9927],
        [0.9805, 0.9785, 0.9510, 0.9927, 0.9924],
        [0.9804, 0.9781, 0.9516, 0.9927, 0.9924],
        [0.9802, 0.9787, 0.9496, 0.9926, 0.9923],
        [0.9803, 0.9791, 0.9490, 0.9926, 0.9924],
        [0.9810, 0.9793, 0.9503, 0.9929, 0.9926]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9802, 0.9787, 0.9495, 0.9926, 0.9923],
        [0.9802, 0.9791, 0.9490, 0.9926, 0.9923],
        [0.9811, 0.9793, 0.9505, 0.9929, 0.9927],
        [0.9805, 0.9785, 0.9510, 0.9927, 0.9924],
        [0.9804, 0.9781, 0.9516, 0.9927, 0.9924],
        [0.9802, 0.9787, 0.9496, 0.9926, 0.9923],
        [0.9803, 0.9791, 0.9490, 0.9926, 0.9924],
        [0.9810, 0.9793, 0.9503, 0.9929, 0.9926]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9770, 0.9769, 0.9462, 0.9914, 0.9910],
        [1.0000, 0.9769, 1.0000, 1.0000, 1.0000],
        [0.9968, 0.9968, 0.9462, 0.9988, 0.9988],
        [0.9770, 0.9769, 0.9461, 0.9913, 0.9910],
        [1.0000, 1.0000, 0.9924, 1.0000, 1.0000],
        [0.9770, 0.9769, 0.9462, 0.9914, 0.9910],
        [1.0000, 0.9769, 1.0000, 1.0000, 1.0000],
        [0.9968, 0.9968, 0.9462, 0.9988, 0.9988]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9800, 0.9786, 0.9493, 0.9925, 0.9922],
        [0.9814, 0.9794, 0.9512, 0.9930, 0.9928],
        [0.9819, 0.9796, 0.9519, 0.9932, 0.9930],
        [0.9806, 0.9785, 0.9511, 0.9927, 0.9925],
        [0.9803, 0.9780, 0.9514, 0.9926, 0.9923],
        [0.9819, 0.9796, 0.9519, 0.9932, 0.9930],
        [0.9802, 0.9791, 0.9486, 0.9926, 0.9923],
        [0.9813, 0.9792, 0.9512, 0.9930, 0.9927]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9802, 0.9787, 0.9495, 0.9926, 0.9923],
        [0.9802, 0.9791, 0.9490, 0.9926, 0.9923],
        [0.9811, 0.9793, 0.9505, 0.9929, 0.9927],
        [0.9805, 0.9785, 0.9510, 0.9927, 0.9924],
        [0.9804, 0.9781, 0.9516, 0.9927, 0.9924],
        [0.9802, 0.9787, 0.9496, 0.9926, 0.9923],
        [0.9803, 0.9791, 0.9490, 0.9926, 0.9924],
        [0.9810, 0.9793, 0.9503, 0.9929, 0.9926]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4267, 0.6425, 0.2888, 0.5790],
        [0.2384, 0.5706, 0.5131, 0.7173],
        [0.6441, 0.4814, 0.3219, 0.5671],
        [0.4184, 0.6697, 0.2852, 0.5885],
        [0.4622, 0.4681, 0.4096, 0.6190],
        [0.5417, 0.7130, 0.2785, 0.6237],
        [0.2404, 0.5617, 0.3439, 0.5639],
        [0.4848, 0.5936, 0.4414, 0.7062]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)]
====================
schedule for this step is
[(3, 1)]
inputting microbatch 3 into partition 1
before moving to cuda:1: tensor([[0.4267, 0.6425, 0.2888, 0.5790],
        [0.2384, 0.5706, 0.5131, 0.7173],
        [0.6441, 0.4814, 0.3219, 0.5671],
        [0.4184, 0.6697, 0.2852, 0.5885],
        [0.4622, 0.4681, 0.4096, 0.6190],
        [0.5417, 0.7130, 0.2785, 0.6237],
        [0.2404, 0.5617, 0.3439, 0.5639],
        [0.4848, 0.5936, 0.4414, 0.7062]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9802, 0.9787, 0.9495, 0.9926],
        [0.9923, 0.9802, 0.9791, 0.9490],
        [0.9926, 0.9923, 0.9811, 0.9793],
        [0.9505, 0.9929, 0.9927, 0.9805],
        [0.9785, 0.9510, 0.9927, 0.9924],
        [0.9804, 0.9781, 0.9516, 0.9927],
        [0.9924, 0.9802, 0.9787, 0.9496],
        [0.9926, 0.9923, 0.9803, 0.9791]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 3
current batch shape is torch.Size([8, 4])
batch is tensor([[0.9802, 0.9787, 0.9495, 0.9926],
        [0.9923, 0.9802, 0.9791, 0.9490],
        [0.9926, 0.9923, 0.9811, 0.9793],
        [0.9505, 0.9929, 0.9927, 0.9805],
        [0.9785, 0.9510, 0.9927, 0.9924],
        [0.9804, 0.9781, 0.9516, 0.9927],
        [0.9924, 0.9802, 0.9787, 0.9496],
        [0.9926, 0.9923, 0.9803, 0.9791]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 3
********************
receiving microbatch 3 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 3, tensor([[0.9802, 0.9787, 0.9495, 0.9926, 0.9923],
        [0.9802, 0.9790, 0.9489, 0.9926, 0.9923],
        [0.9810, 0.9793, 0.9504, 0.9929, 0.9926],
        [0.9805, 0.9784, 0.9510, 0.9927, 0.9924],
        [0.9804, 0.9781, 0.9516, 0.9927, 0.9924],
        [0.9802, 0.9787, 0.9496, 0.9926, 0.9923],
        [0.9802, 0.9790, 0.9489, 0.9926, 0.9923],
        [0.9810, 0.9793, 0.9503, 0.9929, 0.9926]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 3
result is tensor([[0.9802, 0.9787, 0.9495, 0.9926, 0.9923],
        [0.9802, 0.9790, 0.9489, 0.9926, 0.9923],
        [0.9810, 0.9793, 0.9504, 0.9929, 0.9926],
        [0.9805, 0.9784, 0.9510, 0.9927, 0.9924],
        [0.9804, 0.9781, 0.9516, 0.9927, 0.9924],
        [0.9802, 0.9787, 0.9496, 0.9926, 0.9923],
        [0.9802, 0.9790, 0.9489, 0.9926, 0.9923],
        [0.9810, 0.9793, 0.9503, 0.9929, 0.9926]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9802, 0.9787, 0.9495, 0.9926, 0.9923],
        [0.9802, 0.9790, 0.9489, 0.9926, 0.9923],
        [0.9810, 0.9793, 0.9504, 0.9929, 0.9926],
        [0.9805, 0.9784, 0.9510, 0.9927, 0.9924],
        [0.9804, 0.9781, 0.9516, 0.9927, 0.9924],
        [0.9802, 0.9787, 0.9496, 0.9926, 0.9923],
        [0.9802, 0.9790, 0.9489, 0.9926, 0.9923],
        [0.9810, 0.9793, 0.9503, 0.9929, 0.9926]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9770, 0.9769, 0.9462, 0.9914, 0.9910],
        [1.0000, 0.9769, 1.0000, 1.0000, 1.0000],
        [0.9968, 0.9968, 0.9462, 0.9988, 0.9988],
        [0.9770, 0.9769, 0.9461, 0.9913, 0.9910],
        [1.0000, 1.0000, 0.9924, 1.0000, 1.0000],
        [0.9770, 0.9769, 0.9462, 0.9914, 0.9910],
        [1.0000, 0.9769, 1.0000, 1.0000, 1.0000],
        [0.9968, 0.9968, 0.9462, 0.9988, 0.9988]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9800, 0.9786, 0.9493, 0.9925, 0.9922],
        [0.9814, 0.9794, 0.9512, 0.9930, 0.9928],
        [0.9819, 0.9796, 0.9519, 0.9932, 0.9930],
        [0.9806, 0.9785, 0.9511, 0.9927, 0.9925],
        [0.9803, 0.9780, 0.9514, 0.9926, 0.9923],
        [0.9819, 0.9796, 0.9519, 0.9932, 0.9930],
        [0.9802, 0.9791, 0.9486, 0.9926, 0.9923],
        [0.9813, 0.9792, 0.9512, 0.9930, 0.9927]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9802, 0.9787, 0.9495, 0.9926, 0.9923],
        [0.9802, 0.9791, 0.9490, 0.9926, 0.9923],
        [0.9811, 0.9793, 0.9505, 0.9929, 0.9927],
        [0.9805, 0.9785, 0.9510, 0.9927, 0.9924],
        [0.9804, 0.9781, 0.9516, 0.9927, 0.9924],
        [0.9802, 0.9787, 0.9496, 0.9926, 0.9923],
        [0.9803, 0.9791, 0.9490, 0.9926, 0.9924],
        [0.9810, 0.9793, 0.9503, 0.9929, 0.9926]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9802, 0.9787, 0.9495, 0.9926, 0.9923],
        [0.9802, 0.9790, 0.9489, 0.9926, 0.9923],
        [0.9810, 0.9793, 0.9504, 0.9929, 0.9926],
        [0.9805, 0.9784, 0.9510, 0.9927, 0.9924],
        [0.9804, 0.9781, 0.9516, 0.9927, 0.9924],
        [0.9802, 0.9787, 0.9496, 0.9926, 0.9923],
        [0.9802, 0.9790, 0.9489, 0.9926, 0.9923],
        [0.9810, 0.9793, 0.9503, 0.9929, 0.9926]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)]
finished running cycle
_____________________________ test_forward_0[8-64] _____________________________

batch_size = 64, split_size = 8

    @pytest.mark.a4_2_2
    @pytest.mark.parametrize("batch_size", [1, 16, 32, 64])
    @pytest.mark.parametrize("split_size", [1, 2, 4, 8, 16])
    # @pytest.mark.parametrize("batch_size", [1])
    # @pytest.mark.parametrize("split_size", [1, 2])
    def test_forward_0(batch_size, split_size):
        model = nn.Sequential(
            nn.Linear(3, 4).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
            nn.Linear(4, 5).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
        )
    
        x = torch.randn(batch_size, 3).to('cuda:0')
        y0 = model(x).to('cpu')
    
        # move the last two layer to another device
        model[-2] = model[-2].to('cuda:1')
        model[-1] = WithDevice(nn.Sigmoid(), 'cuda:1')
        pipe = Pipe(model, split_size=split_size)
        y1 = pipe(x).to('cpu')
>       assert torch.allclose(y0, y1)
E       assert False
E        +  where False = <built-in method allclose of type object at 0x153da78cb760>(tensor([[0.2924, 0.5853, 0.5332, 0.4841, 0.5290],\n        [0.3226, 0.5614, 0.5406, 0.4604, 0.5148],\n        [0.3484, 0.5275, 0.5211, 0.4721, 0.4992],\n        [0.3114, 0.5686, 0.5343, 0.4742, 0.5193],\n        [0.2897, 0.5877, 0.5363, 0.4739, 0.5338],\n        [0.2832, 0.5895, 0.5310, 0.4780, 0.5384],\n        [0.2721, 0.6154, 0.5546, 0.4638, 0.5450],\n        [0.2877, 0.5840, 0.5359, 0.4483, 0.5455],\n        [0.3071, 0.5647, 0.5277, 0.4634, 0.5287],\n        [0.2774, 0.6020, 0.5387, 0.4861, 0.5369],\n        [0.3165, 0.5586, 0.5243, 0.4831, 0.5157],\n        [0.3214, 0.5521, 0.5280, 0.4574, 0.5212],\n        [0.3243, 0.5499, 0.5218, 0.4817, 0.5118],\n        [0.2941, 0.5863, 0.5453, 0.4491, 0.5376],\n        [0.2859, 0.6058, 0.5527, 0.4850, 0.5268],\n        [0.2808, 0.5973, 0.5309, 0.5058, 0.5295],\n        [0.3394, 0.5268, 0.5067, 0.4965, 0.4994],\n        [0.3192, 0.5506, 0.5190, 0.4754, 0.5188],\n        [0.3481, 0.5119, 0.5015, 0.4721, 0.5058],\n        [0.3525, 0.5100, 0.5139, 0.4303, 0.5156],\n        [0.2760, 0.6035, 0.5398, 0.4836, 0.5385],\n        [0.3071, 0.5604, 0.5197, 0.4723, 0.5280],\n        [0.3371, 0.5328, 0.5227, 0.4458, 0.5169],\n        [0.2794, 0.5995, 0.5394, 0.4787, 0.5382]...495, 0.4686, 0.5391],\n        [0.3048, 0.5695, 0.5327, 0.4588, 0.5304],\n        [0.3223, 0.5619, 0.5444, 0.4477, 0.5186],\n        [0.2934, 0.5957, 0.5449, 0.4936, 0.5210],\n        [0.3027, 0.5812, 0.5382, 0.4854, 0.5197],\n        [0.2937, 0.5895, 0.5317, 0.5119, 0.5184],\n        [0.2744, 0.6177, 0.5478, 0.5114, 0.5271],\n        [0.2857, 0.6021, 0.5458, 0.4912, 0.5269],\n        [0.2951, 0.5957, 0.5473, 0.4914, 0.5198],\n        [0.3482, 0.5159, 0.5135, 0.4420, 0.5142],\n        [0.2769, 0.6180, 0.5514, 0.5089, 0.5251],\n        [0.3205, 0.5570, 0.5290, 0.4736, 0.5151],\n        [0.3123, 0.5628, 0.5265, 0.4788, 0.5195],\n        [0.3158, 0.5642, 0.5296, 0.4874, 0.5128],\n        [0.3375, 0.5256, 0.5086, 0.4647, 0.5142],\n        [0.2801, 0.6121, 0.5514, 0.4959, 0.5274],\n        [0.3456, 0.5233, 0.5176, 0.4522, 0.5104],\n        [0.2854, 0.5914, 0.5369, 0.4722, 0.5373],\n        [0.3217, 0.5493, 0.5279, 0.4466, 0.5252],\n        [0.2900, 0.5912, 0.5440, 0.4621, 0.5357],\n        [0.3161, 0.5579, 0.5230, 0.4831, 0.5165],\n        [0.3524, 0.5148, 0.5155, 0.4461, 0.5090],\n        [0.2710, 0.6173, 0.5456, 0.5068, 0.5316],\n        [0.3300, 0.5474, 0.5242, 0.4827, 0.5067]], grad_fn=<ToCopyBackward0>), tensor([[0.9920, 0.9920, 0.9921, 0.9923, 0.9922],\n        [0.9918, 0.9918, 0.9919, 0.9921, 0.9920],\n        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920],\n        [0.9917, 0.9917, 0.9918, 0.9919, 0.9919],\n        [0.9917, 0.9917, 0.9918, 0.9919, 0.9919],\n        [0.9920, 0.9920, 0.9921, 0.9923, 0.9922],\n        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920],\n        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]...923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],\n        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], grad_fn=<ToCopyBackward0>))
E        +    where <built-in method allclose of type object at 0x153da78cb760> = torch.allclose

batch_size = 64
model      = Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
  (2): Linear(in_features=4, out_features=5, bias=True)
  (3): WithDevice(
    (_module): Sigmoid()
  )
)
pipe       = Pipe(
  (partitions): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=3, out_features=4, bias=True)
   ...inear(in_features=4, out_features=5, bias=True)
      (1): WithDevice(
        (_module): Sigmoid()
      )
    )
  )
)
split_size = 8
x          = tensor([[-0.4431,  0.1658,  0.5859],
        [ 1.2327,  0.3808, -1.0433],
        [-0.1202,  0.0110, -1.9424],
       ... 0.3612, -1.4310, -0.8811],
        [-1.0463,  1.5972,  0.7535],
        [-0.3915,  0.2498, -1.2631]], device='cuda:0')
y0         = tensor([[0.2924, 0.5853, 0.5332, 0.4841, 0.5290],
        [0.3226, 0.5614, 0.5406, 0.4604, 0.5148],
        [0.3484, 0...[0.2710, 0.6173, 0.5456, 0.5068, 0.5316],
        [0.3300, 0.5474, 0.5242, 0.4827, 0.5067]], grad_fn=<ToCopyBackward0>)
y1         = tensor([[0.9920, 0.9920, 0.9921, 0.9923, 0.9922],
        [0.9918, 0.9918, 0.9919, 0.9921, 0.9920],
        [0.9918, 0...[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], grad_fn=<ToCopyBackward0>)

tests/test_pipeline.py:83: AssertionError
----------------------------- Captured stdout call -----------------------------
in init
ModuleList(
  (0): Sequential(
    (0): Linear(in_features=3, out_features=4, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
  (1): Sequential(
    (0): Linear(in_features=4, out_features=5, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
)
[device(type='cuda', index=0), device(type='cuda', index=1)]
before running the cycle
[tensor([[-0.4431,  0.1658,  0.5859],
        [ 1.2327,  0.3808, -1.0433],
        [-0.1202,  0.0110, -1.9424],
        [ 0.2705,  0.2526, -0.4203],
        [ 0.0321, -0.0666,  0.8864],
        [-0.6200, -0.4453,  1.5790],
        [ 1.4405,  0.3634,  1.6497],
        [ 0.7710, -1.1403,  1.8796]], device='cuda:0'), tensor([[ 0.0941, -0.7086,  0.5712],
        [-0.5070,  0.2909,  1.3045],
        [-0.5638, -0.0030, -0.4249],
        [ 0.5005, -0.6408, -0.1319],
        [-0.5840, -0.0747, -0.7321],
        [ 1.4974, -0.3068,  0.8605],
        [ 0.7542,  1.5513, -0.2623],
        [-1.6790,  0.6066,  0.9847]], device='cuda:0'), tensor([[-2.1919, -0.1158, -1.7269],
        [-0.6527, -0.6329, -0.0260],
        [-1.7649, -1.4464, -0.9062],
        [ 0.8023, -2.3270, -0.1564],
        [-0.3774,  0.2342,  1.4262],
        [-0.6999, -0.9495,  0.8029],
        [ 0.7034, -1.1930, -0.3706],
        [-0.1372,  0.1003,  1.3189]], device='cuda:0'), tensor([[ 1.5741, -0.5570,  0.4045],
        [ 1.9186,  0.9406, -0.4729],
        [ 0.8506, -0.5229,  0.8980],
        [ 0.1991, -1.2044,  0.6371],
        [ 0.0433, -0.9056, -0.7860],
        [-1.2190,  0.3123,  0.1434],
        [ 0.0123, -0.3830, -1.2421],
        [-1.0861,  0.8544, -0.1965]], device='cuda:0'), tensor([[ 0.5409,  0.7959,  0.2513],
        [-0.6340, -1.6819,  1.0581],
        [-0.5945,  0.0679,  0.1704],
        [-1.6033, -1.3596,  0.8840],
        [ 0.7619,  1.7228, -1.8603],
        [ 1.5883,  0.3105, -2.3403],
        [-0.1367,  0.8457,  1.3985],
        [-0.7024, -0.9610,  0.2994]], device='cuda:0'), tensor([[ 0.9527,  0.4190,  1.0560],
        [ 0.5174, -0.5807,  0.5599],
        [ 1.8457,  0.1644, -0.8669],
        [ 0.0523,  1.4540, -0.5267],
        [ 0.0467,  0.7982, -0.4542],
        [-1.4751,  1.2232, -0.2171],
        [-0.9700,  2.1179,  0.1464],
        [ 0.0337,  1.2564,  0.0243]], device='cuda:0'), tensor([[ 0.3207,  1.5914, -0.7404],
        [ 0.3108, -1.8794, -0.3084],
        [-0.4839,  2.4059, -0.3014],
        [ 0.0814,  0.0356, -0.6446],
        [-0.3545, -0.1001, -0.1557],
        [-0.3590,  0.5192, -0.8218],
        [-0.8906, -1.4764, -0.1809],
        [ 0.1423,  1.8104, -0.1063]], device='cuda:0'), tensor([[ 0.2548, -1.1388, -0.8045],
        [ 0.0348, -0.1909,  1.2185],
        [ 0.8106, -1.0733,  0.2005],
        [ 0.9363,  0.0263,  0.7788],
        [-0.6542, -0.1064, -0.3209],
        [ 0.3612, -1.4310, -0.8811],
        [-1.0463,  1.5972,  0.7535],
        [-0.3915,  0.2498, -1.2631]], device='cuda:0')]
====================
schedule for this step is
[(0, 0)]
inputting microbatch 0 into partition 0
before moving to cuda:0: tensor([[-0.4431,  0.1658,  0.5859],
        [ 1.2327,  0.3808, -1.0433],
        [-0.1202,  0.0110, -1.9424],
        [ 0.2705,  0.2526, -0.4203],
        [ 0.0321, -0.0666,  0.8864],
        [-0.6200, -0.4453,  1.5790],
        [ 1.4405,  0.3634,  1.6497],
        [ 0.7710, -1.1403,  1.8796]], device='cuda:0')
after: tensor([[-0.4431,  0.1658,  0.5859],
        [ 1.2327,  0.3808, -1.0433],
        [-0.1202,  0.0110, -1.9424],
        [ 0.2705,  0.2526, -0.4203],
        [ 0.0321, -0.0666,  0.8864],
        [-0.6200, -0.4453,  1.5790],
        [ 1.4405,  0.3634,  1.6497],
        [ 0.7710, -1.1403,  1.8796]], device='cuda:0')
********************
observing microbatch 0
current batch shape is torch.Size([8, 3])
batch is tensor([[-0.4431,  0.1658,  0.5859],
        [ 1.2327,  0.3808, -1.0433],
        [-0.1202,  0.0110, -1.9424],
        [ 0.2705,  0.2526, -0.4203],
        [ 0.0321, -0.0666,  0.8864],
        [-0.6200, -0.4453,  1.5790],
        [ 1.4405,  0.3634,  1.6497],
        [ 0.7710, -1.1403,  1.8796]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 0
********************
receiving microbatch 0 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 0, tensor([[0.7239, 0.3743, 0.4729, 0.5461],
        [0.4986, 0.4883, 0.4463, 0.4981],
        [0.4222, 0.5073, 0.5936, 0.2295],
        [0.5999, 0.4345, 0.4765, 0.4802],
        [0.7182, 0.4154, 0.4667, 0.6105],
        [0.7746, 0.4080, 0.5112, 0.6075],
        [0.7673, 0.3969, 0.3224, 0.8350],
        [0.7069, 0.5574, 0.5216, 0.7048]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.7239, 0.3743, 0.4729, 0.5461],
        [0.4986, 0.4883, 0.4463, 0.4981],
        [0.4222, 0.5073, 0.5936, 0.2295],
        [0.5999, 0.4345, 0.4765, 0.4802],
        [0.7182, 0.4154, 0.4667, 0.6105],
        [0.7746, 0.4080, 0.5112, 0.6075],
        [0.7673, 0.3969, 0.3224, 0.8350],
        [0.7069, 0.5574, 0.5216, 0.7048]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.7239, 0.3743, 0.4729, 0.5461],
        [0.4986, 0.4883, 0.4463, 0.4981],
        [0.4222, 0.5073, 0.5936, 0.2295],
        [0.5999, 0.4345, 0.4765, 0.4802],
        [0.7182, 0.4154, 0.4667, 0.6105],
        [0.7746, 0.4080, 0.5112, 0.6075],
        [0.7673, 0.3969, 0.3224, 0.8350],
        [0.7069, 0.5574, 0.5216, 0.7048]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.7239, 0.3743, 0.4729, 0.5461],
        [0.4986, 0.4883, 0.4463, 0.4981],
        [0.4222, 0.5073, 0.5936, 0.2295],
        [0.5999, 0.4345, 0.4765, 0.4802],
        [0.7182, 0.4154, 0.4667, 0.6105],
        [0.7746, 0.4080, 0.5112, 0.6075],
        [0.7673, 0.3969, 0.3224, 0.8350],
        [0.7069, 0.5574, 0.5216, 0.7048]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.0941, -0.7086,  0.5712],
        [-0.5070,  0.2909,  1.3045],
        [-0.5638, -0.0030, -0.4249],
        [ 0.5005, -0.6408, -0.1319],
        [-0.5840, -0.0747, -0.7321],
        [ 1.4974, -0.3068,  0.8605],
        [ 0.7542,  1.5513, -0.2623],
        [-1.6790,  0.6066,  0.9847]], device='cuda:0'), tensor([[-2.1919, -0.1158, -1.7269],
        [-0.6527, -0.6329, -0.0260],
        [-1.7649, -1.4464, -0.9062],
        [ 0.8023, -2.3270, -0.1564],
        [-0.3774,  0.2342,  1.4262],
        [-0.6999, -0.9495,  0.8029],
        [ 0.7034, -1.1930, -0.3706],
        [-0.1372,  0.1003,  1.3189]], device='cuda:0'), tensor([[ 1.5741, -0.5570,  0.4045],
        [ 1.9186,  0.9406, -0.4729],
        [ 0.8506, -0.5229,  0.8980],
        [ 0.1991, -1.2044,  0.6371],
        [ 0.0433, -0.9056, -0.7860],
        [-1.2190,  0.3123,  0.1434],
        [ 0.0123, -0.3830, -1.2421],
        [-1.0861,  0.8544, -0.1965]], device='cuda:0'), tensor([[ 0.5409,  0.7959,  0.2513],
        [-0.6340, -1.6819,  1.0581],
        [-0.5945,  0.0679,  0.1704],
        [-1.6033, -1.3596,  0.8840],
        [ 0.7619,  1.7228, -1.8603],
        [ 1.5883,  0.3105, -2.3403],
        [-0.1367,  0.8457,  1.3985],
        [-0.7024, -0.9610,  0.2994]], device='cuda:0'), tensor([[ 0.9527,  0.4190,  1.0560],
        [ 0.5174, -0.5807,  0.5599],
        [ 1.8457,  0.1644, -0.8669],
        [ 0.0523,  1.4540, -0.5267],
        [ 0.0467,  0.7982, -0.4542],
        [-1.4751,  1.2232, -0.2171],
        [-0.9700,  2.1179,  0.1464],
        [ 0.0337,  1.2564,  0.0243]], device='cuda:0'), tensor([[ 0.3207,  1.5914, -0.7404],
        [ 0.3108, -1.8794, -0.3084],
        [-0.4839,  2.4059, -0.3014],
        [ 0.0814,  0.0356, -0.6446],
        [-0.3545, -0.1001, -0.1557],
        [-0.3590,  0.5192, -0.8218],
        [-0.8906, -1.4764, -0.1809],
        [ 0.1423,  1.8104, -0.1063]], device='cuda:0'), tensor([[ 0.2548, -1.1388, -0.8045],
        [ 0.0348, -0.1909,  1.2185],
        [ 0.8106, -1.0733,  0.2005],
        [ 0.9363,  0.0263,  0.7788],
        [-0.6542, -0.1064, -0.3209],
        [ 0.3612, -1.4310, -0.8811],
        [-1.0463,  1.5972,  0.7535],
        [-0.3915,  0.2498, -1.2631]], device='cuda:0')]
====================
schedule for this step is
[(1, 0), (0, 1)]
inputting microbatch 1 into partition 0
before moving to cuda:0: tensor([[ 0.0941, -0.7086,  0.5712],
        [-0.5070,  0.2909,  1.3045],
        [-0.5638, -0.0030, -0.4249],
        [ 0.5005, -0.6408, -0.1319],
        [-0.5840, -0.0747, -0.7321],
        [ 1.4974, -0.3068,  0.8605],
        [ 0.7542,  1.5513, -0.2623],
        [-1.6790,  0.6066,  0.9847]], device='cuda:0')
after: tensor([[ 0.0941, -0.7086,  0.5712],
        [-0.5070,  0.2909,  1.3045],
        [-0.5638, -0.0030, -0.4249],
        [ 0.5005, -0.6408, -0.1319],
        [-0.5840, -0.0747, -0.7321],
        [ 1.4974, -0.3068,  0.8605],
        [ 0.7542,  1.5513, -0.2623],
        [-1.6790,  0.6066,  0.9847]], device='cuda:0')
********************
observing microbatch 1
current batch shape is torch.Size([8, 3])
batch is tensor([[ 0.0941, -0.7086,  0.5712],
        [-0.5070,  0.2909,  1.3045],
        [-0.5638, -0.0030, -0.4249],
        [ 0.5005, -0.6408, -0.1319],
        [-0.5840, -0.0747, -0.7321],
        [ 1.4974, -0.3068,  0.8605],
        [ 0.7542,  1.5513, -0.2623],
        [-1.6790,  0.6066,  0.9847]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 1
********************
inputting microbatch 0 into partition 1
before moving to cuda:1: tensor([[0.7239, 0.3743, 0.4729, 0.5461],
        [0.4986, 0.4883, 0.4463, 0.4981],
        [0.4222, 0.5073, 0.5936, 0.2295],
        [0.5999, 0.4345, 0.4765, 0.4802],
        [0.7182, 0.4154, 0.4667, 0.6105],
        [0.7746, 0.4080, 0.5112, 0.6075],
        [0.7673, 0.3969, 0.3224, 0.8350],
        [0.7069, 0.5574, 0.5216, 0.7048]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9926, 0.9923, 0.9810, 0.9793],
        [0.9504, 0.9929, 0.9926, 0.9805],
        [0.9784, 0.9510, 0.9927, 0.9924],
        [0.9804, 0.9781, 0.9516, 0.9927],
        [0.9924, 0.9802, 0.9787, 0.9496],
        [0.9926, 0.9923, 0.9802, 0.9790],
        [0.9489, 0.9926, 0.9923, 0.9810],
        [0.9793, 0.9503, 0.9929, 0.9926]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 0
current batch shape is torch.Size([8, 4])
batch is tensor([[0.9926, 0.9923, 0.9810, 0.9793],
        [0.9504, 0.9929, 0.9926, 0.9805],
        [0.9784, 0.9510, 0.9927, 0.9924],
        [0.9804, 0.9781, 0.9516, 0.9927],
        [0.9924, 0.9802, 0.9787, 0.9496],
        [0.9926, 0.9923, 0.9802, 0.9790],
        [0.9489, 0.9926, 0.9923, 0.9810],
        [0.9793, 0.9503, 0.9929, 0.9926]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 0
********************
receiving microbatch 1 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 1, tensor([[0.6342, 0.5171, 0.5584, 0.5068],
        [0.7943, 0.3303, 0.4274, 0.6517],
        [0.6127, 0.4286, 0.5461, 0.3739],
        [0.5446, 0.5551, 0.5641, 0.4524],
        [0.5732, 0.4490, 0.5698, 0.3251],
        [0.6411, 0.5200, 0.4324, 0.7134],
        [0.7019, 0.2895, 0.2975, 0.6766],
        [0.8209, 0.2577, 0.4550, 0.5277]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 1
result is tensor([[0.6342, 0.5171, 0.5584, 0.5068],
        [0.7943, 0.3303, 0.4274, 0.6517],
        [0.6127, 0.4286, 0.5461, 0.3739],
        [0.5446, 0.5551, 0.5641, 0.4524],
        [0.5732, 0.4490, 0.5698, 0.3251],
        [0.6411, 0.5200, 0.4324, 0.7134],
        [0.7019, 0.2895, 0.2975, 0.6766],
        [0.8209, 0.2577, 0.4550, 0.5277]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6342, 0.5171, 0.5584, 0.5068],
        [0.7943, 0.3303, 0.4274, 0.6517],
        [0.6127, 0.4286, 0.5461, 0.3739],
        [0.5446, 0.5551, 0.5641, 0.4524],
        [0.5732, 0.4490, 0.5698, 0.3251],
        [0.6411, 0.5200, 0.4324, 0.7134],
        [0.7019, 0.2895, 0.2975, 0.6766],
        [0.8209, 0.2577, 0.4550, 0.5277]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 0 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 0, tensor([[0.9920, 0.9920, 0.9921, 0.9923, 0.9922],
        [0.9918, 0.9918, 0.9919, 0.9921, 0.9920],
        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920],
        [0.9917, 0.9917, 0.9918, 0.9919, 0.9919],
        [0.9917, 0.9917, 0.9918, 0.9919, 0.9919],
        [0.9920, 0.9920, 0.9921, 0.9923, 0.9922],
        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920],
        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.9920, 0.9920, 0.9921, 0.9923, 0.9922],
        [0.9918, 0.9918, 0.9919, 0.9921, 0.9920],
        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920],
        [0.9917, 0.9917, 0.9918, 0.9919, 0.9919],
        [0.9917, 0.9917, 0.9918, 0.9919, 0.9919],
        [0.9920, 0.9920, 0.9921, 0.9923, 0.9922],
        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920],
        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9920, 0.9920, 0.9921, 0.9923, 0.9922],
        [0.9918, 0.9918, 0.9919, 0.9921, 0.9920],
        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920],
        [0.9917, 0.9917, 0.9918, 0.9919, 0.9919],
        [0.9917, 0.9917, 0.9918, 0.9919, 0.9919],
        [0.9920, 0.9920, 0.9921, 0.9923, 0.9922],
        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920],
        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9920, 0.9920, 0.9921, 0.9923, 0.9922],
        [0.9918, 0.9918, 0.9919, 0.9921, 0.9920],
        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920],
        [0.9917, 0.9917, 0.9918, 0.9919, 0.9919],
        [0.9917, 0.9917, 0.9918, 0.9919, 0.9919],
        [0.9920, 0.9920, 0.9921, 0.9923, 0.9922],
        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920],
        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6342, 0.5171, 0.5584, 0.5068],
        [0.7943, 0.3303, 0.4274, 0.6517],
        [0.6127, 0.4286, 0.5461, 0.3739],
        [0.5446, 0.5551, 0.5641, 0.4524],
        [0.5732, 0.4490, 0.5698, 0.3251],
        [0.6411, 0.5200, 0.4324, 0.7134],
        [0.7019, 0.2895, 0.2975, 0.6766],
        [0.8209, 0.2577, 0.4550, 0.5277]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-2.1919, -0.1158, -1.7269],
        [-0.6527, -0.6329, -0.0260],
        [-1.7649, -1.4464, -0.9062],
        [ 0.8023, -2.3270, -0.1564],
        [-0.3774,  0.2342,  1.4262],
        [-0.6999, -0.9495,  0.8029],
        [ 0.7034, -1.1930, -0.3706],
        [-0.1372,  0.1003,  1.3189]], device='cuda:0'), tensor([[ 1.5741, -0.5570,  0.4045],
        [ 1.9186,  0.9406, -0.4729],
        [ 0.8506, -0.5229,  0.8980],
        [ 0.1991, -1.2044,  0.6371],
        [ 0.0433, -0.9056, -0.7860],
        [-1.2190,  0.3123,  0.1434],
        [ 0.0123, -0.3830, -1.2421],
        [-1.0861,  0.8544, -0.1965]], device='cuda:0'), tensor([[ 0.5409,  0.7959,  0.2513],
        [-0.6340, -1.6819,  1.0581],
        [-0.5945,  0.0679,  0.1704],
        [-1.6033, -1.3596,  0.8840],
        [ 0.7619,  1.7228, -1.8603],
        [ 1.5883,  0.3105, -2.3403],
        [-0.1367,  0.8457,  1.3985],
        [-0.7024, -0.9610,  0.2994]], device='cuda:0'), tensor([[ 0.9527,  0.4190,  1.0560],
        [ 0.5174, -0.5807,  0.5599],
        [ 1.8457,  0.1644, -0.8669],
        [ 0.0523,  1.4540, -0.5267],
        [ 0.0467,  0.7982, -0.4542],
        [-1.4751,  1.2232, -0.2171],
        [-0.9700,  2.1179,  0.1464],
        [ 0.0337,  1.2564,  0.0243]], device='cuda:0'), tensor([[ 0.3207,  1.5914, -0.7404],
        [ 0.3108, -1.8794, -0.3084],
        [-0.4839,  2.4059, -0.3014],
        [ 0.0814,  0.0356, -0.6446],
        [-0.3545, -0.1001, -0.1557],
        [-0.3590,  0.5192, -0.8218],
        [-0.8906, -1.4764, -0.1809],
        [ 0.1423,  1.8104, -0.1063]], device='cuda:0'), tensor([[ 0.2548, -1.1388, -0.8045],
        [ 0.0348, -0.1909,  1.2185],
        [ 0.8106, -1.0733,  0.2005],
        [ 0.9363,  0.0263,  0.7788],
        [-0.6542, -0.1064, -0.3209],
        [ 0.3612, -1.4310, -0.8811],
        [-1.0463,  1.5972,  0.7535],
        [-0.3915,  0.2498, -1.2631]], device='cuda:0')]
====================
schedule for this step is
[(2, 0), (1, 1)]
inputting microbatch 2 into partition 0
before moving to cuda:0: tensor([[-2.1919, -0.1158, -1.7269],
        [-0.6527, -0.6329, -0.0260],
        [-1.7649, -1.4464, -0.9062],
        [ 0.8023, -2.3270, -0.1564],
        [-0.3774,  0.2342,  1.4262],
        [-0.6999, -0.9495,  0.8029],
        [ 0.7034, -1.1930, -0.3706],
        [-0.1372,  0.1003,  1.3189]], device='cuda:0')
after: tensor([[-2.1919, -0.1158, -1.7269],
        [-0.6527, -0.6329, -0.0260],
        [-1.7649, -1.4464, -0.9062],
        [ 0.8023, -2.3270, -0.1564],
        [-0.3774,  0.2342,  1.4262],
        [-0.6999, -0.9495,  0.8029],
        [ 0.7034, -1.1930, -0.3706],
        [-0.1372,  0.1003,  1.3189]], device='cuda:0')
********************
observing microbatch 2
current batch shape is torch.Size([8, 3])
batch is tensor([[-2.1919, -0.1158, -1.7269],
        [-0.6527, -0.6329, -0.0260],
        [-1.7649, -1.4464, -0.9062],
        [ 0.8023, -2.3270, -0.1564],
        [-0.3774,  0.2342,  1.4262],
        [-0.6999, -0.9495,  0.8029],
        [ 0.7034, -1.1930, -0.3706],
        [-0.1372,  0.1003,  1.3189]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 2
********************
inputting microbatch 1 into partition 1
before moving to cuda:1: tensor([[0.6342, 0.5171, 0.5584, 0.5068],
        [0.7943, 0.3303, 0.4274, 0.6517],
        [0.6127, 0.4286, 0.5461, 0.3739],
        [0.5446, 0.5551, 0.5641, 0.4524],
        [0.5732, 0.4490, 0.5698, 0.3251],
        [0.6411, 0.5200, 0.4324, 0.7134],
        [0.7019, 0.2895, 0.2975, 0.6766],
        [0.8209, 0.2577, 0.4550, 0.5277]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9920, 0.9920, 0.9921, 0.9923],
        [0.9922, 0.9918, 0.9918, 0.9919],
        [0.9921, 0.9920, 0.9918, 0.9918],
        [0.9919, 0.9920, 0.9920, 0.9917],
        [0.9917, 0.9918, 0.9919, 0.9919],
        [0.9917, 0.9917, 0.9918, 0.9919],
        [0.9919, 0.9920, 0.9920, 0.9921],
        [0.9923, 0.9922, 0.9918, 0.9918]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 1
current batch shape is torch.Size([8, 4])
batch is tensor([[0.9920, 0.9920, 0.9921, 0.9923],
        [0.9922, 0.9918, 0.9918, 0.9919],
        [0.9921, 0.9920, 0.9918, 0.9918],
        [0.9919, 0.9920, 0.9920, 0.9917],
        [0.9917, 0.9918, 0.9919, 0.9919],
        [0.9917, 0.9917, 0.9918, 0.9919],
        [0.9919, 0.9920, 0.9920, 0.9921],
        [0.9923, 0.9922, 0.9918, 0.9918]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 1
********************
receiving microbatch 2 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 2, tensor([[0.5262, 0.4154, 0.6846, 0.1241],
        [0.6067, 0.4936, 0.6087, 0.3589],
        [0.4833, 0.5831, 0.7705, 0.1346],
        [0.3805, 0.7684, 0.7410, 0.3139],
        [0.7964, 0.3386, 0.4231, 0.6735],
        [0.6706, 0.5018, 0.6118, 0.4376],
        [0.4591, 0.6452, 0.6322, 0.3830],
        [0.7728, 0.3698, 0.4338, 0.6685]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 2
result is tensor([[0.5262, 0.4154, 0.6846, 0.1241],
        [0.6067, 0.4936, 0.6087, 0.3589],
        [0.4833, 0.5831, 0.7705, 0.1346],
        [0.3805, 0.7684, 0.7410, 0.3139],
        [0.7964, 0.3386, 0.4231, 0.6735],
        [0.6706, 0.5018, 0.6118, 0.4376],
        [0.4591, 0.6452, 0.6322, 0.3830],
        [0.7728, 0.3698, 0.4338, 0.6685]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5262, 0.4154, 0.6846, 0.1241],
        [0.6067, 0.4936, 0.6087, 0.3589],
        [0.4833, 0.5831, 0.7705, 0.1346],
        [0.3805, 0.7684, 0.7410, 0.3139],
        [0.7964, 0.3386, 0.4231, 0.6735],
        [0.6706, 0.5018, 0.6118, 0.4376],
        [0.4591, 0.6452, 0.6322, 0.3830],
        [0.7728, 0.3698, 0.4338, 0.6685]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 1 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 1, tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 1
result is tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9920, 0.9920, 0.9921, 0.9923, 0.9922],
        [0.9918, 0.9918, 0.9919, 0.9921, 0.9920],
        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920],
        [0.9917, 0.9917, 0.9918, 0.9919, 0.9919],
        [0.9917, 0.9917, 0.9918, 0.9919, 0.9919],
        [0.9920, 0.9920, 0.9921, 0.9923, 0.9922],
        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920],
        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5262, 0.4154, 0.6846, 0.1241],
        [0.6067, 0.4936, 0.6087, 0.3589],
        [0.4833, 0.5831, 0.7705, 0.1346],
        [0.3805, 0.7684, 0.7410, 0.3139],
        [0.7964, 0.3386, 0.4231, 0.6735],
        [0.6706, 0.5018, 0.6118, 0.4376],
        [0.4591, 0.6452, 0.6322, 0.3830],
        [0.7728, 0.3698, 0.4338, 0.6685]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 1.5741, -0.5570,  0.4045],
        [ 1.9186,  0.9406, -0.4729],
        [ 0.8506, -0.5229,  0.8980],
        [ 0.1991, -1.2044,  0.6371],
        [ 0.0433, -0.9056, -0.7860],
        [-1.2190,  0.3123,  0.1434],
        [ 0.0123, -0.3830, -1.2421],
        [-1.0861,  0.8544, -0.1965]], device='cuda:0'), tensor([[ 0.5409,  0.7959,  0.2513],
        [-0.6340, -1.6819,  1.0581],
        [-0.5945,  0.0679,  0.1704],
        [-1.6033, -1.3596,  0.8840],
        [ 0.7619,  1.7228, -1.8603],
        [ 1.5883,  0.3105, -2.3403],
        [-0.1367,  0.8457,  1.3985],
        [-0.7024, -0.9610,  0.2994]], device='cuda:0'), tensor([[ 0.9527,  0.4190,  1.0560],
        [ 0.5174, -0.5807,  0.5599],
        [ 1.8457,  0.1644, -0.8669],
        [ 0.0523,  1.4540, -0.5267],
        [ 0.0467,  0.7982, -0.4542],
        [-1.4751,  1.2232, -0.2171],
        [-0.9700,  2.1179,  0.1464],
        [ 0.0337,  1.2564,  0.0243]], device='cuda:0'), tensor([[ 0.3207,  1.5914, -0.7404],
        [ 0.3108, -1.8794, -0.3084],
        [-0.4839,  2.4059, -0.3014],
        [ 0.0814,  0.0356, -0.6446],
        [-0.3545, -0.1001, -0.1557],
        [-0.3590,  0.5192, -0.8218],
        [-0.8906, -1.4764, -0.1809],
        [ 0.1423,  1.8104, -0.1063]], device='cuda:0'), tensor([[ 0.2548, -1.1388, -0.8045],
        [ 0.0348, -0.1909,  1.2185],
        [ 0.8106, -1.0733,  0.2005],
        [ 0.9363,  0.0263,  0.7788],
        [-0.6542, -0.1064, -0.3209],
        [ 0.3612, -1.4310, -0.8811],
        [-1.0463,  1.5972,  0.7535],
        [-0.3915,  0.2498, -1.2631]], device='cuda:0')]
====================
schedule for this step is
[(3, 0), (2, 1)]
inputting microbatch 3 into partition 0
before moving to cuda:0: tensor([[ 1.5741, -0.5570,  0.4045],
        [ 1.9186,  0.9406, -0.4729],
        [ 0.8506, -0.5229,  0.8980],
        [ 0.1991, -1.2044,  0.6371],
        [ 0.0433, -0.9056, -0.7860],
        [-1.2190,  0.3123,  0.1434],
        [ 0.0123, -0.3830, -1.2421],
        [-1.0861,  0.8544, -0.1965]], device='cuda:0')
after: tensor([[ 1.5741, -0.5570,  0.4045],
        [ 1.9186,  0.9406, -0.4729],
        [ 0.8506, -0.5229,  0.8980],
        [ 0.1991, -1.2044,  0.6371],
        [ 0.0433, -0.9056, -0.7860],
        [-1.2190,  0.3123,  0.1434],
        [ 0.0123, -0.3830, -1.2421],
        [-1.0861,  0.8544, -0.1965]], device='cuda:0')
********************
observing microbatch 3
current batch shape is torch.Size([8, 3])
batch is tensor([[ 1.5741, -0.5570,  0.4045],
        [ 1.9186,  0.9406, -0.4729],
        [ 0.8506, -0.5229,  0.8980],
        [ 0.1991, -1.2044,  0.6371],
        [ 0.0433, -0.9056, -0.7860],
        [-1.2190,  0.3123,  0.1434],
        [ 0.0123, -0.3830, -1.2421],
        [-1.0861,  0.8544, -0.1965]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 3
********************
inputting microbatch 2 into partition 1
before moving to cuda:1: tensor([[0.5262, 0.4154, 0.6846, 0.1241],
        [0.6067, 0.4936, 0.6087, 0.3589],
        [0.4833, 0.5831, 0.7705, 0.1346],
        [0.3805, 0.7684, 0.7410, 0.3139],
        [0.7964, 0.3386, 0.4231, 0.6735],
        [0.6706, 0.5018, 0.6118, 0.4376],
        [0.4591, 0.6452, 0.6322, 0.3830],
        [0.7728, 0.3698, 0.4338, 0.6685]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9922, 0.9922, 0.9923, 0.9924],
        [0.9924, 0.9922, 0.9922, 0.9923],
        [0.9924, 0.9924, 0.9922, 0.9922],
        [0.9923, 0.9924, 0.9924, 0.9922],
        [0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924],
        [0.9924, 0.9922, 0.9922, 0.9923],
        [0.9924, 0.9924, 0.9922, 0.9922]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 2
current batch shape is torch.Size([8, 4])
batch is tensor([[0.9922, 0.9922, 0.9923, 0.9924],
        [0.9924, 0.9922, 0.9922, 0.9923],
        [0.9924, 0.9924, 0.9922, 0.9922],
        [0.9923, 0.9924, 0.9924, 0.9922],
        [0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924],
        [0.9924, 0.9922, 0.9922, 0.9923],
        [0.9924, 0.9924, 0.9922, 0.9922]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 2
********************
receiving microbatch 3 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 3, tensor([[0.5664, 0.5750, 0.4809, 0.6400],
        [0.5833, 0.4242, 0.3264, 0.6947],
        [0.6534, 0.5162, 0.4863, 0.6433],
        [0.5952, 0.5860, 0.6110, 0.4755],
        [0.4659, 0.5933, 0.6442, 0.2979],
        [0.7208, 0.3371, 0.5099, 0.4219],
        [0.4613, 0.5398, 0.6041, 0.2855],
        [0.7235, 0.2909, 0.4521, 0.4415]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 3
result is tensor([[0.5664, 0.5750, 0.4809, 0.6400],
        [0.5833, 0.4242, 0.3264, 0.6947],
        [0.6534, 0.5162, 0.4863, 0.6433],
        [0.5952, 0.5860, 0.6110, 0.4755],
        [0.4659, 0.5933, 0.6442, 0.2979],
        [0.7208, 0.3371, 0.5099, 0.4219],
        [0.4613, 0.5398, 0.6041, 0.2855],
        [0.7235, 0.2909, 0.4521, 0.4415]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5664, 0.5750, 0.4809, 0.6400],
        [0.5833, 0.4242, 0.3264, 0.6947],
        [0.6534, 0.5162, 0.4863, 0.6433],
        [0.5952, 0.5860, 0.6110, 0.4755],
        [0.4659, 0.5933, 0.6442, 0.2979],
        [0.7208, 0.3371, 0.5099, 0.4219],
        [0.4613, 0.5398, 0.6041, 0.2855],
        [0.7235, 0.2909, 0.4521, 0.4415]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 2 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 2, tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 2
result is tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9920, 0.9920, 0.9921, 0.9923, 0.9922],
        [0.9918, 0.9918, 0.9919, 0.9921, 0.9920],
        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920],
        [0.9917, 0.9917, 0.9918, 0.9919, 0.9919],
        [0.9917, 0.9917, 0.9918, 0.9919, 0.9919],
        [0.9920, 0.9920, 0.9921, 0.9923, 0.9922],
        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920],
        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5664, 0.5750, 0.4809, 0.6400],
        [0.5833, 0.4242, 0.3264, 0.6947],
        [0.6534, 0.5162, 0.4863, 0.6433],
        [0.5952, 0.5860, 0.6110, 0.4755],
        [0.4659, 0.5933, 0.6442, 0.2979],
        [0.7208, 0.3371, 0.5099, 0.4219],
        [0.4613, 0.5398, 0.6041, 0.2855],
        [0.7235, 0.2909, 0.4521, 0.4415]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.5409,  0.7959,  0.2513],
        [-0.6340, -1.6819,  1.0581],
        [-0.5945,  0.0679,  0.1704],
        [-1.6033, -1.3596,  0.8840],
        [ 0.7619,  1.7228, -1.8603],
        [ 1.5883,  0.3105, -2.3403],
        [-0.1367,  0.8457,  1.3985],
        [-0.7024, -0.9610,  0.2994]], device='cuda:0'), tensor([[ 0.9527,  0.4190,  1.0560],
        [ 0.5174, -0.5807,  0.5599],
        [ 1.8457,  0.1644, -0.8669],
        [ 0.0523,  1.4540, -0.5267],
        [ 0.0467,  0.7982, -0.4542],
        [-1.4751,  1.2232, -0.2171],
        [-0.9700,  2.1179,  0.1464],
        [ 0.0337,  1.2564,  0.0243]], device='cuda:0'), tensor([[ 0.3207,  1.5914, -0.7404],
        [ 0.3108, -1.8794, -0.3084],
        [-0.4839,  2.4059, -0.3014],
        [ 0.0814,  0.0356, -0.6446],
        [-0.3545, -0.1001, -0.1557],
        [-0.3590,  0.5192, -0.8218],
        [-0.8906, -1.4764, -0.1809],
        [ 0.1423,  1.8104, -0.1063]], device='cuda:0'), tensor([[ 0.2548, -1.1388, -0.8045],
        [ 0.0348, -0.1909,  1.2185],
        [ 0.8106, -1.0733,  0.2005],
        [ 0.9363,  0.0263,  0.7788],
        [-0.6542, -0.1064, -0.3209],
        [ 0.3612, -1.4310, -0.8811],
        [-1.0463,  1.5972,  0.7535],
        [-0.3915,  0.2498, -1.2631]], device='cuda:0')]
====================
schedule for this step is
[(4, 0), (3, 1)]
inputting microbatch 4 into partition 0
before moving to cuda:0: tensor([[ 0.5409,  0.7959,  0.2513],
        [-0.6340, -1.6819,  1.0581],
        [-0.5945,  0.0679,  0.1704],
        [-1.6033, -1.3596,  0.8840],
        [ 0.7619,  1.7228, -1.8603],
        [ 1.5883,  0.3105, -2.3403],
        [-0.1367,  0.8457,  1.3985],
        [-0.7024, -0.9610,  0.2994]], device='cuda:0')
after: tensor([[ 0.5409,  0.7959,  0.2513],
        [-0.6340, -1.6819,  1.0581],
        [-0.5945,  0.0679,  0.1704],
        [-1.6033, -1.3596,  0.8840],
        [ 0.7619,  1.7228, -1.8603],
        [ 1.5883,  0.3105, -2.3403],
        [-0.1367,  0.8457,  1.3985],
        [-0.7024, -0.9610,  0.2994]], device='cuda:0')
********************
observing microbatch 4
current batch shape is torch.Size([8, 3])
batch is tensor([[ 0.5409,  0.7959,  0.2513],
        [-0.6340, -1.6819,  1.0581],
        [-0.5945,  0.0679,  0.1704],
        [-1.6033, -1.3596,  0.8840],
        [ 0.7619,  1.7228, -1.8603],
        [ 1.5883,  0.3105, -2.3403],
        [-0.1367,  0.8457,  1.3985],
        [-0.7024, -0.9610,  0.2994]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 4
********************
inputting microbatch 3 into partition 1
before moving to cuda:1: tensor([[0.5664, 0.5750, 0.4809, 0.6400],
        [0.5833, 0.4242, 0.3264, 0.6947],
        [0.6534, 0.5162, 0.4863, 0.6433],
        [0.5952, 0.5860, 0.6110, 0.4755],
        [0.4659, 0.5933, 0.6442, 0.2979],
        [0.7208, 0.3371, 0.5099, 0.4219],
        [0.4613, 0.5398, 0.6041, 0.2855],
        [0.7235, 0.2909, 0.4521, 0.4415]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9922, 0.9922, 0.9923, 0.9924],
        [0.9924, 0.9922, 0.9922, 0.9923],
        [0.9924, 0.9924, 0.9922, 0.9922],
        [0.9923, 0.9924, 0.9924, 0.9922],
        [0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924],
        [0.9924, 0.9922, 0.9922, 0.9923],
        [0.9924, 0.9924, 0.9922, 0.9922]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 3
current batch shape is torch.Size([8, 4])
batch is tensor([[0.9922, 0.9922, 0.9923, 0.9924],
        [0.9924, 0.9922, 0.9922, 0.9923],
        [0.9924, 0.9924, 0.9922, 0.9922],
        [0.9923, 0.9924, 0.9924, 0.9922],
        [0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924],
        [0.9924, 0.9922, 0.9922, 0.9923],
        [0.9924, 0.9924, 0.9922, 0.9922]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 3
********************
receiving microbatch 4 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 4, tensor([[0.7029, 0.3520, 0.3686, 0.6545],
        [0.6354, 0.5932, 0.6818, 0.4062],
        [0.6822, 0.3952, 0.5112, 0.4614],
        [0.6815, 0.5099, 0.6916, 0.3255],
        [0.5448, 0.3231, 0.3429, 0.4724],
        [0.3338, 0.5656, 0.4989, 0.3441],
        [0.8216, 0.2789, 0.3421, 0.7418],
        [0.6166, 0.5229, 0.6352, 0.3665]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 4
result is tensor([[0.7029, 0.3520, 0.3686, 0.6545],
        [0.6354, 0.5932, 0.6818, 0.4062],
        [0.6822, 0.3952, 0.5112, 0.4614],
        [0.6815, 0.5099, 0.6916, 0.3255],
        [0.5448, 0.3231, 0.3429, 0.4724],
        [0.3338, 0.5656, 0.4989, 0.3441],
        [0.8216, 0.2789, 0.3421, 0.7418],
        [0.6166, 0.5229, 0.6352, 0.3665]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.7029, 0.3520, 0.3686, 0.6545],
        [0.6354, 0.5932, 0.6818, 0.4062],
        [0.6822, 0.3952, 0.5112, 0.4614],
        [0.6815, 0.5099, 0.6916, 0.3255],
        [0.5448, 0.3231, 0.3429, 0.4724],
        [0.3338, 0.5656, 0.4989, 0.3441],
        [0.8216, 0.2789, 0.3421, 0.7418],
        [0.6166, 0.5229, 0.6352, 0.3665]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 3 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 3, tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 3
result is tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9920, 0.9920, 0.9921, 0.9923, 0.9922],
        [0.9918, 0.9918, 0.9919, 0.9921, 0.9920],
        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920],
        [0.9917, 0.9917, 0.9918, 0.9919, 0.9919],
        [0.9917, 0.9917, 0.9918, 0.9919, 0.9919],
        [0.9920, 0.9920, 0.9921, 0.9923, 0.9922],
        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920],
        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.7029, 0.3520, 0.3686, 0.6545],
        [0.6354, 0.5932, 0.6818, 0.4062],
        [0.6822, 0.3952, 0.5112, 0.4614],
        [0.6815, 0.5099, 0.6916, 0.3255],
        [0.5448, 0.3231, 0.3429, 0.4724],
        [0.3338, 0.5656, 0.4989, 0.3441],
        [0.8216, 0.2789, 0.3421, 0.7418],
        [0.6166, 0.5229, 0.6352, 0.3665]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.9527,  0.4190,  1.0560],
        [ 0.5174, -0.5807,  0.5599],
        [ 1.8457,  0.1644, -0.8669],
        [ 0.0523,  1.4540, -0.5267],
        [ 0.0467,  0.7982, -0.4542],
        [-1.4751,  1.2232, -0.2171],
        [-0.9700,  2.1179,  0.1464],
        [ 0.0337,  1.2564,  0.0243]], device='cuda:0'), tensor([[ 0.3207,  1.5914, -0.7404],
        [ 0.3108, -1.8794, -0.3084],
        [-0.4839,  2.4059, -0.3014],
        [ 0.0814,  0.0356, -0.6446],
        [-0.3545, -0.1001, -0.1557],
        [-0.3590,  0.5192, -0.8218],
        [-0.8906, -1.4764, -0.1809],
        [ 0.1423,  1.8104, -0.1063]], device='cuda:0'), tensor([[ 0.2548, -1.1388, -0.8045],
        [ 0.0348, -0.1909,  1.2185],
        [ 0.8106, -1.0733,  0.2005],
        [ 0.9363,  0.0263,  0.7788],
        [-0.6542, -0.1064, -0.3209],
        [ 0.3612, -1.4310, -0.8811],
        [-1.0463,  1.5972,  0.7535],
        [-0.3915,  0.2498, -1.2631]], device='cuda:0')]
====================
schedule for this step is
[(5, 0), (4, 1)]
inputting microbatch 5 into partition 0
before moving to cuda:0: tensor([[ 0.9527,  0.4190,  1.0560],
        [ 0.5174, -0.5807,  0.5599],
        [ 1.8457,  0.1644, -0.8669],
        [ 0.0523,  1.4540, -0.5267],
        [ 0.0467,  0.7982, -0.4542],
        [-1.4751,  1.2232, -0.2171],
        [-0.9700,  2.1179,  0.1464],
        [ 0.0337,  1.2564,  0.0243]], device='cuda:0')
after: tensor([[ 0.9527,  0.4190,  1.0560],
        [ 0.5174, -0.5807,  0.5599],
        [ 1.8457,  0.1644, -0.8669],
        [ 0.0523,  1.4540, -0.5267],
        [ 0.0467,  0.7982, -0.4542],
        [-1.4751,  1.2232, -0.2171],
        [-0.9700,  2.1179,  0.1464],
        [ 0.0337,  1.2564,  0.0243]], device='cuda:0')
********************
observing microbatch 5
current batch shape is torch.Size([8, 3])
batch is tensor([[ 0.9527,  0.4190,  1.0560],
        [ 0.5174, -0.5807,  0.5599],
        [ 1.8457,  0.1644, -0.8669],
        [ 0.0523,  1.4540, -0.5267],
        [ 0.0467,  0.7982, -0.4542],
        [-1.4751,  1.2232, -0.2171],
        [-0.9700,  2.1179,  0.1464],
        [ 0.0337,  1.2564,  0.0243]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 5
********************
inputting microbatch 4 into partition 1
before moving to cuda:1: tensor([[0.7029, 0.3520, 0.3686, 0.6545],
        [0.6354, 0.5932, 0.6818, 0.4062],
        [0.6822, 0.3952, 0.5112, 0.4614],
        [0.6815, 0.5099, 0.6916, 0.3255],
        [0.5448, 0.3231, 0.3429, 0.4724],
        [0.3338, 0.5656, 0.4989, 0.3441],
        [0.8216, 0.2789, 0.3421, 0.7418],
        [0.6166, 0.5229, 0.6352, 0.3665]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9922, 0.9922, 0.9923, 0.9924],
        [0.9924, 0.9922, 0.9922, 0.9923],
        [0.9924, 0.9924, 0.9922, 0.9922],
        [0.9923, 0.9924, 0.9924, 0.9922],
        [0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924],
        [0.9924, 0.9922, 0.9922, 0.9923],
        [0.9924, 0.9924, 0.9922, 0.9922]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 4
current batch shape is torch.Size([8, 4])
batch is tensor([[0.9922, 0.9922, 0.9923, 0.9924],
        [0.9924, 0.9922, 0.9922, 0.9923],
        [0.9924, 0.9924, 0.9922, 0.9922],
        [0.9923, 0.9924, 0.9924, 0.9922],
        [0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924],
        [0.9924, 0.9922, 0.9922, 0.9923],
        [0.9924, 0.9924, 0.9922, 0.9922]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 4
********************
receiving microbatch 5 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 5, tensor([[0.7360, 0.3891, 0.3605, 0.7521],
        [0.6263, 0.5209, 0.5242, 0.5597],
        [0.4727, 0.5407, 0.4377, 0.5612],
        [0.6949, 0.2806, 0.3463, 0.5669],
        [0.6511, 0.3548, 0.4206, 0.5096],
        [0.7599, 0.2379, 0.4249, 0.4383],
        [0.8250, 0.1669, 0.2874, 0.6294],
        [0.7330, 0.2841, 0.3463, 0.6221]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 5
result is tensor([[0.7360, 0.3891, 0.3605, 0.7521],
        [0.6263, 0.5209, 0.5242, 0.5597],
        [0.4727, 0.5407, 0.4377, 0.5612],
        [0.6949, 0.2806, 0.3463, 0.5669],
        [0.6511, 0.3548, 0.4206, 0.5096],
        [0.7599, 0.2379, 0.4249, 0.4383],
        [0.8250, 0.1669, 0.2874, 0.6294],
        [0.7330, 0.2841, 0.3463, 0.6221]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.7360, 0.3891, 0.3605, 0.7521],
        [0.6263, 0.5209, 0.5242, 0.5597],
        [0.4727, 0.5407, 0.4377, 0.5612],
        [0.6949, 0.2806, 0.3463, 0.5669],
        [0.6511, 0.3548, 0.4206, 0.5096],
        [0.7599, 0.2379, 0.4249, 0.4383],
        [0.8250, 0.1669, 0.2874, 0.6294],
        [0.7330, 0.2841, 0.3463, 0.6221]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 4 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 4, tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 4
result is tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9920, 0.9920, 0.9921, 0.9923, 0.9922],
        [0.9918, 0.9918, 0.9919, 0.9921, 0.9920],
        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920],
        [0.9917, 0.9917, 0.9918, 0.9919, 0.9919],
        [0.9917, 0.9917, 0.9918, 0.9919, 0.9919],
        [0.9920, 0.9920, 0.9921, 0.9923, 0.9922],
        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920],
        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.7360, 0.3891, 0.3605, 0.7521],
        [0.6263, 0.5209, 0.5242, 0.5597],
        [0.4727, 0.5407, 0.4377, 0.5612],
        [0.6949, 0.2806, 0.3463, 0.5669],
        [0.6511, 0.3548, 0.4206, 0.5096],
        [0.7599, 0.2379, 0.4249, 0.4383],
        [0.8250, 0.1669, 0.2874, 0.6294],
        [0.7330, 0.2841, 0.3463, 0.6221]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.3207,  1.5914, -0.7404],
        [ 0.3108, -1.8794, -0.3084],
        [-0.4839,  2.4059, -0.3014],
        [ 0.0814,  0.0356, -0.6446],
        [-0.3545, -0.1001, -0.1557],
        [-0.3590,  0.5192, -0.8218],
        [-0.8906, -1.4764, -0.1809],
        [ 0.1423,  1.8104, -0.1063]], device='cuda:0'), tensor([[ 0.2548, -1.1388, -0.8045],
        [ 0.0348, -0.1909,  1.2185],
        [ 0.8106, -1.0733,  0.2005],
        [ 0.9363,  0.0263,  0.7788],
        [-0.6542, -0.1064, -0.3209],
        [ 0.3612, -1.4310, -0.8811],
        [-1.0463,  1.5972,  0.7535],
        [-0.3915,  0.2498, -1.2631]], device='cuda:0')]
====================
schedule for this step is
[(6, 0), (5, 1)]
inputting microbatch 6 into partition 0
before moving to cuda:0: tensor([[ 0.3207,  1.5914, -0.7404],
        [ 0.3108, -1.8794, -0.3084],
        [-0.4839,  2.4059, -0.3014],
        [ 0.0814,  0.0356, -0.6446],
        [-0.3545, -0.1001, -0.1557],
        [-0.3590,  0.5192, -0.8218],
        [-0.8906, -1.4764, -0.1809],
        [ 0.1423,  1.8104, -0.1063]], device='cuda:0')
after: tensor([[ 0.3207,  1.5914, -0.7404],
        [ 0.3108, -1.8794, -0.3084],
        [-0.4839,  2.4059, -0.3014],
        [ 0.0814,  0.0356, -0.6446],
        [-0.3545, -0.1001, -0.1557],
        [-0.3590,  0.5192, -0.8218],
        [-0.8906, -1.4764, -0.1809],
        [ 0.1423,  1.8104, -0.1063]], device='cuda:0')
********************
observing microbatch 6
current batch shape is torch.Size([8, 3])
batch is tensor([[ 0.3207,  1.5914, -0.7404],
        [ 0.3108, -1.8794, -0.3084],
        [-0.4839,  2.4059, -0.3014],
        [ 0.0814,  0.0356, -0.6446],
        [-0.3545, -0.1001, -0.1557],
        [-0.3590,  0.5192, -0.8218],
        [-0.8906, -1.4764, -0.1809],
        [ 0.1423,  1.8104, -0.1063]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 6
********************
inputting microbatch 5 into partition 1
before moving to cuda:1: tensor([[0.7360, 0.3891, 0.3605, 0.7521],
        [0.6263, 0.5209, 0.5242, 0.5597],
        [0.4727, 0.5407, 0.4377, 0.5612],
        [0.6949, 0.2806, 0.3463, 0.5669],
        [0.6511, 0.3548, 0.4206, 0.5096],
        [0.7599, 0.2379, 0.4249, 0.4383],
        [0.8250, 0.1669, 0.2874, 0.6294],
        [0.7330, 0.2841, 0.3463, 0.6221]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9922, 0.9922, 0.9923, 0.9924],
        [0.9924, 0.9922, 0.9922, 0.9923],
        [0.9924, 0.9924, 0.9922, 0.9922],
        [0.9923, 0.9924, 0.9924, 0.9922],
        [0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924],
        [0.9924, 0.9922, 0.9922, 0.9923],
        [0.9924, 0.9924, 0.9922, 0.9922]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 5
current batch shape is torch.Size([8, 4])
batch is tensor([[0.9922, 0.9922, 0.9923, 0.9924],
        [0.9924, 0.9922, 0.9922, 0.9923],
        [0.9924, 0.9924, 0.9922, 0.9922],
        [0.9923, 0.9924, 0.9924, 0.9922],
        [0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924],
        [0.9924, 0.9922, 0.9922, 0.9923],
        [0.9924, 0.9924, 0.9922, 0.9922]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 5
********************
receiving microbatch 6 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 6, tensor([[0.6741, 0.2829, 0.3290, 0.5764],
        [0.4228, 0.7076, 0.7199, 0.2935],
        [0.7960, 0.1682, 0.2580, 0.6413],
        [0.5641, 0.4631, 0.5225, 0.4079],
        [0.6251, 0.4413, 0.5364, 0.4212],
        [0.6049, 0.3849, 0.4901, 0.3897],
        [0.5257, 0.6010, 0.7181, 0.2477],
        [0.7556, 0.2345, 0.2875, 0.6669]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 6
result is tensor([[0.6741, 0.2829, 0.3290, 0.5764],
        [0.4228, 0.7076, 0.7199, 0.2935],
        [0.7960, 0.1682, 0.2580, 0.6413],
        [0.5641, 0.4631, 0.5225, 0.4079],
        [0.6251, 0.4413, 0.5364, 0.4212],
        [0.6049, 0.3849, 0.4901, 0.3897],
        [0.5257, 0.6010, 0.7181, 0.2477],
        [0.7556, 0.2345, 0.2875, 0.6669]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6741, 0.2829, 0.3290, 0.5764],
        [0.4228, 0.7076, 0.7199, 0.2935],
        [0.7960, 0.1682, 0.2580, 0.6413],
        [0.5641, 0.4631, 0.5225, 0.4079],
        [0.6251, 0.4413, 0.5364, 0.4212],
        [0.6049, 0.3849, 0.4901, 0.3897],
        [0.5257, 0.6010, 0.7181, 0.2477],
        [0.7556, 0.2345, 0.2875, 0.6669]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 5 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 5, tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 5
result is tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9920, 0.9920, 0.9921, 0.9923, 0.9922],
        [0.9918, 0.9918, 0.9919, 0.9921, 0.9920],
        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920],
        [0.9917, 0.9917, 0.9918, 0.9919, 0.9919],
        [0.9917, 0.9917, 0.9918, 0.9919, 0.9919],
        [0.9920, 0.9920, 0.9921, 0.9923, 0.9922],
        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920],
        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6741, 0.2829, 0.3290, 0.5764],
        [0.4228, 0.7076, 0.7199, 0.2935],
        [0.7960, 0.1682, 0.2580, 0.6413],
        [0.5641, 0.4631, 0.5225, 0.4079],
        [0.6251, 0.4413, 0.5364, 0.4212],
        [0.6049, 0.3849, 0.4901, 0.3897],
        [0.5257, 0.6010, 0.7181, 0.2477],
        [0.7556, 0.2345, 0.2875, 0.6669]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.2548, -1.1388, -0.8045],
        [ 0.0348, -0.1909,  1.2185],
        [ 0.8106, -1.0733,  0.2005],
        [ 0.9363,  0.0263,  0.7788],
        [-0.6542, -0.1064, -0.3209],
        [ 0.3612, -1.4310, -0.8811],
        [-1.0463,  1.5972,  0.7535],
        [-0.3915,  0.2498, -1.2631]], device='cuda:0')]
====================
schedule for this step is
[(7, 0), (6, 1)]
inputting microbatch 7 into partition 0
before moving to cuda:0: tensor([[ 0.2548, -1.1388, -0.8045],
        [ 0.0348, -0.1909,  1.2185],
        [ 0.8106, -1.0733,  0.2005],
        [ 0.9363,  0.0263,  0.7788],
        [-0.6542, -0.1064, -0.3209],
        [ 0.3612, -1.4310, -0.8811],
        [-1.0463,  1.5972,  0.7535],
        [-0.3915,  0.2498, -1.2631]], device='cuda:0')
after: tensor([[ 0.2548, -1.1388, -0.8045],
        [ 0.0348, -0.1909,  1.2185],
        [ 0.8106, -1.0733,  0.2005],
        [ 0.9363,  0.0263,  0.7788],
        [-0.6542, -0.1064, -0.3209],
        [ 0.3612, -1.4310, -0.8811],
        [-1.0463,  1.5972,  0.7535],
        [-0.3915,  0.2498, -1.2631]], device='cuda:0')
********************
observing microbatch 7
current batch shape is torch.Size([8, 3])
batch is tensor([[ 0.2548, -1.1388, -0.8045],
        [ 0.0348, -0.1909,  1.2185],
        [ 0.8106, -1.0733,  0.2005],
        [ 0.9363,  0.0263,  0.7788],
        [-0.6542, -0.1064, -0.3209],
        [ 0.3612, -1.4310, -0.8811],
        [-1.0463,  1.5972,  0.7535],
        [-0.3915,  0.2498, -1.2631]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 7
********************
inputting microbatch 6 into partition 1
before moving to cuda:1: tensor([[0.6741, 0.2829, 0.3290, 0.5764],
        [0.4228, 0.7076, 0.7199, 0.2935],
        [0.7960, 0.1682, 0.2580, 0.6413],
        [0.5641, 0.4631, 0.5225, 0.4079],
        [0.6251, 0.4413, 0.5364, 0.4212],
        [0.6049, 0.3849, 0.4901, 0.3897],
        [0.5257, 0.6010, 0.7181, 0.2477],
        [0.7556, 0.2345, 0.2875, 0.6669]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9922, 0.9922, 0.9923, 0.9924],
        [0.9924, 0.9922, 0.9922, 0.9923],
        [0.9924, 0.9924, 0.9922, 0.9922],
        [0.9923, 0.9924, 0.9924, 0.9922],
        [0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924],
        [0.9924, 0.9922, 0.9922, 0.9923],
        [0.9924, 0.9924, 0.9922, 0.9922]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 6
current batch shape is torch.Size([8, 4])
batch is tensor([[0.9922, 0.9922, 0.9923, 0.9924],
        [0.9924, 0.9922, 0.9922, 0.9923],
        [0.9924, 0.9924, 0.9922, 0.9922],
        [0.9923, 0.9924, 0.9924, 0.9922],
        [0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924],
        [0.9924, 0.9922, 0.9922, 0.9923],
        [0.9924, 0.9924, 0.9922, 0.9922]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 6
********************
receiving microbatch 7 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 7, tensor([[0.4340, 0.6338, 0.6627, 0.2929],
        [0.7396, 0.4193, 0.4667, 0.6436],
        [0.5308, 0.6141, 0.5883, 0.4861],
        [0.6814, 0.4507, 0.4201, 0.6860],
        [0.6191, 0.4340, 0.5581, 0.3697],
        [0.3956, 0.6770, 0.6935, 0.2681],
        [0.8400, 0.1923, 0.3216, 0.6537],
        [0.5330, 0.4354, 0.5455, 0.3042]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 7
result is tensor([[0.4340, 0.6338, 0.6627, 0.2929],
        [0.7396, 0.4193, 0.4667, 0.6436],
        [0.5308, 0.6141, 0.5883, 0.4861],
        [0.6814, 0.4507, 0.4201, 0.6860],
        [0.6191, 0.4340, 0.5581, 0.3697],
        [0.3956, 0.6770, 0.6935, 0.2681],
        [0.8400, 0.1923, 0.3216, 0.6537],
        [0.5330, 0.4354, 0.5455, 0.3042]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4340, 0.6338, 0.6627, 0.2929],
        [0.7396, 0.4193, 0.4667, 0.6436],
        [0.5308, 0.6141, 0.5883, 0.4861],
        [0.6814, 0.4507, 0.4201, 0.6860],
        [0.6191, 0.4340, 0.5581, 0.3697],
        [0.3956, 0.6770, 0.6935, 0.2681],
        [0.8400, 0.1923, 0.3216, 0.6537],
        [0.5330, 0.4354, 0.5455, 0.3042]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 6 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 6, tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 6
result is tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9920, 0.9920, 0.9921, 0.9923, 0.9922],
        [0.9918, 0.9918, 0.9919, 0.9921, 0.9920],
        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920],
        [0.9917, 0.9917, 0.9918, 0.9919, 0.9919],
        [0.9917, 0.9917, 0.9918, 0.9919, 0.9919],
        [0.9920, 0.9920, 0.9921, 0.9923, 0.9922],
        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920],
        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4340, 0.6338, 0.6627, 0.2929],
        [0.7396, 0.4193, 0.4667, 0.6436],
        [0.5308, 0.6141, 0.5883, 0.4861],
        [0.6814, 0.4507, 0.4201, 0.6860],
        [0.6191, 0.4340, 0.5581, 0.3697],
        [0.3956, 0.6770, 0.6935, 0.2681],
        [0.8400, 0.1923, 0.3216, 0.6537],
        [0.5330, 0.4354, 0.5455, 0.3042]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)]
====================
schedule for this step is
[(7, 1)]
inputting microbatch 7 into partition 1
before moving to cuda:1: tensor([[0.4340, 0.6338, 0.6627, 0.2929],
        [0.7396, 0.4193, 0.4667, 0.6436],
        [0.5308, 0.6141, 0.5883, 0.4861],
        [0.6814, 0.4507, 0.4201, 0.6860],
        [0.6191, 0.4340, 0.5581, 0.3697],
        [0.3956, 0.6770, 0.6935, 0.2681],
        [0.8400, 0.1923, 0.3216, 0.6537],
        [0.5330, 0.4354, 0.5455, 0.3042]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9922, 0.9922, 0.9923, 0.9924],
        [0.9924, 0.9922, 0.9922, 0.9923],
        [0.9924, 0.9924, 0.9922, 0.9922],
        [0.9923, 0.9924, 0.9924, 0.9922],
        [0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924],
        [0.9924, 0.9922, 0.9922, 0.9923],
        [0.9924, 0.9924, 0.9922, 0.9922]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 7
current batch shape is torch.Size([8, 4])
batch is tensor([[0.9922, 0.9922, 0.9923, 0.9924],
        [0.9924, 0.9922, 0.9922, 0.9923],
        [0.9924, 0.9924, 0.9922, 0.9922],
        [0.9923, 0.9924, 0.9924, 0.9922],
        [0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924],
        [0.9924, 0.9922, 0.9922, 0.9923],
        [0.9924, 0.9924, 0.9922, 0.9922]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 7
********************
receiving microbatch 7 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 7, tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 7
result is tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9920, 0.9920, 0.9921, 0.9923, 0.9922],
        [0.9918, 0.9918, 0.9919, 0.9921, 0.9920],
        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920],
        [0.9917, 0.9917, 0.9918, 0.9919, 0.9919],
        [0.9917, 0.9917, 0.9918, 0.9919, 0.9919],
        [0.9920, 0.9920, 0.9921, 0.9923, 0.9922],
        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920],
        [0.9918, 0.9918, 0.9919, 0.9920, 0.9920]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924],
        [0.9922, 0.9922, 0.9923, 0.9924, 0.9924]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)]
finished running cycle
_____________________________ test_forward_0[16-1] _____________________________

batch_size = 1, split_size = 16

    @pytest.mark.a4_2_2
    @pytest.mark.parametrize("batch_size", [1, 16, 32, 64])
    @pytest.mark.parametrize("split_size", [1, 2, 4, 8, 16])
    # @pytest.mark.parametrize("batch_size", [1])
    # @pytest.mark.parametrize("split_size", [1, 2])
    def test_forward_0(batch_size, split_size):
        model = nn.Sequential(
            nn.Linear(3, 4).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
            nn.Linear(4, 5).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
        )
    
        x = torch.randn(batch_size, 3).to('cuda:0')
        y0 = model(x).to('cpu')
    
        # move the last two layer to another device
        model[-2] = model[-2].to('cuda:1')
        model[-1] = WithDevice(nn.Sigmoid(), 'cuda:1')
        pipe = Pipe(model, split_size=split_size)
        y1 = pipe(x).to('cpu')
>       assert torch.allclose(y0, y1)
E       assert False
E        +  where False = <built-in method allclose of type object at 0x153da78cb760>(tensor([[0.5768, 0.4606, 0.5907, 0.5772, 0.6404]], grad_fn=<ToCopyBackward0>), tensor([[0.8090, 0.9665, 0.9627, 0.9516, 0.9540]], grad_fn=<ToCopyBackward0>))
E        +    where <built-in method allclose of type object at 0x153da78cb760> = torch.allclose

batch_size = 1
model      = Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
  (2): Linear(in_features=4, out_features=5, bias=True)
  (3): WithDevice(
    (_module): Sigmoid()
  )
)
pipe       = Pipe(
  (partitions): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=3, out_features=4, bias=True)
   ...inear(in_features=4, out_features=5, bias=True)
      (1): WithDevice(
        (_module): Sigmoid()
      )
    )
  )
)
split_size = 16
x          = tensor([[1.8759, 0.6206, 0.5658]], device='cuda:0')
y0         = tensor([[0.5768, 0.4606, 0.5907, 0.5772, 0.6404]], grad_fn=<ToCopyBackward0>)
y1         = tensor([[0.8090, 0.9665, 0.9627, 0.9516, 0.9540]], grad_fn=<ToCopyBackward0>)

tests/test_pipeline.py:83: AssertionError
----------------------------- Captured stdout call -----------------------------
in init
ModuleList(
  (0): Sequential(
    (0): Linear(in_features=3, out_features=4, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
  (1): Sequential(
    (0): Linear(in_features=4, out_features=5, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
)
[device(type='cuda', index=0), device(type='cuda', index=1)]
before running the cycle
[tensor([[1.8759, 0.6206, 0.5658]], device='cuda:0')]
====================
schedule for this step is
[(0, 0)]
inputting microbatch 0 into partition 0
before moving to cuda:0: tensor([[1.8759, 0.6206, 0.5658]], device='cuda:0')
after: tensor([[1.8759, 0.6206, 0.5658]], device='cuda:0')
********************
observing microbatch 0
current batch shape is torch.Size([1, 3])
batch is tensor([[1.8759, 0.6206, 0.5658]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 0
********************
receiving microbatch 0 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 0, tensor([[0.4484, 0.2078, 0.8096, 0.4905]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.4484, 0.2078, 0.8096, 0.4905]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4484, 0.2078, 0.8096, 0.4905]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.4484, 0.2078, 0.8096, 0.4905]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)]
====================
schedule for this step is
[(0, 1)]
inputting microbatch 0 into partition 1
before moving to cuda:1: tensor([[0.4484, 0.2078, 0.8096, 0.4905]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9220, 0.7980, 0.7202, 0.5453]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 0
current batch shape is torch.Size([1, 4])
batch is tensor([[0.9220, 0.7980, 0.7202, 0.5453]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 0
********************
receiving microbatch 0 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 0, tensor([[0.8090, 0.9665, 0.9627, 0.9516, 0.9540]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.8090, 0.9665, 0.9627, 0.9516, 0.9540]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.8090, 0.9665, 0.9627, 0.9516, 0.9540]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.8090, 0.9665, 0.9627, 0.9516, 0.9540]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)]
finished running cycle
____________________________ test_forward_0[16-16] _____________________________

batch_size = 16, split_size = 16

    @pytest.mark.a4_2_2
    @pytest.mark.parametrize("batch_size", [1, 16, 32, 64])
    @pytest.mark.parametrize("split_size", [1, 2, 4, 8, 16])
    # @pytest.mark.parametrize("batch_size", [1])
    # @pytest.mark.parametrize("split_size", [1, 2])
    def test_forward_0(batch_size, split_size):
        model = nn.Sequential(
            nn.Linear(3, 4).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
            nn.Linear(4, 5).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
        )
    
        x = torch.randn(batch_size, 3).to('cuda:0')
        y0 = model(x).to('cpu')
    
        # move the last two layer to another device
        model[-2] = model[-2].to('cuda:1')
        model[-1] = WithDevice(nn.Sigmoid(), 'cuda:1')
        pipe = Pipe(model, split_size=split_size)
        y1 = pipe(x).to('cpu')
>       assert torch.allclose(y0, y1)
E       assert False
E        +  where False = <built-in method allclose of type object at 0x153da78cb760>(tensor([[0.5266, 0.5234, 0.6245, 0.3462, 0.5887],\n        [0.4984, 0.4834, 0.5752, 0.3254, 0.5862],\n        [0.4362, 0.4495, 0.5020, 0.3248, 0.5707],\n        [0.5129, 0.4918, 0.5915, 0.3253, 0.5893],\n        [0.4980, 0.4964, 0.5856, 0.3362, 0.5855],\n        [0.5573, 0.5122, 0.6350, 0.3248, 0.5947],\n        [0.4964, 0.4894, 0.5787, 0.3306, 0.5858],\n        [0.5117, 0.4738, 0.5763, 0.3128, 0.5892],\n        [0.5307, 0.5027, 0.6113, 0.3250, 0.5928],\n        [0.5502, 0.5394, 0.6503, 0.3492, 0.5912],\n        [0.5178, 0.4984, 0.6001, 0.3291, 0.5897],\n        [0.5323, 0.5033, 0.6130, 0.3260, 0.5925],\n        [0.5244, 0.5139, 0.6164, 0.3401, 0.5892],\n        [0.5600, 0.5191, 0.6419, 0.3288, 0.5950],\n        [0.4224, 0.4507, 0.4937, 0.3333, 0.5670],\n        [0.5260, 0.5004, 0.6068, 0.3268, 0.5914]], grad_fn=<ToCopyBackward0>), tensor([[0.9992, 0.9985, 0.9996, 1.0000, 0.9996],\n        [0.9992, 0.9986, 0.9996, 1.0000, 0.9996],\n        [0.9966, 0.9916, 0.9981, 0.9981, 0.9980],\n        [0.9964, 0.9945, 0.9984, 0.9985, 0.9901],\n        [0.9964, 0.9945, 0.9984, 0.9985, 0.9896],\n        [0.9963, 0.9944, 0.9983, 0.9986, 0.9901],\n        [0.9963, 0.9944, 0.9983, 0.9985, 0.9893],\n        [0.9965, 0.9946, 0.9984, 0.9987, 0.9901],\n        [0.9964, 0.9945, 0.9984, 0.9985, 0.9901],\n        [0.9964, 0.9945, 0.9984, 0.9985, 0.9896],\n        [0.8852, 0.6848, 0.5018, 0.9805, 0.9996],\n        [0.9749, 0.9360, 0.8695, 0.9996, 0.9996],\n        [0.0000, 0.0000, 0.0000, 1.0000, 0.9996],\n        [0.9962, 0.9904, 0.9972, 0.9996, 0.9996],\n        [0.0000, 0.0000, 0.0000, 1.0000, 0.9996],\n        [0.9949, 0.9900, 0.9778, 1.0000, 0.9996]], grad_fn=<ToCopyBackward0>))
E        +    where <built-in method allclose of type object at 0x153da78cb760> = torch.allclose

batch_size = 16
model      = Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
  (2): Linear(in_features=4, out_features=5, bias=True)
  (3): WithDevice(
    (_module): Sigmoid()
  )
)
pipe       = Pipe(
  (partitions): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=3, out_features=4, bias=True)
   ...inear(in_features=4, out_features=5, bias=True)
      (1): WithDevice(
        (_module): Sigmoid()
      )
    )
  )
)
split_size = 16
x          = tensor([[-1.1803, -0.9390,  0.7459],
        [-0.0676,  0.3401,  0.0225],
        [-1.7284,  1.2885, -2.7904],
       ... 0.7902, -1.4748,  0.0765],
        [-1.5313,  2.8857,  0.2292],
        [-0.0928, -0.7071, -0.4267]], device='cuda:0')
y0         = tensor([[0.5266, 0.5234, 0.6245, 0.3462, 0.5887],
        [0.4984, 0.4834, 0.5752, 0.3254, 0.5862],
        [0.4362, 0...[0.4224, 0.4507, 0.4937, 0.3333, 0.5670],
        [0.5260, 0.5004, 0.6068, 0.3268, 0.5914]], grad_fn=<ToCopyBackward0>)
y1         = tensor([[0.9992, 0.9985, 0.9996, 1.0000, 0.9996],
        [0.9992, 0.9986, 0.9996, 1.0000, 0.9996],
        [0.9966, 0...[0.0000, 0.0000, 0.0000, 1.0000, 0.9996],
        [0.9949, 0.9900, 0.9778, 1.0000, 0.9996]], grad_fn=<ToCopyBackward0>)

tests/test_pipeline.py:83: AssertionError
----------------------------- Captured stdout call -----------------------------
in init
ModuleList(
  (0): Sequential(
    (0): Linear(in_features=3, out_features=4, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
  (1): Sequential(
    (0): Linear(in_features=4, out_features=5, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
)
[device(type='cuda', index=0), device(type='cuda', index=1)]
before running the cycle
[tensor([[-1.1803, -0.9390,  0.7459],
        [-0.0676,  0.3401,  0.0225],
        [-1.7284,  1.2885, -2.7904],
        [-0.1730, -0.3198, -0.4844],
        [-0.8140,  0.0416,  0.3158],
        [ 1.1551, -1.2433,  0.0309],
        [-0.7120, -0.0127, -0.3456],
        [ 1.5424,  0.8585,  0.7123],
        [-0.6923, -1.5576, -2.0126],
        [-1.0340, -1.5282,  1.3137],
        [-0.1613, -0.3472,  0.0331],
        [-0.1490, -1.0657, -0.8614],
        [-0.2398, -0.2142,  1.5629],
        [ 0.7902, -1.4748,  0.0765],
        [-1.5313,  2.8857,  0.2292],
        [-0.0928, -0.7071, -0.4267]], device='cuda:0')]
====================
schedule for this step is
[(0, 0)]
inputting microbatch 0 into partition 0
before moving to cuda:0: tensor([[-1.1803, -0.9390,  0.7459],
        [-0.0676,  0.3401,  0.0225],
        [-1.7284,  1.2885, -2.7904],
        [-0.1730, -0.3198, -0.4844],
        [-0.8140,  0.0416,  0.3158],
        [ 1.1551, -1.2433,  0.0309],
        [-0.7120, -0.0127, -0.3456],
        [ 1.5424,  0.8585,  0.7123],
        [-0.6923, -1.5576, -2.0126],
        [-1.0340, -1.5282,  1.3137],
        [-0.1613, -0.3472,  0.0331],
        [-0.1490, -1.0657, -0.8614],
        [-0.2398, -0.2142,  1.5629],
        [ 0.7902, -1.4748,  0.0765],
        [-1.5313,  2.8857,  0.2292],
        [-0.0928, -0.7071, -0.4267]], device='cuda:0')
after: tensor([[-1.1803, -0.9390,  0.7459],
        [-0.0676,  0.3401,  0.0225],
        [-1.7284,  1.2885, -2.7904],
        [-0.1730, -0.3198, -0.4844],
        [-0.8140,  0.0416,  0.3158],
        [ 1.1551, -1.2433,  0.0309],
        [-0.7120, -0.0127, -0.3456],
        [ 1.5424,  0.8585,  0.7123],
        [-0.6923, -1.5576, -2.0126],
        [-1.0340, -1.5282,  1.3137],
        [-0.1613, -0.3472,  0.0331],
        [-0.1490, -1.0657, -0.8614],
        [-0.2398, -0.2142,  1.5629],
        [ 0.7902, -1.4748,  0.0765],
        [-1.5313,  2.8857,  0.2292],
        [-0.0928, -0.7071, -0.4267]], device='cuda:0')
********************
observing microbatch 0
current batch shape is torch.Size([16, 3])
batch is tensor([[-1.1803, -0.9390,  0.7459],
        [-0.0676,  0.3401,  0.0225],
        [-1.7284,  1.2885, -2.7904],
        [-0.1730, -0.3198, -0.4844],
        [-0.8140,  0.0416,  0.3158],
        [ 1.1551, -1.2433,  0.0309],
        [-0.7120, -0.0127, -0.3456],
        [ 1.5424,  0.8585,  0.7123],
        [-0.6923, -1.5576, -2.0126],
        [-1.0340, -1.5282,  1.3137],
        [-0.1613, -0.3472,  0.0331],
        [-0.1490, -1.0657, -0.8614],
        [-0.2398, -0.2142,  1.5629],
        [ 0.7902, -1.4748,  0.0765],
        [-1.5313,  2.8857,  0.2292],
        [-0.0928, -0.7071, -0.4267]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 0
********************
receiving microbatch 0 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 0, tensor([[0.5915, 0.2511, 0.7275, 0.5813],
        [0.5619, 0.5200, 0.4597, 0.3501],
        [0.2419, 0.8044, 0.3871, 0.1297],
        [0.6300, 0.4682, 0.4938, 0.4068],
        [0.5082, 0.4221, 0.5903, 0.4118],
        [0.8453, 0.3438, 0.4516, 0.5678],
        [0.5252, 0.4789, 0.5398, 0.3759],
        [0.6910, 0.5910, 0.2885, 0.3205],
        [0.7089, 0.4129, 0.5565, 0.4808],
        [0.6822, 0.1653, 0.7798, 0.6939],
        [0.6369, 0.4175, 0.5292, 0.4451],
        [0.7168, 0.3983, 0.5288, 0.4868],
        [0.6181, 0.3025, 0.6265, 0.5304],
        [0.8370, 0.2990, 0.5166, 0.6042],
        [0.1390, 0.7761, 0.4294, 0.1193],
        [0.6850, 0.4112, 0.5210, 0.4649]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.5915, 0.2511, 0.7275, 0.5813],
        [0.5619, 0.5200, 0.4597, 0.3501],
        [0.2419, 0.8044, 0.3871, 0.1297],
        [0.6300, 0.4682, 0.4938, 0.4068],
        [0.5082, 0.4221, 0.5903, 0.4118],
        [0.8453, 0.3438, 0.4516, 0.5678],
        [0.5252, 0.4789, 0.5398, 0.3759],
        [0.6910, 0.5910, 0.2885, 0.3205],
        [0.7089, 0.4129, 0.5565, 0.4808],
        [0.6822, 0.1653, 0.7798, 0.6939],
        [0.6369, 0.4175, 0.5292, 0.4451],
        [0.7168, 0.3983, 0.5288, 0.4868],
        [0.6181, 0.3025, 0.6265, 0.5304],
        [0.8370, 0.2990, 0.5166, 0.6042],
        [0.1390, 0.7761, 0.4294, 0.1193],
        [0.6850, 0.4112, 0.5210, 0.4649]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5915, 0.2511, 0.7275, 0.5813],
        [0.5619, 0.5200, 0.4597, 0.3501],
        [0.2419, 0.8044, 0.3871, 0.1297],
        [0.6300, 0.4682, 0.4938, 0.4068],
        [0.5082, 0.4221, 0.5903, 0.4118],
        [0.8453, 0.3438, 0.4516, 0.5678],
        [0.5252, 0.4789, 0.5398, 0.3759],
        [0.6910, 0.5910, 0.2885, 0.3205],
        [0.7089, 0.4129, 0.5565, 0.4808],
        [0.6822, 0.1653, 0.7798, 0.6939],
        [0.6369, 0.4175, 0.5292, 0.4451],
        [0.7168, 0.3983, 0.5288, 0.4868],
        [0.6181, 0.3025, 0.6265, 0.5304],
        [0.8370, 0.2990, 0.5166, 0.6042],
        [0.1390, 0.7761, 0.4294, 0.1193],
        [0.6850, 0.4112, 0.5210, 0.4649]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.5915, 0.2511, 0.7275, 0.5813],
        [0.5619, 0.5200, 0.4597, 0.3501],
        [0.2419, 0.8044, 0.3871, 0.1297],
        [0.6300, 0.4682, 0.4938, 0.4068],
        [0.5082, 0.4221, 0.5903, 0.4118],
        [0.8453, 0.3438, 0.4516, 0.5678],
        [0.5252, 0.4789, 0.5398, 0.3759],
        [0.6910, 0.5910, 0.2885, 0.3205],
        [0.7089, 0.4129, 0.5565, 0.4808],
        [0.6822, 0.1653, 0.7798, 0.6939],
        [0.6369, 0.4175, 0.5292, 0.4451],
        [0.7168, 0.3983, 0.5288, 0.4868],
        [0.6181, 0.3025, 0.6265, 0.5304],
        [0.8370, 0.2990, 0.5166, 0.6042],
        [0.1390, 0.7761, 0.4294, 0.1193],
        [0.6850, 0.4112, 0.5210, 0.4649]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)]
====================
schedule for this step is
[(0, 1)]
inputting microbatch 0 into partition 1
before moving to cuda:1: tensor([[0.5915, 0.2511, 0.7275, 0.5813],
        [0.5619, 0.5200, 0.4597, 0.3501],
        [0.2419, 0.8044, 0.3871, 0.1297],
        [0.6300, 0.4682, 0.4938, 0.4068],
        [0.5082, 0.4221, 0.5903, 0.4118],
        [0.8453, 0.3438, 0.4516, 0.5678],
        [0.5252, 0.4789, 0.5398, 0.3759],
        [0.6910, 0.5910, 0.2885, 0.3205],
        [0.7089, 0.4129, 0.5565, 0.4808],
        [0.6822, 0.1653, 0.7798, 0.6939],
        [0.6369, 0.4175, 0.5292, 0.4451],
        [0.7168, 0.3983, 0.5288, 0.4868],
        [0.6181, 0.3025, 0.6265, 0.5304],
        [0.8370, 0.2990, 0.5166, 0.6042],
        [0.1390, 0.7761, 0.4294, 0.1193],
        [0.6850, 0.4112, 0.5210, 0.4649]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[ 2.0000e+00,  1.8273e+00,  0.0000e+00,  1.8666e+00],
        [ 2.0000e+00,  1.8657e+00,  1.0842e-19,  1.8629e+00],
        [ 1.0842e-19,  1.8635e+00,  9.8106e-01,  9.7930e-01],
        [ 9.5045e-01,  9.9289e-01,  9.9266e-01,  9.8052e-01],
        [ 9.7846e-01,  9.5102e-01,  9.9269e-01,  9.9245e-01],
        [ 9.8044e-01,  9.7813e-01,  9.5155e-01,  9.9266e-01],
        [ 9.9242e-01,  9.8020e-01,  9.7873e-01,  9.4962e-01],
        [ 9.9257e-01,  9.9233e-01,  9.8027e-01,  9.7906e-01],
        [ 9.4902e-01,  9.9259e-01,  9.9235e-01,  9.8102e-01],
        [ 9.7931e-01,  9.5035e-01,  9.9288e-01,  9.9265e-01],
        [-2.0000e+00,  1.8701e+00, -2.0000e+00,  1.8695e+00],
        [ 0.0000e+00,  1.8629e+00, -2.0000e+00,  1.8731e+00],
        [-0.0000e+00,  1.8731e+00, -3.6893e+19,  1.8705e+00],
        [ 0.0000e+00,  1.8699e+00,  0.0000e+00,  1.8630e+00],
        [ 0.0000e+00,  1.8733e+00, -3.6893e+19,  1.8732e+00],
        [ 2.0000e+00,  1.8700e+00, -2.0000e+00,  1.8698e+00]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 0
current batch shape is torch.Size([16, 4])
batch is tensor([[ 2.0000e+00,  1.8273e+00,  0.0000e+00,  1.8666e+00],
        [ 2.0000e+00,  1.8657e+00,  1.0842e-19,  1.8629e+00],
        [ 1.0842e-19,  1.8635e+00,  9.8106e-01,  9.7930e-01],
        [ 9.5045e-01,  9.9289e-01,  9.9266e-01,  9.8052e-01],
        [ 9.7846e-01,  9.5102e-01,  9.9269e-01,  9.9245e-01],
        [ 9.8044e-01,  9.7813e-01,  9.5155e-01,  9.9266e-01],
        [ 9.9242e-01,  9.8020e-01,  9.7873e-01,  9.4962e-01],
        [ 9.9257e-01,  9.9233e-01,  9.8027e-01,  9.7906e-01],
        [ 9.4902e-01,  9.9259e-01,  9.9235e-01,  9.8102e-01],
        [ 9.7931e-01,  9.5035e-01,  9.9288e-01,  9.9265e-01],
        [-2.0000e+00,  1.8701e+00, -2.0000e+00,  1.8695e+00],
        [ 0.0000e+00,  1.8629e+00, -2.0000e+00,  1.8731e+00],
        [-0.0000e+00,  1.8731e+00, -3.6893e+19,  1.8705e+00],
        [ 0.0000e+00,  1.8699e+00,  0.0000e+00,  1.8630e+00],
        [ 0.0000e+00,  1.8733e+00, -3.6893e+19,  1.8732e+00],
        [ 2.0000e+00,  1.8700e+00, -2.0000e+00,  1.8698e+00]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 0
********************
receiving microbatch 0 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 0, tensor([[0.9992, 0.9985, 0.9996, 1.0000, 0.9996],
        [0.9992, 0.9986, 0.9996, 1.0000, 0.9996],
        [0.9966, 0.9916, 0.9981, 0.9981, 0.9980],
        [0.9964, 0.9945, 0.9984, 0.9985, 0.9901],
        [0.9964, 0.9945, 0.9984, 0.9985, 0.9896],
        [0.9963, 0.9944, 0.9983, 0.9986, 0.9901],
        [0.9963, 0.9944, 0.9983, 0.9985, 0.9893],
        [0.9965, 0.9946, 0.9984, 0.9987, 0.9901],
        [0.9964, 0.9945, 0.9984, 0.9985, 0.9901],
        [0.9964, 0.9945, 0.9984, 0.9985, 0.9896],
        [0.8852, 0.6848, 0.5018, 0.9805, 0.9996],
        [0.9749, 0.9360, 0.8695, 0.9996, 0.9996],
        [0.0000, 0.0000, 0.0000, 1.0000, 0.9996],
        [0.9962, 0.9904, 0.9972, 0.9996, 0.9996],
        [0.0000, 0.0000, 0.0000, 1.0000, 0.9996],
        [0.9949, 0.9900, 0.9778, 1.0000, 0.9996]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.9992, 0.9985, 0.9996, 1.0000, 0.9996],
        [0.9992, 0.9986, 0.9996, 1.0000, 0.9996],
        [0.9966, 0.9916, 0.9981, 0.9981, 0.9980],
        [0.9964, 0.9945, 0.9984, 0.9985, 0.9901],
        [0.9964, 0.9945, 0.9984, 0.9985, 0.9896],
        [0.9963, 0.9944, 0.9983, 0.9986, 0.9901],
        [0.9963, 0.9944, 0.9983, 0.9985, 0.9893],
        [0.9965, 0.9946, 0.9984, 0.9987, 0.9901],
        [0.9964, 0.9945, 0.9984, 0.9985, 0.9901],
        [0.9964, 0.9945, 0.9984, 0.9985, 0.9896],
        [0.8852, 0.6848, 0.5018, 0.9805, 0.9996],
        [0.9749, 0.9360, 0.8695, 0.9996, 0.9996],
        [0.0000, 0.0000, 0.0000, 1.0000, 0.9996],
        [0.9962, 0.9904, 0.9972, 0.9996, 0.9996],
        [0.0000, 0.0000, 0.0000, 1.0000, 0.9996],
        [0.9949, 0.9900, 0.9778, 1.0000, 0.9996]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9992, 0.9985, 0.9996, 1.0000, 0.9996],
        [0.9992, 0.9986, 0.9996, 1.0000, 0.9996],
        [0.9966, 0.9916, 0.9981, 0.9981, 0.9980],
        [0.9964, 0.9945, 0.9984, 0.9985, 0.9901],
        [0.9964, 0.9945, 0.9984, 0.9985, 0.9896],
        [0.9963, 0.9944, 0.9983, 0.9986, 0.9901],
        [0.9963, 0.9944, 0.9983, 0.9985, 0.9893],
        [0.9965, 0.9946, 0.9984, 0.9987, 0.9901],
        [0.9964, 0.9945, 0.9984, 0.9985, 0.9901],
        [0.9964, 0.9945, 0.9984, 0.9985, 0.9896],
        [0.8852, 0.6848, 0.5018, 0.9805, 0.9996],
        [0.9749, 0.9360, 0.8695, 0.9996, 0.9996],
        [0.0000, 0.0000, 0.0000, 1.0000, 0.9996],
        [0.9962, 0.9904, 0.9972, 0.9996, 0.9996],
        [0.0000, 0.0000, 0.0000, 1.0000, 0.9996],
        [0.9949, 0.9900, 0.9778, 1.0000, 0.9996]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9992, 0.9985, 0.9996, 1.0000, 0.9996],
        [0.9992, 0.9986, 0.9996, 1.0000, 0.9996],
        [0.9966, 0.9916, 0.9981, 0.9981, 0.9980],
        [0.9964, 0.9945, 0.9984, 0.9985, 0.9901],
        [0.9964, 0.9945, 0.9984, 0.9985, 0.9896],
        [0.9963, 0.9944, 0.9983, 0.9986, 0.9901],
        [0.9963, 0.9944, 0.9983, 0.9985, 0.9893],
        [0.9965, 0.9946, 0.9984, 0.9987, 0.9901],
        [0.9964, 0.9945, 0.9984, 0.9985, 0.9901],
        [0.9964, 0.9945, 0.9984, 0.9985, 0.9896],
        [0.8852, 0.6848, 0.5018, 0.9805, 0.9996],
        [0.9749, 0.9360, 0.8695, 0.9996, 0.9996],
        [0.0000, 0.0000, 0.0000, 1.0000, 0.9996],
        [0.9962, 0.9904, 0.9972, 0.9996, 0.9996],
        [0.0000, 0.0000, 0.0000, 1.0000, 0.9996],
        [0.9949, 0.9900, 0.9778, 1.0000, 0.9996]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)]
finished running cycle
____________________________ test_forward_0[16-32] _____________________________

batch_size = 32, split_size = 16

    @pytest.mark.a4_2_2
    @pytest.mark.parametrize("batch_size", [1, 16, 32, 64])
    @pytest.mark.parametrize("split_size", [1, 2, 4, 8, 16])
    # @pytest.mark.parametrize("batch_size", [1])
    # @pytest.mark.parametrize("split_size", [1, 2])
    def test_forward_0(batch_size, split_size):
        model = nn.Sequential(
            nn.Linear(3, 4).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
            nn.Linear(4, 5).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
        )
    
        x = torch.randn(batch_size, 3).to('cuda:0')
        y0 = model(x).to('cpu')
    
        # move the last two layer to another device
        model[-2] = model[-2].to('cuda:1')
        model[-1] = WithDevice(nn.Sigmoid(), 'cuda:1')
        pipe = Pipe(model, split_size=split_size)
        y1 = pipe(x).to('cpu')
>       assert torch.allclose(y0, y1)
E       assert False
E        +  where False = <built-in method allclose of type object at 0x153da78cb760>(tensor([[0.4990, 0.5639, 0.5290, 0.4253, 0.5471],\n        [0.4863, 0.5773, 0.5548, 0.4605, 0.5267],\n        [0.4914, 0.5737, 0.5176, 0.4302, 0.5438],\n        [0.4788, 0.5893, 0.5143, 0.4463, 0.5334],\n        [0.4742, 0.5951, 0.5121, 0.4507, 0.5303],\n        [0.4888, 0.5762, 0.5274, 0.4422, 0.5373],\n        [0.4951, 0.5681, 0.5382, 0.4380, 0.5399],\n        [0.4894, 0.5758, 0.5252, 0.4364, 0.5399],\n        [0.4860, 0.5811, 0.5045, 0.4275, 0.5441],\n        [0.5112, 0.5495, 0.5399, 0.4145, 0.5539],\n        [0.4705, 0.5984, 0.5324, 0.4761, 0.5174],\n        [0.4959, 0.5678, 0.5271, 0.4262, 0.5458],\n        [0.4944, 0.5703, 0.5149, 0.4194, 0.5490],\n        [0.4933, 0.5701, 0.5426, 0.4405, 0.5378],\n        [0.4878, 0.5762, 0.5456, 0.4556, 0.5301],\n        [0.4743, 0.5951, 0.5105, 0.4534, 0.5295],\n        [0.4759, 0.5921, 0.5264, 0.4608, 0.5257],\n        [0.4940, 0.5697, 0.5336, 0.4366, 0.5406],\n        [0.4640, 0.6092, 0.4783, 0.4478, 0.5313],\n        [0.4823, 0.5824, 0.5487, 0.4672, 0.5236],\n        [0.4903, 0.5750, 0.5177, 0.4292, 0.5437],\n        [0.4932, 0.5716, 0.5171, 0.4244, 0.5465],\n        [0.4966, 0.5673, 0.5203, 0.4223, 0.5482],\n        [0.4948, 0.5683, 0.5443, 0.4391, 0.5386],\n        [0.5024, 0.5607, 0.5122, 0.4096, 0.5560],\n        [0.4987, 0.5654, 0.5106, 0.4116, 0.5540],\n        [0.4878, 0.5772, 0.5335, 0.4441, 0.5355],\n        [0.4916, 0.5736, 0.5151, 0.4258, 0.5457],\n        [0.4851, 0.5813, 0.5191, 0.4375, 0.5385],\n        [0.4918, 0.5735, 0.5156, 0.4237, 0.5464],\n        [0.4725, 0.5969, 0.5168, 0.4567, 0.5269],\n        [0.4897, 0.5756, 0.5222, 0.4340, 0.5411]], grad_fn=<ToCopyBackward0>), tensor([[0.9796, 0.9964, 0.9924, 0.9924, 0.9923],\n        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],\n        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],\n        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],\n        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],\n        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],\n        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],\n        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],\n        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],\n        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],\n        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],\n        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],\n        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],\n        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],\n        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],\n        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],\n        [0.9813, 0.9967, 0.9930, 0.9930, 0.9929],\n        [0.9558, 0.9920, 0.9831, 0.9832, 0.9830],\n        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],\n        [0.9365, 0.9884, 0.9755, 0.9756, 0.9754],\n        [0.9545, 0.9918, 0.9826, 0.9827, 0.9825],\n        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],\n        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],\n        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],\n        [0.9365, 0.9884, 0.9755, 0.9756, 0.9754],\n        [0.9545, 0.9918, 0.9826, 0.9827, 0.9825],\n        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],\n        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],\n        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],\n        [0.9365, 0.9884, 0.9755, 0.9756, 0.9754],\n        [0.9545, 0.9918, 0.9826, 0.9827, 0.9825],\n        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781]], grad_fn=<ToCopyBackward0>))
E        +    where <built-in method allclose of type object at 0x153da78cb760> = torch.allclose

batch_size = 32
model      = Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
  (2): Linear(in_features=4, out_features=5, bias=True)
  (3): WithDevice(
    (_module): Sigmoid()
  )
)
pipe       = Pipe(
  (partitions): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=3, out_features=4, bias=True)
   ...inear(in_features=4, out_features=5, bias=True)
      (1): WithDevice(
        (_module): Sigmoid()
      )
    )
  )
)
split_size = 16
x          = tensor([[-1.3940, -0.0832,  0.0580],
        [ 1.1821, -1.5684, -0.7958],
        [-1.5847,  0.9482, -0.4474],
       ... 0.9753, -0.0985,  1.2677],
        [ 2.2637,  0.3285, -0.0991],
        [-0.1570,  0.2406,  0.0794]], device='cuda:0')
y0         = tensor([[0.4990, 0.5639, 0.5290, 0.4253, 0.5471],
        [0.4863, 0.5773, 0.5548, 0.4605, 0.5267],
        [0.4914, 0...[0.4725, 0.5969, 0.5168, 0.4567, 0.5269],
        [0.4897, 0.5756, 0.5222, 0.4340, 0.5411]], grad_fn=<ToCopyBackward0>)
y1         = tensor([[0.9796, 0.9964, 0.9924, 0.9924, 0.9923],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0...[0.9545, 0.9918, 0.9826, 0.9827, 0.9825],
        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781]], grad_fn=<ToCopyBackward0>)

tests/test_pipeline.py:83: AssertionError
----------------------------- Captured stdout call -----------------------------
in init
ModuleList(
  (0): Sequential(
    (0): Linear(in_features=3, out_features=4, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
  (1): Sequential(
    (0): Linear(in_features=4, out_features=5, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
)
[device(type='cuda', index=0), device(type='cuda', index=1)]
before running the cycle
[tensor([[-1.3940, -0.0832,  0.0580],
        [ 1.1821, -1.5684, -0.7958],
        [-1.5847,  0.9482, -0.4474],
        [ 0.5668,  0.8333, -0.3488],
        [ 1.8611,  0.6005,  0.0340],
        [-1.7871,  0.7838, -1.2311],
        [-1.1340, -0.3448, -0.5597],
        [ 0.0442,  0.0497,  0.0590],
        [-0.2217,  1.1789,  0.3396],
        [-1.2053, -1.7204,  1.1200],
        [ 0.6318,  0.4822, -2.1296],
        [ 0.4213, -0.5829,  0.9189],
        [ 0.6910, -0.0996,  1.3983],
        [ 1.1809, -1.4240,  0.5004],
        [-0.9537, -0.3033, -1.5591],
        [-0.1773,  1.5433, -1.1909]], device='cuda:0'), tensor([[ 0.5869,  0.4903, -1.1721],
        [-1.0935, -0.0840, -0.4801],
        [ 0.4509,  3.2593, -0.7471],
        [-0.7337, -0.2373, -2.2127],
        [ 0.0502,  0.2972,  0.4588],
        [-0.1803,  0.2599,  0.6386],
        [-0.8804,  0.2269,  0.4443],
        [ 1.3583, -1.6737,  0.7033],
        [-2.6681,  0.9959,  0.3140],
        [-0.7459,  0.4595,  1.1513],
        [ 0.6444, -0.4634, -0.0620],
        [-0.2749,  0.4713,  0.4886],
        [ 1.0372,  0.1009,  0.4576],
        [ 0.9753, -0.0985,  1.2677],
        [ 2.2637,  0.3285, -0.0991],
        [-0.1570,  0.2406,  0.0794]], device='cuda:0')]
====================
schedule for this step is
[(0, 0)]
inputting microbatch 0 into partition 0
before moving to cuda:0: tensor([[-1.3940, -0.0832,  0.0580],
        [ 1.1821, -1.5684, -0.7958],
        [-1.5847,  0.9482, -0.4474],
        [ 0.5668,  0.8333, -0.3488],
        [ 1.8611,  0.6005,  0.0340],
        [-1.7871,  0.7838, -1.2311],
        [-1.1340, -0.3448, -0.5597],
        [ 0.0442,  0.0497,  0.0590],
        [-0.2217,  1.1789,  0.3396],
        [-1.2053, -1.7204,  1.1200],
        [ 0.6318,  0.4822, -2.1296],
        [ 0.4213, -0.5829,  0.9189],
        [ 0.6910, -0.0996,  1.3983],
        [ 1.1809, -1.4240,  0.5004],
        [-0.9537, -0.3033, -1.5591],
        [-0.1773,  1.5433, -1.1909]], device='cuda:0')
after: tensor([[-1.3940, -0.0832,  0.0580],
        [ 1.1821, -1.5684, -0.7958],
        [-1.5847,  0.9482, -0.4474],
        [ 0.5668,  0.8333, -0.3488],
        [ 1.8611,  0.6005,  0.0340],
        [-1.7871,  0.7838, -1.2311],
        [-1.1340, -0.3448, -0.5597],
        [ 0.0442,  0.0497,  0.0590],
        [-0.2217,  1.1789,  0.3396],
        [-1.2053, -1.7204,  1.1200],
        [ 0.6318,  0.4822, -2.1296],
        [ 0.4213, -0.5829,  0.9189],
        [ 0.6910, -0.0996,  1.3983],
        [ 1.1809, -1.4240,  0.5004],
        [-0.9537, -0.3033, -1.5591],
        [-0.1773,  1.5433, -1.1909]], device='cuda:0')
********************
observing microbatch 0
current batch shape is torch.Size([16, 3])
batch is tensor([[-1.3940, -0.0832,  0.0580],
        [ 1.1821, -1.5684, -0.7958],
        [-1.5847,  0.9482, -0.4474],
        [ 0.5668,  0.8333, -0.3488],
        [ 1.8611,  0.6005,  0.0340],
        [-1.7871,  0.7838, -1.2311],
        [-1.1340, -0.3448, -0.5597],
        [ 0.0442,  0.0497,  0.0590],
        [-0.2217,  1.1789,  0.3396],
        [-1.2053, -1.7204,  1.1200],
        [ 0.6318,  0.4822, -2.1296],
        [ 0.4213, -0.5829,  0.9189],
        [ 0.6910, -0.0996,  1.3983],
        [ 1.1809, -1.4240,  0.5004],
        [-0.9537, -0.3033, -1.5591],
        [-0.1773,  1.5433, -1.1909]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 0
********************
receiving microbatch 0 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 0, tensor([[0.2857, 0.4971, 0.6021, 0.6748],
        [0.3104, 0.7636, 0.4449, 0.5861],
        [0.3851, 0.4640, 0.5351, 0.5957],
        [0.5074, 0.5301, 0.4245, 0.4997],
        [0.5557, 0.5471, 0.3906, 0.4720],
        [0.3757, 0.5577, 0.4910, 0.5670],
        [0.2899, 0.5939, 0.5495, 0.6419],
        [0.3806, 0.5316, 0.5160, 0.5993],
        [0.4752, 0.4022, 0.5106, 0.5626],
        [0.1563, 0.5172, 0.7225, 0.7963],
        [0.5211, 0.7376, 0.3036, 0.4057],
        [0.3201, 0.4990, 0.5858, 0.6680],
        [0.3714, 0.4161, 0.5897, 0.6564],
        [0.2944, 0.6340, 0.5418, 0.6554],
        [0.3263, 0.6971, 0.4587, 0.5700],
        [0.5578, 0.5428, 0.3756, 0.4411]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.2857, 0.4971, 0.6021, 0.6748],
        [0.3104, 0.7636, 0.4449, 0.5861],
        [0.3851, 0.4640, 0.5351, 0.5957],
        [0.5074, 0.5301, 0.4245, 0.4997],
        [0.5557, 0.5471, 0.3906, 0.4720],
        [0.3757, 0.5577, 0.4910, 0.5670],
        [0.2899, 0.5939, 0.5495, 0.6419],
        [0.3806, 0.5316, 0.5160, 0.5993],
        [0.4752, 0.4022, 0.5106, 0.5626],
        [0.1563, 0.5172, 0.7225, 0.7963],
        [0.5211, 0.7376, 0.3036, 0.4057],
        [0.3201, 0.4990, 0.5858, 0.6680],
        [0.3714, 0.4161, 0.5897, 0.6564],
        [0.2944, 0.6340, 0.5418, 0.6554],
        [0.3263, 0.6971, 0.4587, 0.5700],
        [0.5578, 0.5428, 0.3756, 0.4411]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.2857, 0.4971, 0.6021, 0.6748],
        [0.3104, 0.7636, 0.4449, 0.5861],
        [0.3851, 0.4640, 0.5351, 0.5957],
        [0.5074, 0.5301, 0.4245, 0.4997],
        [0.5557, 0.5471, 0.3906, 0.4720],
        [0.3757, 0.5577, 0.4910, 0.5670],
        [0.2899, 0.5939, 0.5495, 0.6419],
        [0.3806, 0.5316, 0.5160, 0.5993],
        [0.4752, 0.4022, 0.5106, 0.5626],
        [0.1563, 0.5172, 0.7225, 0.7963],
        [0.5211, 0.7376, 0.3036, 0.4057],
        [0.3201, 0.4990, 0.5858, 0.6680],
        [0.3714, 0.4161, 0.5897, 0.6564],
        [0.2944, 0.6340, 0.5418, 0.6554],
        [0.3263, 0.6971, 0.4587, 0.5700],
        [0.5578, 0.5428, 0.3756, 0.4411]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.2857, 0.4971, 0.6021, 0.6748],
        [0.3104, 0.7636, 0.4449, 0.5861],
        [0.3851, 0.4640, 0.5351, 0.5957],
        [0.5074, 0.5301, 0.4245, 0.4997],
        [0.5557, 0.5471, 0.3906, 0.4720],
        [0.3757, 0.5577, 0.4910, 0.5670],
        [0.2899, 0.5939, 0.5495, 0.6419],
        [0.3806, 0.5316, 0.5160, 0.5993],
        [0.4752, 0.4022, 0.5106, 0.5626],
        [0.1563, 0.5172, 0.7225, 0.7963],
        [0.5211, 0.7376, 0.3036, 0.4057],
        [0.3201, 0.4990, 0.5858, 0.6680],
        [0.3714, 0.4161, 0.5897, 0.6564],
        [0.2944, 0.6340, 0.5418, 0.6554],
        [0.3263, 0.6971, 0.4587, 0.5700],
        [0.5578, 0.5428, 0.3756, 0.4411]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[ 0.5869,  0.4903, -1.1721],
        [-1.0935, -0.0840, -0.4801],
        [ 0.4509,  3.2593, -0.7471],
        [-0.7337, -0.2373, -2.2127],
        [ 0.0502,  0.2972,  0.4588],
        [-0.1803,  0.2599,  0.6386],
        [-0.8804,  0.2269,  0.4443],
        [ 1.3583, -1.6737,  0.7033],
        [-2.6681,  0.9959,  0.3140],
        [-0.7459,  0.4595,  1.1513],
        [ 0.6444, -0.4634, -0.0620],
        [-0.2749,  0.4713,  0.4886],
        [ 1.0372,  0.1009,  0.4576],
        [ 0.9753, -0.0985,  1.2677],
        [ 2.2637,  0.3285, -0.0991],
        [-0.1570,  0.2406,  0.0794]], device='cuda:0')]
====================
schedule for this step is
[(1, 0), (0, 1)]
inputting microbatch 1 into partition 0
before moving to cuda:0: tensor([[ 0.5869,  0.4903, -1.1721],
        [-1.0935, -0.0840, -0.4801],
        [ 0.4509,  3.2593, -0.7471],
        [-0.7337, -0.2373, -2.2127],
        [ 0.0502,  0.2972,  0.4588],
        [-0.1803,  0.2599,  0.6386],
        [-0.8804,  0.2269,  0.4443],
        [ 1.3583, -1.6737,  0.7033],
        [-2.6681,  0.9959,  0.3140],
        [-0.7459,  0.4595,  1.1513],
        [ 0.6444, -0.4634, -0.0620],
        [-0.2749,  0.4713,  0.4886],
        [ 1.0372,  0.1009,  0.4576],
        [ 0.9753, -0.0985,  1.2677],
        [ 2.2637,  0.3285, -0.0991],
        [-0.1570,  0.2406,  0.0794]], device='cuda:0')
after: tensor([[ 0.5869,  0.4903, -1.1721],
        [-1.0935, -0.0840, -0.4801],
        [ 0.4509,  3.2593, -0.7471],
        [-0.7337, -0.2373, -2.2127],
        [ 0.0502,  0.2972,  0.4588],
        [-0.1803,  0.2599,  0.6386],
        [-0.8804,  0.2269,  0.4443],
        [ 1.3583, -1.6737,  0.7033],
        [-2.6681,  0.9959,  0.3140],
        [-0.7459,  0.4595,  1.1513],
        [ 0.6444, -0.4634, -0.0620],
        [-0.2749,  0.4713,  0.4886],
        [ 1.0372,  0.1009,  0.4576],
        [ 0.9753, -0.0985,  1.2677],
        [ 2.2637,  0.3285, -0.0991],
        [-0.1570,  0.2406,  0.0794]], device='cuda:0')
********************
observing microbatch 1
current batch shape is torch.Size([16, 3])
batch is tensor([[ 0.5869,  0.4903, -1.1721],
        [-1.0935, -0.0840, -0.4801],
        [ 0.4509,  3.2593, -0.7471],
        [-0.7337, -0.2373, -2.2127],
        [ 0.0502,  0.2972,  0.4588],
        [-0.1803,  0.2599,  0.6386],
        [-0.8804,  0.2269,  0.4443],
        [ 1.3583, -1.6737,  0.7033],
        [-2.6681,  0.9959,  0.3140],
        [-0.7459,  0.4595,  1.1513],
        [ 0.6444, -0.4634, -0.0620],
        [-0.2749,  0.4713,  0.4886],
        [ 1.0372,  0.1009,  0.4576],
        [ 0.9753, -0.0985,  1.2677],
        [ 2.2637,  0.3285, -0.0991],
        [-0.1570,  0.2406,  0.0794]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 1
********************
inputting microbatch 0 into partition 1
before moving to cuda:1: tensor([[0.2857, 0.4971, 0.6021, 0.6748],
        [0.3104, 0.7636, 0.4449, 0.5861],
        [0.3851, 0.4640, 0.5351, 0.5957],
        [0.5074, 0.5301, 0.4245, 0.4997],
        [0.5557, 0.5471, 0.3906, 0.4720],
        [0.3757, 0.5577, 0.4910, 0.5670],
        [0.2899, 0.5939, 0.5495, 0.6419],
        [0.3806, 0.5316, 0.5160, 0.5993],
        [0.4752, 0.4022, 0.5106, 0.5626],
        [0.1563, 0.5172, 0.7225, 0.7963],
        [0.5211, 0.7376, 0.3036, 0.4057],
        [0.3201, 0.4990, 0.5858, 0.6680],
        [0.3714, 0.4161, 0.5897, 0.6564],
        [0.2944, 0.6340, 0.5418, 0.6554],
        [0.3263, 0.6971, 0.4587, 0.5700],
        [0.5578, 0.5428, 0.3756, 0.4411]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[2.0000e+00, 1.8750e+00, 2.3694e-38, 2.3694e-38],
        [2.3694e-38, 2.3694e-38, 2.3694e-38, 2.3694e-38],
        [2.3694e-38, 2.3694e-38, 2.3694e-38, 2.3694e-38],
        [2.3694e-38, 2.3694e-38, 2.3694e-38, 2.3694e-38],
        [2.3694e-38, 2.3694e-38, 2.3694e-38, 2.3694e-38],
        [1.5414e-44, 0.0000e+00, 1.6816e-44, 0.0000e+00],
        [1.8217e-44, 0.0000e+00, 1.9618e-44, 0.0000e+00],
        [2.1019e-44, 0.0000e+00, 2.2421e-44, 0.0000e+00],
        [2.3822e-44, 0.0000e+00, 2.5223e-44, 0.0000e+00],
        [2.6625e-44, 0.0000e+00, 2.8026e-44, 0.0000e+00],
        [2.9427e-44, 0.0000e+00, 3.0829e-44, 0.0000e+00],
        [3.2230e-44, 0.0000e+00, 3.3631e-44, 0.0000e+00],
        [3.5032e-44, 0.0000e+00, 3.6434e-44, 0.0000e+00],
        [3.7835e-44, 0.0000e+00, 3.9236e-44, 0.0000e+00],
        [4.0638e-44, 0.0000e+00, 4.2039e-44, 0.0000e+00],
        [4.3440e-44, 0.0000e+00, 4.4842e-44, 0.0000e+00]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 0
current batch shape is torch.Size([16, 4])
batch is tensor([[2.0000e+00, 1.8750e+00, 2.3694e-38, 2.3694e-38],
        [2.3694e-38, 2.3694e-38, 2.3694e-38, 2.3694e-38],
        [2.3694e-38, 2.3694e-38, 2.3694e-38, 2.3694e-38],
        [2.3694e-38, 2.3694e-38, 2.3694e-38, 2.3694e-38],
        [2.3694e-38, 2.3694e-38, 2.3694e-38, 2.3694e-38],
        [1.5414e-44, 0.0000e+00, 1.6816e-44, 0.0000e+00],
        [1.8217e-44, 0.0000e+00, 1.9618e-44, 0.0000e+00],
        [2.1019e-44, 0.0000e+00, 2.2421e-44, 0.0000e+00],
        [2.3822e-44, 0.0000e+00, 2.5223e-44, 0.0000e+00],
        [2.6625e-44, 0.0000e+00, 2.8026e-44, 0.0000e+00],
        [2.9427e-44, 0.0000e+00, 3.0829e-44, 0.0000e+00],
        [3.2230e-44, 0.0000e+00, 3.3631e-44, 0.0000e+00],
        [3.5032e-44, 0.0000e+00, 3.6434e-44, 0.0000e+00],
        [3.7835e-44, 0.0000e+00, 3.9236e-44, 0.0000e+00],
        [4.0638e-44, 0.0000e+00, 4.2039e-44, 0.0000e+00],
        [4.3440e-44, 0.0000e+00, 4.4842e-44, 0.0000e+00]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 0
********************
receiving microbatch 1 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 1, tensor([[0.4936, 0.6456, 0.3734, 0.4674],
        [0.3140, 0.5660, 0.5436, 0.6298],
        [0.7478, 0.3775, 0.3158, 0.3377],
        [0.3612, 0.7559, 0.3933, 0.5134],
        [0.3962, 0.4675, 0.5379, 0.6088],
        [0.3738, 0.4434, 0.5664, 0.6329],
        [0.3338, 0.4455, 0.5918, 0.6563],
        [0.2774, 0.6370, 0.5570, 0.6724],
        [0.3081, 0.3465, 0.6510, 0.6883],
        [0.3472, 0.3564, 0.6293, 0.6775],
        [0.3683, 0.6039, 0.4916, 0.5924],
        [0.3933, 0.4401, 0.5522, 0.6163],
        [0.4378, 0.5145, 0.4887, 0.5714],
        [0.3922, 0.4390, 0.5637, 0.6354],
        [0.5560, 0.5955, 0.3687, 0.4596],
        [0.3872, 0.5076, 0.5219, 0.5992]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 1
result is tensor([[0.4936, 0.6456, 0.3734, 0.4674],
        [0.3140, 0.5660, 0.5436, 0.6298],
        [0.7478, 0.3775, 0.3158, 0.3377],
        [0.3612, 0.7559, 0.3933, 0.5134],
        [0.3962, 0.4675, 0.5379, 0.6088],
        [0.3738, 0.4434, 0.5664, 0.6329],
        [0.3338, 0.4455, 0.5918, 0.6563],
        [0.2774, 0.6370, 0.5570, 0.6724],
        [0.3081, 0.3465, 0.6510, 0.6883],
        [0.3472, 0.3564, 0.6293, 0.6775],
        [0.3683, 0.6039, 0.4916, 0.5924],
        [0.3933, 0.4401, 0.5522, 0.6163],
        [0.4378, 0.5145, 0.4887, 0.5714],
        [0.3922, 0.4390, 0.5637, 0.6354],
        [0.5560, 0.5955, 0.3687, 0.4596],
        [0.3872, 0.5076, 0.5219, 0.5992]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4936, 0.6456, 0.3734, 0.4674],
        [0.3140, 0.5660, 0.5436, 0.6298],
        [0.7478, 0.3775, 0.3158, 0.3377],
        [0.3612, 0.7559, 0.3933, 0.5134],
        [0.3962, 0.4675, 0.5379, 0.6088],
        [0.3738, 0.4434, 0.5664, 0.6329],
        [0.3338, 0.4455, 0.5918, 0.6563],
        [0.2774, 0.6370, 0.5570, 0.6724],
        [0.3081, 0.3465, 0.6510, 0.6883],
        [0.3472, 0.3564, 0.6293, 0.6775],
        [0.3683, 0.6039, 0.4916, 0.5924],
        [0.3933, 0.4401, 0.5522, 0.6163],
        [0.4378, 0.5145, 0.4887, 0.5714],
        [0.3922, 0.4390, 0.5637, 0.6354],
        [0.5560, 0.5955, 0.3687, 0.4596],
        [0.3872, 0.5076, 0.5219, 0.5992]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 0 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 0, tensor([[0.9796, 0.9964, 0.9924, 0.9924, 0.9923],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.9796, 0.9964, 0.9924, 0.9924, 0.9923],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9796, 0.9964, 0.9924, 0.9924, 0.9923],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9796, 0.9964, 0.9924, 0.9924, 0.9923],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4936, 0.6456, 0.3734, 0.4674],
        [0.3140, 0.5660, 0.5436, 0.6298],
        [0.7478, 0.3775, 0.3158, 0.3377],
        [0.3612, 0.7559, 0.3933, 0.5134],
        [0.3962, 0.4675, 0.5379, 0.6088],
        [0.3738, 0.4434, 0.5664, 0.6329],
        [0.3338, 0.4455, 0.5918, 0.6563],
        [0.2774, 0.6370, 0.5570, 0.6724],
        [0.3081, 0.3465, 0.6510, 0.6883],
        [0.3472, 0.3564, 0.6293, 0.6775],
        [0.3683, 0.6039, 0.4916, 0.5924],
        [0.3933, 0.4401, 0.5522, 0.6163],
        [0.4378, 0.5145, 0.4887, 0.5714],
        [0.3922, 0.4390, 0.5637, 0.6354],
        [0.5560, 0.5955, 0.3687, 0.4596],
        [0.3872, 0.5076, 0.5219, 0.5992]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)]
====================
schedule for this step is
[(1, 1)]
inputting microbatch 1 into partition 1
before moving to cuda:1: tensor([[0.4936, 0.6456, 0.3734, 0.4674],
        [0.3140, 0.5660, 0.5436, 0.6298],
        [0.7478, 0.3775, 0.3158, 0.3377],
        [0.3612, 0.7559, 0.3933, 0.5134],
        [0.3962, 0.4675, 0.5379, 0.6088],
        [0.3738, 0.4434, 0.5664, 0.6329],
        [0.3338, 0.4455, 0.5918, 0.6563],
        [0.2774, 0.6370, 0.5570, 0.6724],
        [0.3081, 0.3465, 0.6510, 0.6883],
        [0.3472, 0.3564, 0.6293, 0.6775],
        [0.3683, 0.6039, 0.4916, 0.5924],
        [0.3933, 0.4401, 0.5522, 0.6163],
        [0.4378, 0.5145, 0.4887, 0.5714],
        [0.3922, 0.4390, 0.5637, 0.6354],
        [0.5560, 0.5955, 0.3687, 0.4596],
        [0.3872, 0.5076, 0.5219, 0.5992]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9796, 0.9964, 0.9924, 0.9924],
        [0.9923, 0.5000, 0.8520, 0.7310],
        [0.7311, 0.7310, 0.5000, 0.8520],
        [0.7310, 0.7311, 0.7310, 0.5000],
        [0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311],
        [0.7310, 0.5000, 0.8520, 0.7310],
        [0.7311, 0.7310, 0.5000, 0.8520],
        [0.7310, 0.7311, 0.7310, 0.5000],
        [0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311],
        [0.7310, 0.5000, 0.8520, 0.7310],
        [0.7311, 0.7310, 0.5000, 0.8520],
        [0.7310, 0.7311, 0.7310, 0.5000],
        [0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 1
current batch shape is torch.Size([16, 4])
batch is tensor([[0.9796, 0.9964, 0.9924, 0.9924],
        [0.9923, 0.5000, 0.8520, 0.7310],
        [0.7311, 0.7310, 0.5000, 0.8520],
        [0.7310, 0.7311, 0.7310, 0.5000],
        [0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311],
        [0.7310, 0.5000, 0.8520, 0.7310],
        [0.7311, 0.7310, 0.5000, 0.8520],
        [0.7310, 0.7311, 0.7310, 0.5000],
        [0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311],
        [0.7310, 0.5000, 0.8520, 0.7310],
        [0.7311, 0.7310, 0.5000, 0.8520],
        [0.7310, 0.7311, 0.7310, 0.5000],
        [0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 1
********************
receiving microbatch 1 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 1, tensor([[0.9813, 0.9967, 0.9930, 0.9930, 0.9929],
        [0.9558, 0.9920, 0.9831, 0.9832, 0.9830],
        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],
        [0.9365, 0.9884, 0.9755, 0.9756, 0.9754],
        [0.9545, 0.9918, 0.9826, 0.9827, 0.9825],
        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],
        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],
        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],
        [0.9365, 0.9884, 0.9755, 0.9756, 0.9754],
        [0.9545, 0.9918, 0.9826, 0.9827, 0.9825],
        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],
        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],
        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],
        [0.9365, 0.9884, 0.9755, 0.9756, 0.9754],
        [0.9545, 0.9918, 0.9826, 0.9827, 0.9825],
        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 1
result is tensor([[0.9813, 0.9967, 0.9930, 0.9930, 0.9929],
        [0.9558, 0.9920, 0.9831, 0.9832, 0.9830],
        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],
        [0.9365, 0.9884, 0.9755, 0.9756, 0.9754],
        [0.9545, 0.9918, 0.9826, 0.9827, 0.9825],
        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],
        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],
        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],
        [0.9365, 0.9884, 0.9755, 0.9756, 0.9754],
        [0.9545, 0.9918, 0.9826, 0.9827, 0.9825],
        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],
        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],
        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],
        [0.9365, 0.9884, 0.9755, 0.9756, 0.9754],
        [0.9545, 0.9918, 0.9826, 0.9827, 0.9825],
        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9813, 0.9967, 0.9930, 0.9930, 0.9929],
        [0.9558, 0.9920, 0.9831, 0.9832, 0.9830],
        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],
        [0.9365, 0.9884, 0.9755, 0.9756, 0.9754],
        [0.9545, 0.9918, 0.9826, 0.9827, 0.9825],
        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],
        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],
        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],
        [0.9365, 0.9884, 0.9755, 0.9756, 0.9754],
        [0.9545, 0.9918, 0.9826, 0.9827, 0.9825],
        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],
        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],
        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],
        [0.9365, 0.9884, 0.9755, 0.9756, 0.9754],
        [0.9545, 0.9918, 0.9826, 0.9827, 0.9825],
        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9796, 0.9964, 0.9924, 0.9924, 0.9923],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310],
        [0.5000, 0.8520, 0.7310, 0.7311, 0.7310]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9813, 0.9967, 0.9930, 0.9930, 0.9929],
        [0.9558, 0.9920, 0.9831, 0.9832, 0.9830],
        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],
        [0.9365, 0.9884, 0.9755, 0.9756, 0.9754],
        [0.9545, 0.9918, 0.9826, 0.9827, 0.9825],
        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],
        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],
        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],
        [0.9365, 0.9884, 0.9755, 0.9756, 0.9754],
        [0.9545, 0.9918, 0.9826, 0.9827, 0.9825],
        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],
        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],
        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781],
        [0.9365, 0.9884, 0.9755, 0.9756, 0.9754],
        [0.9545, 0.9918, 0.9826, 0.9827, 0.9825],
        [0.9433, 0.9897, 0.9782, 0.9783, 0.9781]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)]
finished running cycle
____________________________ test_forward_0[16-64] _____________________________

batch_size = 64, split_size = 16

    @pytest.mark.a4_2_2
    @pytest.mark.parametrize("batch_size", [1, 16, 32, 64])
    @pytest.mark.parametrize("split_size", [1, 2, 4, 8, 16])
    # @pytest.mark.parametrize("batch_size", [1])
    # @pytest.mark.parametrize("split_size", [1, 2])
    def test_forward_0(batch_size, split_size):
        model = nn.Sequential(
            nn.Linear(3, 4).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
            nn.Linear(4, 5).to('cuda:0'),
            WithDevice(nn.Sigmoid(), 'cuda:0'),
        )
    
        x = torch.randn(batch_size, 3).to('cuda:0')
        y0 = model(x).to('cpu')
    
        # move the last two layer to another device
        model[-2] = model[-2].to('cuda:1')
        model[-1] = WithDevice(nn.Sigmoid(), 'cuda:1')
        pipe = Pipe(model, split_size=split_size)
        y1 = pipe(x).to('cpu')
>       assert torch.allclose(y0, y1)
E       assert False
E        +  where False = <built-in method allclose of type object at 0x153da78cb760>(tensor([[0.4374, 0.4511, 0.4442, 0.6242, 0.3477],\n        [0.4098, 0.4593, 0.4182, 0.6181, 0.3226],\n        [0.4342, 0.4421, 0.4376, 0.6206, 0.3221],\n        [0.4200, 0.4482, 0.4310, 0.6221, 0.3308],\n        [0.3986, 0.4389, 0.4352, 0.6334, 0.3631],\n        [0.4337, 0.4194, 0.4498, 0.6286, 0.3307],\n        [0.4332, 0.4179, 0.4631, 0.6380, 0.3692],\n        [0.4161, 0.4373, 0.4352, 0.6261, 0.3351],\n        [0.4176, 0.4642, 0.4233, 0.6185, 0.3319],\n        [0.4473, 0.4294, 0.4539, 0.6260, 0.3347],\n        [0.4381, 0.4332, 0.4491, 0.6266, 0.3387],\n        [0.4577, 0.4205, 0.4686, 0.6316, 0.3516],\n        [0.4374, 0.4149, 0.4645, 0.6369, 0.3626],\n        [0.4432, 0.4129, 0.4600, 0.6314, 0.3382],\n        [0.4508, 0.4357, 0.4597, 0.6289, 0.3547],\n        [0.4215, 0.4417, 0.4396, 0.6271, 0.3455],\n        [0.4560, 0.4137, 0.4664, 0.6305, 0.3388],\n        [0.4067, 0.4372, 0.4235, 0.6227, 0.3168],\n        [0.4100, 0.4358, 0.4477, 0.6383, 0.3829],\n        [0.4249, 0.4376, 0.4473, 0.6310, 0.3585],\n        [0.4481, 0.4160, 0.4681, 0.6350, 0.3586],\n        [0.3737, 0.4430, 0.4156, 0.6299, 0.3459],\n        [0.3866, 0.4484, 0.4219, 0.6294, 0.3530],\n        [0.4547, 0.4295, 0.4580, 0.6257, 0.3359]...653, 0.6363, 0.3659],\n        [0.4680, 0.4227, 0.4707, 0.6288, 0.3452],\n        [0.4348, 0.4194, 0.4595, 0.6346, 0.3569],\n        [0.4169, 0.4503, 0.4330, 0.6249, 0.3439],\n        [0.4209, 0.4504, 0.4371, 0.6261, 0.3502],\n        [0.4341, 0.4513, 0.4339, 0.6182, 0.3223],\n        [0.4475, 0.4231, 0.4543, 0.6261, 0.3283],\n        [0.4269, 0.4665, 0.4263, 0.6167, 0.3296],\n        [0.4678, 0.4277, 0.4628, 0.6234, 0.3283],\n        [0.4481, 0.4457, 0.4488, 0.6226, 0.3385],\n        [0.3984, 0.4337, 0.4314, 0.6303, 0.3446],\n        [0.4311, 0.4476, 0.4407, 0.6242, 0.3423],\n        [0.4436, 0.4511, 0.4347, 0.6143, 0.3098],\n        [0.4090, 0.4536, 0.4235, 0.6218, 0.3321],\n        [0.4027, 0.4530, 0.4193, 0.6215, 0.3282],\n        [0.4020, 0.4189, 0.4481, 0.6402, 0.3715],\n        [0.4340, 0.4322, 0.4489, 0.6281, 0.3430],\n        [0.4570, 0.4103, 0.4691, 0.6318, 0.3410],\n        [0.4177, 0.4355, 0.4408, 0.6292, 0.3467],\n        [0.4353, 0.4501, 0.4376, 0.6203, 0.3300],\n        [0.4524, 0.4123, 0.4724, 0.6361, 0.3603],\n        [0.4289, 0.4562, 0.4270, 0.6157, 0.3157],\n        [0.4486, 0.4063, 0.4736, 0.6382, 0.3618],\n        [0.4441, 0.4408, 0.4385, 0.6169, 0.3089]], grad_fn=<ToCopyBackward0>), tensor([[0.9795, 0.9747, 0.9446, 0.9441, 0.9438],\n        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],\n        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],\n        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],\n        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],\n        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],\n        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],\n        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],\n        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],\n        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],\n        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],\n        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],\n        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],\n        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],\n        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],\n        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],\n        [0.9966, 0.9957, 0.9900, 0.9897, 0.9900],\n        [0.9954, 0.9946, 0.9869, 0.9867, 0.9871],\n        [0.9944, 0.9933, 0.9839, 0.9836, 0.9842],\n        [0.9936, 0.9924, 0.9818, 0.9813, 0.9820],\n        [0.9935, 0.9923, 0.9815, 0.9811, 0.9817],\n        [0.9944, 0.9933, 0.9840, 0.9837, 0.9842],\n        [0.9944, 0.9933, 0.9839, 0.9837, 0.9842],\n        [0.9944, 0.9933, 0.9839, 0.9836, 0.9842]...922, 0.9919, 0.9923],\n        [0.9973, 0.9968, 0.9921, 0.9919, 0.9923],\n        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],\n        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],\n        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],\n        [0.9973, 0.9968, 0.9922, 0.9919, 0.9923],\n        [0.9973, 0.9968, 0.9921, 0.9919, 0.9923],\n        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],\n        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],\n        [0.9974, 0.9969, 0.9924, 0.9921, 0.9925],\n        [0.9974, 0.9968, 0.9924, 0.9921, 0.9925],\n        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],\n        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],\n        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],\n        [0.9974, 0.9969, 0.9924, 0.9921, 0.9925],\n        [0.9974, 0.9968, 0.9924, 0.9921, 0.9925],\n        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],\n        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],\n        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],\n        [0.9974, 0.9969, 0.9924, 0.9921, 0.9925],\n        [0.9974, 0.9968, 0.9924, 0.9921, 0.9925],\n        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925]], grad_fn=<ToCopyBackward0>))
E        +    where <built-in method allclose of type object at 0x153da78cb760> = torch.allclose

batch_size = 64
model      = Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
  (2): Linear(in_features=4, out_features=5, bias=True)
  (3): WithDevice(
    (_module): Sigmoid()
  )
)
pipe       = Pipe(
  (partitions): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=3, out_features=4, bias=True)
   ...inear(in_features=4, out_features=5, bias=True)
      (1): WithDevice(
        (_module): Sigmoid()
      )
    )
  )
)
split_size = 16
x          = tensor([[ 0.6200,  0.5243,  1.1984],
        [ 1.7429, -0.6508, -0.0401],
        [ 0.3012, -1.0815, -0.0151],
       ... 1.0929, -1.1854,  0.3694],
        [-1.5989,  0.3886, -0.4314],
        [ 0.0420, -2.0618, -0.2124]], device='cuda:0')
y0         = tensor([[0.4374, 0.4511, 0.4442, 0.6242, 0.3477],
        [0.4098, 0.4593, 0.4182, 0.6181, 0.3226],
        [0.4342, 0...[0.4486, 0.4063, 0.4736, 0.6382, 0.3618],
        [0.4441, 0.4408, 0.4385, 0.6169, 0.3089]], grad_fn=<ToCopyBackward0>)
y1         = tensor([[0.9795, 0.9747, 0.9446, 0.9441, 0.9438],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0...[0.9974, 0.9968, 0.9924, 0.9921, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925]], grad_fn=<ToCopyBackward0>)

tests/test_pipeline.py:83: AssertionError
----------------------------- Captured stdout call -----------------------------
in init
ModuleList(
  (0): Sequential(
    (0): Linear(in_features=3, out_features=4, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
  (1): Sequential(
    (0): Linear(in_features=4, out_features=5, bias=True)
    (1): WithDevice(
      (_module): Sigmoid()
    )
  )
)
[device(type='cuda', index=0), device(type='cuda', index=1)]
before running the cycle
[tensor([[ 0.6200,  0.5243,  1.1984],
        [ 1.7429, -0.6508, -0.0401],
        [ 0.3012, -1.0815, -0.0151],
        [ 0.8833, -0.4341, -0.0184],
        [ 0.8498,  1.1033, -0.9035],
        [-0.7093, -1.0516, -1.0670],
        [-0.8146,  1.0136, -0.3744],
        [ 0.4434, -0.4235, -0.6773],
        [ 1.7802, -0.0298,  0.7734],
        [-0.5169, -0.6030,  0.1809],
        [-0.1812, -0.3032,  0.1086],
        [-1.1151,  0.1312,  0.5097],
        [-1.0034,  0.5991, -0.4323],
        [-1.1680, -0.7483, -0.7959],
        [-0.3298,  0.5917,  1.0809],
        [ 0.5208,  0.2292,  0.0140]], device='cuda:0'), tensor([[-1.3735, -0.6908, -0.2094],
        [ 0.6123, -1.5707, -1.7691],
        [ 0.3613,  2.1948, -0.3684],
        [ 0.2366,  0.8291,  0.1708],
        [-1.1440,  0.4130,  0.0164],
        [ 1.8599,  0.2967, -2.0783],
        [ 1.7364,  0.7783, -1.0498],
        [-0.6463, -0.5311,  0.5174],
        [ 0.8521,  1.2264, -0.7268],
        [-0.1035, -0.4576,  1.6602],
        [-0.4437,  1.6736,  0.2673],
        [ 0.7854, -1.0078,  0.9484],
        [-1.3139, -1.4127,  1.4715],
        [-0.0063,  2.1027,  0.4195],
        [ 0.2040, -0.2725,  0.3088],
        [-2.3683, -0.2676, -1.1569]], device='cuda:0'), tensor([[-1.7707,  0.0346, -0.0970],
        [-2.1457, -0.2010,  0.6117],
        [-1.7926,  1.2749,  1.3779],
        [-0.4217, -1.3502, -0.0543],
        [-1.0585,  0.8078,  0.3151],
        [ 0.4293, -1.3759, -0.1188],
        [-0.1100,  0.2823,  0.7980],
        [-1.1911, -1.2282,  1.9931],
        [-0.8752,  0.8638,  0.0146],
        [-1.2057, -0.1674,  0.8924],
        [-0.7466,  0.3898, -0.3952],
        [ 1.0395,  0.3195,  0.2460],
        [ 0.9366,  0.6479,  0.5353],
        [ 0.7350, -0.8738,  0.5073],
        [-0.7957, -1.1098, -0.3468],
        [ 1.6595, -0.1217,  1.2198]], device='cuda:0'), tensor([[-0.9603, -1.0131,  0.7534],
        [ 0.1782, -0.0693,  1.1642],
        [ 0.6518,  0.0169, -1.4375],
        [ 0.5991,  0.1760,  0.6608],
        [ 0.5417, -1.7350,  0.4834],
        [ 1.4317, -0.2399, -0.1557],
        [ 1.5763, -0.4629, -0.5511],
        [-0.2084,  1.1663, -1.8349],
        [-0.1498, -0.0937, -0.0156],
        [-1.5478, -0.6390, -0.2962],
        [ 0.3162,  0.1714, -0.4450],
        [ 0.6451, -0.4480,  0.6902],
        [-1.3963,  0.4275,  0.0231],
        [ 1.0929, -1.1854,  0.3694],
        [-1.5989,  0.3886, -0.4314],
        [ 0.0420, -2.0618, -0.2124]], device='cuda:0')]
====================
schedule for this step is
[(0, 0)]
inputting microbatch 0 into partition 0
before moving to cuda:0: tensor([[ 0.6200,  0.5243,  1.1984],
        [ 1.7429, -0.6508, -0.0401],
        [ 0.3012, -1.0815, -0.0151],
        [ 0.8833, -0.4341, -0.0184],
        [ 0.8498,  1.1033, -0.9035],
        [-0.7093, -1.0516, -1.0670],
        [-0.8146,  1.0136, -0.3744],
        [ 0.4434, -0.4235, -0.6773],
        [ 1.7802, -0.0298,  0.7734],
        [-0.5169, -0.6030,  0.1809],
        [-0.1812, -0.3032,  0.1086],
        [-1.1151,  0.1312,  0.5097],
        [-1.0034,  0.5991, -0.4323],
        [-1.1680, -0.7483, -0.7959],
        [-0.3298,  0.5917,  1.0809],
        [ 0.5208,  0.2292,  0.0140]], device='cuda:0')
after: tensor([[ 0.6200,  0.5243,  1.1984],
        [ 1.7429, -0.6508, -0.0401],
        [ 0.3012, -1.0815, -0.0151],
        [ 0.8833, -0.4341, -0.0184],
        [ 0.8498,  1.1033, -0.9035],
        [-0.7093, -1.0516, -1.0670],
        [-0.8146,  1.0136, -0.3744],
        [ 0.4434, -0.4235, -0.6773],
        [ 1.7802, -0.0298,  0.7734],
        [-0.5169, -0.6030,  0.1809],
        [-0.1812, -0.3032,  0.1086],
        [-1.1151,  0.1312,  0.5097],
        [-1.0034,  0.5991, -0.4323],
        [-1.1680, -0.7483, -0.7959],
        [-0.3298,  0.5917,  1.0809],
        [ 0.5208,  0.2292,  0.0140]], device='cuda:0')
********************
observing microbatch 0
current batch shape is torch.Size([16, 3])
batch is tensor([[ 0.6200,  0.5243,  1.1984],
        [ 1.7429, -0.6508, -0.0401],
        [ 0.3012, -1.0815, -0.0151],
        [ 0.8833, -0.4341, -0.0184],
        [ 0.8498,  1.1033, -0.9035],
        [-0.7093, -1.0516, -1.0670],
        [-0.8146,  1.0136, -0.3744],
        [ 0.4434, -0.4235, -0.6773],
        [ 1.7802, -0.0298,  0.7734],
        [-0.5169, -0.6030,  0.1809],
        [-0.1812, -0.3032,  0.1086],
        [-1.1151,  0.1312,  0.5097],
        [-1.0034,  0.5991, -0.4323],
        [-1.1680, -0.7483, -0.7959],
        [-0.3298,  0.5917,  1.0809],
        [ 0.5208,  0.2292,  0.0140]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 0
********************
receiving microbatch 0 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 0, tensor([[0.4830, 0.3589, 0.6098, 0.4858],
        [0.6065, 0.2259, 0.6383, 0.6704],
        [0.6350, 0.3819, 0.5853, 0.7040],
        [0.5603, 0.3119, 0.6330, 0.6346],
        [0.3109, 0.2967, 0.7555, 0.4499],
        [0.5591, 0.4899, 0.6304, 0.7063],
        [0.3139, 0.5084, 0.6987, 0.4323],
        [0.5164, 0.3512, 0.6632, 0.6393],
        [0.5632, 0.2315, 0.6259, 0.5803],
        [0.5638, 0.4866, 0.5850, 0.6362],
        [0.5259, 0.4412, 0.6128, 0.6039],
        [0.4657, 0.5664, 0.5955, 0.5292],
        [0.3592, 0.5332, 0.6775, 0.4855],
        [0.5210, 0.5526, 0.6227, 0.6626],
        [0.4474, 0.4747, 0.6024, 0.4675],
        [0.4621, 0.3515, 0.6604, 0.5449]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.4830, 0.3589, 0.6098, 0.4858],
        [0.6065, 0.2259, 0.6383, 0.6704],
        [0.6350, 0.3819, 0.5853, 0.7040],
        [0.5603, 0.3119, 0.6330, 0.6346],
        [0.3109, 0.2967, 0.7555, 0.4499],
        [0.5591, 0.4899, 0.6304, 0.7063],
        [0.3139, 0.5084, 0.6987, 0.4323],
        [0.5164, 0.3512, 0.6632, 0.6393],
        [0.5632, 0.2315, 0.6259, 0.5803],
        [0.5638, 0.4866, 0.5850, 0.6362],
        [0.5259, 0.4412, 0.6128, 0.6039],
        [0.4657, 0.5664, 0.5955, 0.5292],
        [0.3592, 0.5332, 0.6775, 0.4855],
        [0.5210, 0.5526, 0.6227, 0.6626],
        [0.4474, 0.4747, 0.6024, 0.4675],
        [0.4621, 0.3515, 0.6604, 0.5449]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4830, 0.3589, 0.6098, 0.4858],
        [0.6065, 0.2259, 0.6383, 0.6704],
        [0.6350, 0.3819, 0.5853, 0.7040],
        [0.5603, 0.3119, 0.6330, 0.6346],
        [0.3109, 0.2967, 0.7555, 0.4499],
        [0.5591, 0.4899, 0.6304, 0.7063],
        [0.3139, 0.5084, 0.6987, 0.4323],
        [0.5164, 0.3512, 0.6632, 0.6393],
        [0.5632, 0.2315, 0.6259, 0.5803],
        [0.5638, 0.4866, 0.5850, 0.6362],
        [0.5259, 0.4412, 0.6128, 0.6039],
        [0.4657, 0.5664, 0.5955, 0.5292],
        [0.3592, 0.5332, 0.6775, 0.4855],
        [0.5210, 0.5526, 0.6227, 0.6626],
        [0.4474, 0.4747, 0.6024, 0.4675],
        [0.4621, 0.3515, 0.6604, 0.5449]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.4830, 0.3589, 0.6098, 0.4858],
        [0.6065, 0.2259, 0.6383, 0.6704],
        [0.6350, 0.3819, 0.5853, 0.7040],
        [0.5603, 0.3119, 0.6330, 0.6346],
        [0.3109, 0.2967, 0.7555, 0.4499],
        [0.5591, 0.4899, 0.6304, 0.7063],
        [0.3139, 0.5084, 0.6987, 0.4323],
        [0.5164, 0.3512, 0.6632, 0.6393],
        [0.5632, 0.2315, 0.6259, 0.5803],
        [0.5638, 0.4866, 0.5850, 0.6362],
        [0.5259, 0.4412, 0.6128, 0.6039],
        [0.4657, 0.5664, 0.5955, 0.5292],
        [0.3592, 0.5332, 0.6775, 0.4855],
        [0.5210, 0.5526, 0.6227, 0.6626],
        [0.4474, 0.4747, 0.6024, 0.4675],
        [0.4621, 0.3515, 0.6604, 0.5449]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-1.3735, -0.6908, -0.2094],
        [ 0.6123, -1.5707, -1.7691],
        [ 0.3613,  2.1948, -0.3684],
        [ 0.2366,  0.8291,  0.1708],
        [-1.1440,  0.4130,  0.0164],
        [ 1.8599,  0.2967, -2.0783],
        [ 1.7364,  0.7783, -1.0498],
        [-0.6463, -0.5311,  0.5174],
        [ 0.8521,  1.2264, -0.7268],
        [-0.1035, -0.4576,  1.6602],
        [-0.4437,  1.6736,  0.2673],
        [ 0.7854, -1.0078,  0.9484],
        [-1.3139, -1.4127,  1.4715],
        [-0.0063,  2.1027,  0.4195],
        [ 0.2040, -0.2725,  0.3088],
        [-2.3683, -0.2676, -1.1569]], device='cuda:0'), tensor([[-1.7707,  0.0346, -0.0970],
        [-2.1457, -0.2010,  0.6117],
        [-1.7926,  1.2749,  1.3779],
        [-0.4217, -1.3502, -0.0543],
        [-1.0585,  0.8078,  0.3151],
        [ 0.4293, -1.3759, -0.1188],
        [-0.1100,  0.2823,  0.7980],
        [-1.1911, -1.2282,  1.9931],
        [-0.8752,  0.8638,  0.0146],
        [-1.2057, -0.1674,  0.8924],
        [-0.7466,  0.3898, -0.3952],
        [ 1.0395,  0.3195,  0.2460],
        [ 0.9366,  0.6479,  0.5353],
        [ 0.7350, -0.8738,  0.5073],
        [-0.7957, -1.1098, -0.3468],
        [ 1.6595, -0.1217,  1.2198]], device='cuda:0'), tensor([[-0.9603, -1.0131,  0.7534],
        [ 0.1782, -0.0693,  1.1642],
        [ 0.6518,  0.0169, -1.4375],
        [ 0.5991,  0.1760,  0.6608],
        [ 0.5417, -1.7350,  0.4834],
        [ 1.4317, -0.2399, -0.1557],
        [ 1.5763, -0.4629, -0.5511],
        [-0.2084,  1.1663, -1.8349],
        [-0.1498, -0.0937, -0.0156],
        [-1.5478, -0.6390, -0.2962],
        [ 0.3162,  0.1714, -0.4450],
        [ 0.6451, -0.4480,  0.6902],
        [-1.3963,  0.4275,  0.0231],
        [ 1.0929, -1.1854,  0.3694],
        [-1.5989,  0.3886, -0.4314],
        [ 0.0420, -2.0618, -0.2124]], device='cuda:0')]
====================
schedule for this step is
[(1, 0), (0, 1)]
inputting microbatch 1 into partition 0
before moving to cuda:0: tensor([[-1.3735, -0.6908, -0.2094],
        [ 0.6123, -1.5707, -1.7691],
        [ 0.3613,  2.1948, -0.3684],
        [ 0.2366,  0.8291,  0.1708],
        [-1.1440,  0.4130,  0.0164],
        [ 1.8599,  0.2967, -2.0783],
        [ 1.7364,  0.7783, -1.0498],
        [-0.6463, -0.5311,  0.5174],
        [ 0.8521,  1.2264, -0.7268],
        [-0.1035, -0.4576,  1.6602],
        [-0.4437,  1.6736,  0.2673],
        [ 0.7854, -1.0078,  0.9484],
        [-1.3139, -1.4127,  1.4715],
        [-0.0063,  2.1027,  0.4195],
        [ 0.2040, -0.2725,  0.3088],
        [-2.3683, -0.2676, -1.1569]], device='cuda:0')
after: tensor([[-1.3735, -0.6908, -0.2094],
        [ 0.6123, -1.5707, -1.7691],
        [ 0.3613,  2.1948, -0.3684],
        [ 0.2366,  0.8291,  0.1708],
        [-1.1440,  0.4130,  0.0164],
        [ 1.8599,  0.2967, -2.0783],
        [ 1.7364,  0.7783, -1.0498],
        [-0.6463, -0.5311,  0.5174],
        [ 0.8521,  1.2264, -0.7268],
        [-0.1035, -0.4576,  1.6602],
        [-0.4437,  1.6736,  0.2673],
        [ 0.7854, -1.0078,  0.9484],
        [-1.3139, -1.4127,  1.4715],
        [-0.0063,  2.1027,  0.4195],
        [ 0.2040, -0.2725,  0.3088],
        [-2.3683, -0.2676, -1.1569]], device='cuda:0')
********************
observing microbatch 1
current batch shape is torch.Size([16, 3])
batch is tensor([[-1.3735, -0.6908, -0.2094],
        [ 0.6123, -1.5707, -1.7691],
        [ 0.3613,  2.1948, -0.3684],
        [ 0.2366,  0.8291,  0.1708],
        [-1.1440,  0.4130,  0.0164],
        [ 1.8599,  0.2967, -2.0783],
        [ 1.7364,  0.7783, -1.0498],
        [-0.6463, -0.5311,  0.5174],
        [ 0.8521,  1.2264, -0.7268],
        [-0.1035, -0.4576,  1.6602],
        [-0.4437,  1.6736,  0.2673],
        [ 0.7854, -1.0078,  0.9484],
        [-1.3139, -1.4127,  1.4715],
        [-0.0063,  2.1027,  0.4195],
        [ 0.2040, -0.2725,  0.3088],
        [-2.3683, -0.2676, -1.1569]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 1
********************
inputting microbatch 0 into partition 1
before moving to cuda:1: tensor([[0.4830, 0.3589, 0.6098, 0.4858],
        [0.6065, 0.2259, 0.6383, 0.6704],
        [0.6350, 0.3819, 0.5853, 0.7040],
        [0.5603, 0.3119, 0.6330, 0.6346],
        [0.3109, 0.2967, 0.7555, 0.4499],
        [0.5591, 0.4899, 0.6304, 0.7063],
        [0.3139, 0.5084, 0.6987, 0.4323],
        [0.5164, 0.3512, 0.6632, 0.6393],
        [0.5632, 0.2315, 0.6259, 0.5803],
        [0.5638, 0.4866, 0.5850, 0.6362],
        [0.5259, 0.4412, 0.6128, 0.6039],
        [0.4657, 0.5664, 0.5955, 0.5292],
        [0.3592, 0.5332, 0.6775, 0.4855],
        [0.5210, 0.5526, 0.6227, 0.6626],
        [0.4474, 0.4747, 0.6024, 0.4675],
        [0.4621, 0.3515, 0.6604, 0.5449]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[-0.0000e+00, 1.8742e+00, 2.3694e-38, 2.3694e-38],
        [2.3694e-38, 2.3694e-38, 2.3694e-38, 2.3694e-38],
        [2.3694e-38, 2.3694e-38, 2.3694e-38, 2.3694e-38],
        [2.3694e-38, 2.3694e-38, 2.3694e-38, 2.3694e-38],
        [2.3694e-38, 2.3694e-38, 2.3694e-38, 2.3694e-38],
        [1.4013e-44, 0.0000e+00, 1.5414e-44, 0.0000e+00],
        [1.6816e-44, 0.0000e+00, 1.8217e-44, 0.0000e+00],
        [1.9618e-44, 0.0000e+00, 2.1019e-44, 0.0000e+00],
        [2.2421e-44, 0.0000e+00, 2.3822e-44, 0.0000e+00],
        [2.5223e-44, 0.0000e+00, 2.6625e-44, 0.0000e+00],
        [2.8026e-44, 0.0000e+00, 2.9427e-44, 0.0000e+00],
        [3.0829e-44, 0.0000e+00, 3.2230e-44, 0.0000e+00],
        [3.3631e-44, 0.0000e+00, 3.5032e-44, 0.0000e+00],
        [3.6434e-44, 0.0000e+00, 3.7835e-44, 0.0000e+00],
        [3.9236e-44, 0.0000e+00, 4.0638e-44, 0.0000e+00],
        [4.2039e-44, 0.0000e+00, 4.3440e-44, 0.0000e+00]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 0
current batch shape is torch.Size([16, 4])
batch is tensor([[-0.0000e+00, 1.8742e+00, 2.3694e-38, 2.3694e-38],
        [2.3694e-38, 2.3694e-38, 2.3694e-38, 2.3694e-38],
        [2.3694e-38, 2.3694e-38, 2.3694e-38, 2.3694e-38],
        [2.3694e-38, 2.3694e-38, 2.3694e-38, 2.3694e-38],
        [2.3694e-38, 2.3694e-38, 2.3694e-38, 2.3694e-38],
        [1.4013e-44, 0.0000e+00, 1.5414e-44, 0.0000e+00],
        [1.6816e-44, 0.0000e+00, 1.8217e-44, 0.0000e+00],
        [1.9618e-44, 0.0000e+00, 2.1019e-44, 0.0000e+00],
        [2.2421e-44, 0.0000e+00, 2.3822e-44, 0.0000e+00],
        [2.5223e-44, 0.0000e+00, 2.6625e-44, 0.0000e+00],
        [2.8026e-44, 0.0000e+00, 2.9427e-44, 0.0000e+00],
        [3.0829e-44, 0.0000e+00, 3.2230e-44, 0.0000e+00],
        [3.3631e-44, 0.0000e+00, 3.5032e-44, 0.0000e+00],
        [3.6434e-44, 0.0000e+00, 3.7835e-44, 0.0000e+00],
        [3.9236e-44, 0.0000e+00, 4.0638e-44, 0.0000e+00],
        [4.2039e-44, 0.0000e+00, 4.3440e-44, 0.0000e+00]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 0
********************
receiving microbatch 1 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 1, tensor([[0.5382, 0.5889, 0.5870, 0.6438],
        [0.6221, 0.3177, 0.6666, 0.7804],
        [0.2079, 0.3570, 0.7709, 0.3011],
        [0.3826, 0.3861, 0.6781, 0.4593],
        [0.4021, 0.5600, 0.6398, 0.5004],
        [0.3782, 0.1889, 0.7879, 0.5892],
        [0.3617, 0.2100, 0.7611, 0.5063],
        [0.5679, 0.5092, 0.5660, 0.6202],
        [0.3040, 0.2988, 0.7527, 0.4307],
        [0.6248, 0.4604, 0.5101, 0.5975],
        [0.2713, 0.4702, 0.7040, 0.3428],
        [0.6790, 0.3407, 0.5398, 0.6860],
        [0.7109, 0.6140, 0.4414, 0.7019],
        [0.2389, 0.4159, 0.7241, 0.2957],
        [0.5400, 0.3965, 0.6099, 0.6010],
        [0.4110, 0.6887, 0.6478, 0.5957]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 1
result is tensor([[0.5382, 0.5889, 0.5870, 0.6438],
        [0.6221, 0.3177, 0.6666, 0.7804],
        [0.2079, 0.3570, 0.7709, 0.3011],
        [0.3826, 0.3861, 0.6781, 0.4593],
        [0.4021, 0.5600, 0.6398, 0.5004],
        [0.3782, 0.1889, 0.7879, 0.5892],
        [0.3617, 0.2100, 0.7611, 0.5063],
        [0.5679, 0.5092, 0.5660, 0.6202],
        [0.3040, 0.2988, 0.7527, 0.4307],
        [0.6248, 0.4604, 0.5101, 0.5975],
        [0.2713, 0.4702, 0.7040, 0.3428],
        [0.6790, 0.3407, 0.5398, 0.6860],
        [0.7109, 0.6140, 0.4414, 0.7019],
        [0.2389, 0.4159, 0.7241, 0.2957],
        [0.5400, 0.3965, 0.6099, 0.6010],
        [0.4110, 0.6887, 0.6478, 0.5957]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.5382, 0.5889, 0.5870, 0.6438],
        [0.6221, 0.3177, 0.6666, 0.7804],
        [0.2079, 0.3570, 0.7709, 0.3011],
        [0.3826, 0.3861, 0.6781, 0.4593],
        [0.4021, 0.5600, 0.6398, 0.5004],
        [0.3782, 0.1889, 0.7879, 0.5892],
        [0.3617, 0.2100, 0.7611, 0.5063],
        [0.5679, 0.5092, 0.5660, 0.6202],
        [0.3040, 0.2988, 0.7527, 0.4307],
        [0.6248, 0.4604, 0.5101, 0.5975],
        [0.2713, 0.4702, 0.7040, 0.3428],
        [0.6790, 0.3407, 0.5398, 0.6860],
        [0.7109, 0.6140, 0.4414, 0.7019],
        [0.2389, 0.4159, 0.7241, 0.2957],
        [0.5400, 0.3965, 0.6099, 0.6010],
        [0.4110, 0.6887, 0.6478, 0.5957]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 0 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 0, tensor([[0.9795, 0.9747, 0.9446, 0.9441, 0.9438],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 0
result is tensor([[0.9795, 0.9747, 0.9446, 0.9441, 0.9438],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9795, 0.9747, 0.9446, 0.9441, 0.9438],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9795, 0.9747, 0.9446, 0.9441, 0.9438],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.5382, 0.5889, 0.5870, 0.6438],
        [0.6221, 0.3177, 0.6666, 0.7804],
        [0.2079, 0.3570, 0.7709, 0.3011],
        [0.3826, 0.3861, 0.6781, 0.4593],
        [0.4021, 0.5600, 0.6398, 0.5004],
        [0.3782, 0.1889, 0.7879, 0.5892],
        [0.3617, 0.2100, 0.7611, 0.5063],
        [0.5679, 0.5092, 0.5660, 0.6202],
        [0.3040, 0.2988, 0.7527, 0.4307],
        [0.6248, 0.4604, 0.5101, 0.5975],
        [0.2713, 0.4702, 0.7040, 0.3428],
        [0.6790, 0.3407, 0.5398, 0.6860],
        [0.7109, 0.6140, 0.4414, 0.7019],
        [0.2389, 0.4159, 0.7241, 0.2957],
        [0.5400, 0.3965, 0.6099, 0.6010],
        [0.4110, 0.6887, 0.6478, 0.5957]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-1.7707,  0.0346, -0.0970],
        [-2.1457, -0.2010,  0.6117],
        [-1.7926,  1.2749,  1.3779],
        [-0.4217, -1.3502, -0.0543],
        [-1.0585,  0.8078,  0.3151],
        [ 0.4293, -1.3759, -0.1188],
        [-0.1100,  0.2823,  0.7980],
        [-1.1911, -1.2282,  1.9931],
        [-0.8752,  0.8638,  0.0146],
        [-1.2057, -0.1674,  0.8924],
        [-0.7466,  0.3898, -0.3952],
        [ 1.0395,  0.3195,  0.2460],
        [ 0.9366,  0.6479,  0.5353],
        [ 0.7350, -0.8738,  0.5073],
        [-0.7957, -1.1098, -0.3468],
        [ 1.6595, -0.1217,  1.2198]], device='cuda:0'), tensor([[-0.9603, -1.0131,  0.7534],
        [ 0.1782, -0.0693,  1.1642],
        [ 0.6518,  0.0169, -1.4375],
        [ 0.5991,  0.1760,  0.6608],
        [ 0.5417, -1.7350,  0.4834],
        [ 1.4317, -0.2399, -0.1557],
        [ 1.5763, -0.4629, -0.5511],
        [-0.2084,  1.1663, -1.8349],
        [-0.1498, -0.0937, -0.0156],
        [-1.5478, -0.6390, -0.2962],
        [ 0.3162,  0.1714, -0.4450],
        [ 0.6451, -0.4480,  0.6902],
        [-1.3963,  0.4275,  0.0231],
        [ 1.0929, -1.1854,  0.3694],
        [-1.5989,  0.3886, -0.4314],
        [ 0.0420, -2.0618, -0.2124]], device='cuda:0')]
====================
schedule for this step is
[(2, 0), (1, 1)]
inputting microbatch 2 into partition 0
before moving to cuda:0: tensor([[-1.7707,  0.0346, -0.0970],
        [-2.1457, -0.2010,  0.6117],
        [-1.7926,  1.2749,  1.3779],
        [-0.4217, -1.3502, -0.0543],
        [-1.0585,  0.8078,  0.3151],
        [ 0.4293, -1.3759, -0.1188],
        [-0.1100,  0.2823,  0.7980],
        [-1.1911, -1.2282,  1.9931],
        [-0.8752,  0.8638,  0.0146],
        [-1.2057, -0.1674,  0.8924],
        [-0.7466,  0.3898, -0.3952],
        [ 1.0395,  0.3195,  0.2460],
        [ 0.9366,  0.6479,  0.5353],
        [ 0.7350, -0.8738,  0.5073],
        [-0.7957, -1.1098, -0.3468],
        [ 1.6595, -0.1217,  1.2198]], device='cuda:0')
after: tensor([[-1.7707,  0.0346, -0.0970],
        [-2.1457, -0.2010,  0.6117],
        [-1.7926,  1.2749,  1.3779],
        [-0.4217, -1.3502, -0.0543],
        [-1.0585,  0.8078,  0.3151],
        [ 0.4293, -1.3759, -0.1188],
        [-0.1100,  0.2823,  0.7980],
        [-1.1911, -1.2282,  1.9931],
        [-0.8752,  0.8638,  0.0146],
        [-1.2057, -0.1674,  0.8924],
        [-0.7466,  0.3898, -0.3952],
        [ 1.0395,  0.3195,  0.2460],
        [ 0.9366,  0.6479,  0.5353],
        [ 0.7350, -0.8738,  0.5073],
        [-0.7957, -1.1098, -0.3468],
        [ 1.6595, -0.1217,  1.2198]], device='cuda:0')
********************
observing microbatch 2
current batch shape is torch.Size([16, 3])
batch is tensor([[-1.7707,  0.0346, -0.0970],
        [-2.1457, -0.2010,  0.6117],
        [-1.7926,  1.2749,  1.3779],
        [-0.4217, -1.3502, -0.0543],
        [-1.0585,  0.8078,  0.3151],
        [ 0.4293, -1.3759, -0.1188],
        [-0.1100,  0.2823,  0.7980],
        [-1.1911, -1.2282,  1.9931],
        [-0.8752,  0.8638,  0.0146],
        [-1.2057, -0.1674,  0.8924],
        [-0.7466,  0.3898, -0.3952],
        [ 1.0395,  0.3195,  0.2460],
        [ 0.9366,  0.6479,  0.5353],
        [ 0.7350, -0.8738,  0.5073],
        [-0.7957, -1.1098, -0.3468],
        [ 1.6595, -0.1217,  1.2198]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 2
********************
inputting microbatch 1 into partition 1
before moving to cuda:1: tensor([[0.5382, 0.5889, 0.5870, 0.6438],
        [0.6221, 0.3177, 0.6666, 0.7804],
        [0.2079, 0.3570, 0.7709, 0.3011],
        [0.3826, 0.3861, 0.6781, 0.4593],
        [0.4021, 0.5600, 0.6398, 0.5004],
        [0.3782, 0.1889, 0.7879, 0.5892],
        [0.3617, 0.2100, 0.7611, 0.5063],
        [0.5679, 0.5092, 0.5660, 0.6202],
        [0.3040, 0.2988, 0.7527, 0.4307],
        [0.6248, 0.4604, 0.5101, 0.5975],
        [0.2713, 0.4702, 0.7040, 0.3428],
        [0.6790, 0.3407, 0.5398, 0.6860],
        [0.7109, 0.6140, 0.4414, 0.7019],
        [0.2389, 0.4159, 0.7241, 0.2957],
        [0.5400, 0.3965, 0.6099, 0.6010],
        [0.4110, 0.6887, 0.6478, 0.5957]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[1.0842e-19, 1.8074e+00, 9.4456e-01, 9.4409e-01],
        [9.4381e-01, 8.8080e-01, 8.6519e-01, 7.2967e-01],
        [7.2968e-01, 7.2966e-01, 8.8080e-01, 8.6519e-01],
        [7.2967e-01, 7.2968e-01, 7.2966e-01, 8.8080e-01],
        [8.6519e-01, 7.2967e-01, 7.2968e-01, 7.2966e-01],
        [8.8080e-01, 8.6519e-01, 7.2967e-01, 7.2968e-01],
        [7.2966e-01, 8.8080e-01, 8.6519e-01, 7.2967e-01],
        [7.2968e-01, 7.2966e-01, 8.8080e-01, 8.6519e-01],
        [7.2967e-01, 7.2968e-01, 7.2966e-01, 8.8080e-01],
        [8.6519e-01, 7.2967e-01, 7.2968e-01, 7.2966e-01],
        [8.8080e-01, 8.6519e-01, 7.2967e-01, 7.2968e-01],
        [7.2966e-01, 8.8080e-01, 8.6519e-01, 7.2967e-01],
        [7.2968e-01, 7.2966e-01, 8.8080e-01, 8.6519e-01],
        [7.2967e-01, 7.2968e-01, 7.2966e-01, 8.8080e-01],
        [8.6519e-01, 7.2967e-01, 7.2968e-01, 7.2966e-01],
        [8.8080e-01, 8.6519e-01, 7.2967e-01, 7.2968e-01]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 1
current batch shape is torch.Size([16, 4])
batch is tensor([[1.0842e-19, 1.8074e+00, 9.4456e-01, 9.4409e-01],
        [9.4381e-01, 8.8080e-01, 8.6519e-01, 7.2967e-01],
        [7.2968e-01, 7.2966e-01, 8.8080e-01, 8.6519e-01],
        [7.2967e-01, 7.2968e-01, 7.2966e-01, 8.8080e-01],
        [8.6519e-01, 7.2967e-01, 7.2968e-01, 7.2966e-01],
        [8.8080e-01, 8.6519e-01, 7.2967e-01, 7.2968e-01],
        [7.2966e-01, 8.8080e-01, 8.6519e-01, 7.2967e-01],
        [7.2968e-01, 7.2966e-01, 8.8080e-01, 8.6519e-01],
        [7.2967e-01, 7.2968e-01, 7.2966e-01, 8.8080e-01],
        [8.6519e-01, 7.2967e-01, 7.2968e-01, 7.2966e-01],
        [8.8080e-01, 8.6519e-01, 7.2967e-01, 7.2968e-01],
        [7.2966e-01, 8.8080e-01, 8.6519e-01, 7.2967e-01],
        [7.2968e-01, 7.2966e-01, 8.8080e-01, 8.6519e-01],
        [7.2967e-01, 7.2968e-01, 7.2966e-01, 8.8080e-01],
        [8.6519e-01, 7.2967e-01, 7.2968e-01, 7.2966e-01],
        [8.8080e-01, 8.6519e-01, 7.2967e-01, 7.2968e-01]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 1
********************
receiving microbatch 2 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 2, tensor([[0.4347, 0.6367, 0.6138, 0.5449],
        [0.4949, 0.6921, 0.5498, 0.5586],
        [0.3414, 0.6599, 0.5953, 0.3577],
        [0.6532, 0.4729, 0.5579, 0.7265],
        [0.3661, 0.5532, 0.6457, 0.4439],
        [0.6699, 0.3657, 0.5771, 0.7382],
        [0.4808, 0.4426, 0.6061, 0.5162],
        [0.7137, 0.6076, 0.4225, 0.6741],
        [0.3486, 0.5240, 0.6689, 0.4442],
        [0.5246, 0.5858, 0.5530, 0.5605],
        [0.3935, 0.5017, 0.6691, 0.5157],
        [0.4724, 0.2967, 0.6615, 0.5352],
        [0.4393, 0.3113, 0.6608, 0.4852],
        [0.6413, 0.3387, 0.5739, 0.6770],
        [0.6007, 0.5146, 0.5825, 0.7010],
        [0.5952, 0.2491, 0.5918, 0.5829]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 2
result is tensor([[0.4347, 0.6367, 0.6138, 0.5449],
        [0.4949, 0.6921, 0.5498, 0.5586],
        [0.3414, 0.6599, 0.5953, 0.3577],
        [0.6532, 0.4729, 0.5579, 0.7265],
        [0.3661, 0.5532, 0.6457, 0.4439],
        [0.6699, 0.3657, 0.5771, 0.7382],
        [0.4808, 0.4426, 0.6061, 0.5162],
        [0.7137, 0.6076, 0.4225, 0.6741],
        [0.3486, 0.5240, 0.6689, 0.4442],
        [0.5246, 0.5858, 0.5530, 0.5605],
        [0.3935, 0.5017, 0.6691, 0.5157],
        [0.4724, 0.2967, 0.6615, 0.5352],
        [0.4393, 0.3113, 0.6608, 0.4852],
        [0.6413, 0.3387, 0.5739, 0.6770],
        [0.6007, 0.5146, 0.5825, 0.7010],
        [0.5952, 0.2491, 0.5918, 0.5829]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.4347, 0.6367, 0.6138, 0.5449],
        [0.4949, 0.6921, 0.5498, 0.5586],
        [0.3414, 0.6599, 0.5953, 0.3577],
        [0.6532, 0.4729, 0.5579, 0.7265],
        [0.3661, 0.5532, 0.6457, 0.4439],
        [0.6699, 0.3657, 0.5771, 0.7382],
        [0.4808, 0.4426, 0.6061, 0.5162],
        [0.7137, 0.6076, 0.4225, 0.6741],
        [0.3486, 0.5240, 0.6689, 0.4442],
        [0.5246, 0.5858, 0.5530, 0.5605],
        [0.3935, 0.5017, 0.6691, 0.5157],
        [0.4724, 0.2967, 0.6615, 0.5352],
        [0.4393, 0.3113, 0.6608, 0.4852],
        [0.6413, 0.3387, 0.5739, 0.6770],
        [0.6007, 0.5146, 0.5825, 0.7010],
        [0.5952, 0.2491, 0.5918, 0.5829]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 1 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 1, tensor([[0.9966, 0.9957, 0.9900, 0.9897, 0.9900],
        [0.9954, 0.9946, 0.9869, 0.9867, 0.9871],
        [0.9944, 0.9933, 0.9839, 0.9836, 0.9842],
        [0.9936, 0.9924, 0.9818, 0.9813, 0.9820],
        [0.9935, 0.9923, 0.9815, 0.9811, 0.9817],
        [0.9944, 0.9933, 0.9840, 0.9837, 0.9842],
        [0.9944, 0.9933, 0.9839, 0.9837, 0.9842],
        [0.9944, 0.9933, 0.9839, 0.9836, 0.9842],
        [0.9936, 0.9924, 0.9818, 0.9813, 0.9820],
        [0.9935, 0.9923, 0.9815, 0.9811, 0.9817],
        [0.9944, 0.9933, 0.9840, 0.9837, 0.9842],
        [0.9944, 0.9933, 0.9839, 0.9837, 0.9842],
        [0.9944, 0.9933, 0.9839, 0.9836, 0.9842],
        [0.9936, 0.9924, 0.9818, 0.9813, 0.9820],
        [0.9935, 0.9923, 0.9815, 0.9811, 0.9817],
        [0.9944, 0.9933, 0.9840, 0.9837, 0.9842]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 1
result is tensor([[0.9966, 0.9957, 0.9900, 0.9897, 0.9900],
        [0.9954, 0.9946, 0.9869, 0.9867, 0.9871],
        [0.9944, 0.9933, 0.9839, 0.9836, 0.9842],
        [0.9936, 0.9924, 0.9818, 0.9813, 0.9820],
        [0.9935, 0.9923, 0.9815, 0.9811, 0.9817],
        [0.9944, 0.9933, 0.9840, 0.9837, 0.9842],
        [0.9944, 0.9933, 0.9839, 0.9837, 0.9842],
        [0.9944, 0.9933, 0.9839, 0.9836, 0.9842],
        [0.9936, 0.9924, 0.9818, 0.9813, 0.9820],
        [0.9935, 0.9923, 0.9815, 0.9811, 0.9817],
        [0.9944, 0.9933, 0.9840, 0.9837, 0.9842],
        [0.9944, 0.9933, 0.9839, 0.9837, 0.9842],
        [0.9944, 0.9933, 0.9839, 0.9836, 0.9842],
        [0.9936, 0.9924, 0.9818, 0.9813, 0.9820],
        [0.9935, 0.9923, 0.9815, 0.9811, 0.9817],
        [0.9944, 0.9933, 0.9840, 0.9837, 0.9842]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9966, 0.9957, 0.9900, 0.9897, 0.9900],
        [0.9954, 0.9946, 0.9869, 0.9867, 0.9871],
        [0.9944, 0.9933, 0.9839, 0.9836, 0.9842],
        [0.9936, 0.9924, 0.9818, 0.9813, 0.9820],
        [0.9935, 0.9923, 0.9815, 0.9811, 0.9817],
        [0.9944, 0.9933, 0.9840, 0.9837, 0.9842],
        [0.9944, 0.9933, 0.9839, 0.9837, 0.9842],
        [0.9944, 0.9933, 0.9839, 0.9836, 0.9842],
        [0.9936, 0.9924, 0.9818, 0.9813, 0.9820],
        [0.9935, 0.9923, 0.9815, 0.9811, 0.9817],
        [0.9944, 0.9933, 0.9840, 0.9837, 0.9842],
        [0.9944, 0.9933, 0.9839, 0.9837, 0.9842],
        [0.9944, 0.9933, 0.9839, 0.9836, 0.9842],
        [0.9936, 0.9924, 0.9818, 0.9813, 0.9820],
        [0.9935, 0.9923, 0.9815, 0.9811, 0.9817],
        [0.9944, 0.9933, 0.9840, 0.9837, 0.9842]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9795, 0.9747, 0.9446, 0.9441, 0.9438],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9966, 0.9957, 0.9900, 0.9897, 0.9900],
        [0.9954, 0.9946, 0.9869, 0.9867, 0.9871],
        [0.9944, 0.9933, 0.9839, 0.9836, 0.9842],
        [0.9936, 0.9924, 0.9818, 0.9813, 0.9820],
        [0.9935, 0.9923, 0.9815, 0.9811, 0.9817],
        [0.9944, 0.9933, 0.9840, 0.9837, 0.9842],
        [0.9944, 0.9933, 0.9839, 0.9837, 0.9842],
        [0.9944, 0.9933, 0.9839, 0.9836, 0.9842],
        [0.9936, 0.9924, 0.9818, 0.9813, 0.9820],
        [0.9935, 0.9923, 0.9815, 0.9811, 0.9817],
        [0.9944, 0.9933, 0.9840, 0.9837, 0.9842],
        [0.9944, 0.9933, 0.9839, 0.9837, 0.9842],
        [0.9944, 0.9933, 0.9839, 0.9836, 0.9842],
        [0.9936, 0.9924, 0.9818, 0.9813, 0.9820],
        [0.9935, 0.9923, 0.9815, 0.9811, 0.9817],
        [0.9944, 0.9933, 0.9840, 0.9837, 0.9842]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.4347, 0.6367, 0.6138, 0.5449],
        [0.4949, 0.6921, 0.5498, 0.5586],
        [0.3414, 0.6599, 0.5953, 0.3577],
        [0.6532, 0.4729, 0.5579, 0.7265],
        [0.3661, 0.5532, 0.6457, 0.4439],
        [0.6699, 0.3657, 0.5771, 0.7382],
        [0.4808, 0.4426, 0.6061, 0.5162],
        [0.7137, 0.6076, 0.4225, 0.6741],
        [0.3486, 0.5240, 0.6689, 0.4442],
        [0.5246, 0.5858, 0.5530, 0.5605],
        [0.3935, 0.5017, 0.6691, 0.5157],
        [0.4724, 0.2967, 0.6615, 0.5352],
        [0.4393, 0.3113, 0.6608, 0.4852],
        [0.6413, 0.3387, 0.5739, 0.6770],
        [0.6007, 0.5146, 0.5825, 0.7010],
        [0.5952, 0.2491, 0.5918, 0.5829]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), tensor([[-0.9603, -1.0131,  0.7534],
        [ 0.1782, -0.0693,  1.1642],
        [ 0.6518,  0.0169, -1.4375],
        [ 0.5991,  0.1760,  0.6608],
        [ 0.5417, -1.7350,  0.4834],
        [ 1.4317, -0.2399, -0.1557],
        [ 1.5763, -0.4629, -0.5511],
        [-0.2084,  1.1663, -1.8349],
        [-0.1498, -0.0937, -0.0156],
        [-1.5478, -0.6390, -0.2962],
        [ 0.3162,  0.1714, -0.4450],
        [ 0.6451, -0.4480,  0.6902],
        [-1.3963,  0.4275,  0.0231],
        [ 1.0929, -1.1854,  0.3694],
        [-1.5989,  0.3886, -0.4314],
        [ 0.0420, -2.0618, -0.2124]], device='cuda:0')]
====================
schedule for this step is
[(3, 0), (2, 1)]
inputting microbatch 3 into partition 0
before moving to cuda:0: tensor([[-0.9603, -1.0131,  0.7534],
        [ 0.1782, -0.0693,  1.1642],
        [ 0.6518,  0.0169, -1.4375],
        [ 0.5991,  0.1760,  0.6608],
        [ 0.5417, -1.7350,  0.4834],
        [ 1.4317, -0.2399, -0.1557],
        [ 1.5763, -0.4629, -0.5511],
        [-0.2084,  1.1663, -1.8349],
        [-0.1498, -0.0937, -0.0156],
        [-1.5478, -0.6390, -0.2962],
        [ 0.3162,  0.1714, -0.4450],
        [ 0.6451, -0.4480,  0.6902],
        [-1.3963,  0.4275,  0.0231],
        [ 1.0929, -1.1854,  0.3694],
        [-1.5989,  0.3886, -0.4314],
        [ 0.0420, -2.0618, -0.2124]], device='cuda:0')
after: tensor([[-0.9603, -1.0131,  0.7534],
        [ 0.1782, -0.0693,  1.1642],
        [ 0.6518,  0.0169, -1.4375],
        [ 0.5991,  0.1760,  0.6608],
        [ 0.5417, -1.7350,  0.4834],
        [ 1.4317, -0.2399, -0.1557],
        [ 1.5763, -0.4629, -0.5511],
        [-0.2084,  1.1663, -1.8349],
        [-0.1498, -0.0937, -0.0156],
        [-1.5478, -0.6390, -0.2962],
        [ 0.3162,  0.1714, -0.4450],
        [ 0.6451, -0.4480,  0.6902],
        [-1.3963,  0.4275,  0.0231],
        [ 1.0929, -1.1854,  0.3694],
        [-1.5989,  0.3886, -0.4314],
        [ 0.0420, -2.0618, -0.2124]], device='cuda:0')
********************
observing microbatch 3
current batch shape is torch.Size([16, 3])
batch is tensor([[-0.9603, -1.0131,  0.7534],
        [ 0.1782, -0.0693,  1.1642],
        [ 0.6518,  0.0169, -1.4375],
        [ 0.5991,  0.1760,  0.6608],
        [ 0.5417, -1.7350,  0.4834],
        [ 1.4317, -0.2399, -0.1557],
        [ 1.5763, -0.4629, -0.5511],
        [-0.2084,  1.1663, -1.8349],
        [-0.1498, -0.0937, -0.0156],
        [-1.5478, -0.6390, -0.2962],
        [ 0.3162,  0.1714, -0.4450],
        [ 0.6451, -0.4480,  0.6902],
        [-1.3963,  0.4275,  0.0231],
        [ 1.0929, -1.1854,  0.3694],
        [-1.5989,  0.3886, -0.4314],
        [ 0.0420, -2.0618, -0.2124]], device='cuda:0')
about to be put into Sequential(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:0
task is partition: 0 microbatch: 3
********************
inputting microbatch 2 into partition 1
before moving to cuda:1: tensor([[0.4347, 0.6367, 0.6138, 0.5449],
        [0.4949, 0.6921, 0.5498, 0.5586],
        [0.3414, 0.6599, 0.5953, 0.3577],
        [0.6532, 0.4729, 0.5579, 0.7265],
        [0.3661, 0.5532, 0.6457, 0.4439],
        [0.6699, 0.3657, 0.5771, 0.7382],
        [0.4808, 0.4426, 0.6061, 0.5162],
        [0.7137, 0.6076, 0.4225, 0.6741],
        [0.3486, 0.5240, 0.6689, 0.4442],
        [0.5246, 0.5858, 0.5530, 0.5605],
        [0.3935, 0.5017, 0.6691, 0.5157],
        [0.4724, 0.2967, 0.6615, 0.5352],
        [0.4393, 0.3113, 0.6608, 0.4852],
        [0.6413, 0.3387, 0.5739, 0.6770],
        [0.6007, 0.5146, 0.5825, 0.7010],
        [0.5952, 0.2491, 0.5918, 0.5829]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[0.9966, 0.9957, 0.9900, 0.9897],
        [0.9900, 0.9954, 0.9946, 0.9869],
        [0.9867, 0.9871, 0.9944, 0.9933],
        [0.9839, 0.9836, 0.9842, 0.9936],
        [0.9924, 0.9818, 0.9813, 0.9820],
        [0.9935, 0.9923, 0.9815, 0.9811],
        [0.9817, 0.9944, 0.9933, 0.9840],
        [0.9837, 0.9842, 0.9944, 0.9933],
        [0.9839, 0.9837, 0.9842, 0.9944],
        [0.9933, 0.9839, 0.9836, 0.9842],
        [0.9936, 0.9924, 0.9818, 0.9813],
        [0.9820, 0.9935, 0.9923, 0.9815],
        [0.9811, 0.9817, 0.9944, 0.9933],
        [0.9840, 0.9837, 0.9842, 0.9944],
        [0.9933, 0.9839, 0.9837, 0.9842],
        [0.9944, 0.9933, 0.9839, 0.9836]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 2
current batch shape is torch.Size([16, 4])
batch is tensor([[0.9966, 0.9957, 0.9900, 0.9897],
        [0.9900, 0.9954, 0.9946, 0.9869],
        [0.9867, 0.9871, 0.9944, 0.9933],
        [0.9839, 0.9836, 0.9842, 0.9936],
        [0.9924, 0.9818, 0.9813, 0.9820],
        [0.9935, 0.9923, 0.9815, 0.9811],
        [0.9817, 0.9944, 0.9933, 0.9840],
        [0.9837, 0.9842, 0.9944, 0.9933],
        [0.9839, 0.9837, 0.9842, 0.9944],
        [0.9933, 0.9839, 0.9836, 0.9842],
        [0.9936, 0.9924, 0.9818, 0.9813],
        [0.9820, 0.9935, 0.9923, 0.9815],
        [0.9811, 0.9817, 0.9944, 0.9933],
        [0.9840, 0.9837, 0.9842, 0.9944],
        [0.9933, 0.9839, 0.9837, 0.9842],
        [0.9944, 0.9933, 0.9839, 0.9836]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 2
********************
receiving microbatch 3 from partition 0
getting result from out queue
status True result (partition: 0 microbatch: 3, tensor([[0.6370, 0.5556, 0.5168, 0.6712],
        [0.5542, 0.4143, 0.5693, 0.5594],
        [0.4218, 0.3134, 0.7288, 0.5996],
        [0.5039, 0.3535, 0.6217, 0.5413],
        [0.7387, 0.3639, 0.5211, 0.7665],
        [0.5383, 0.2523, 0.6614, 0.6191],
        [0.5524, 0.2339, 0.6742, 0.6546],
        [0.2479, 0.4040, 0.7844, 0.4456],
        [0.4911, 0.4342, 0.6324, 0.5796],
        [0.5229, 0.6087, 0.5918, 0.6370],
        [0.4428, 0.3681, 0.6792, 0.5583],
        [0.5922, 0.3507, 0.5855, 0.6216],
        [0.3953, 0.5918, 0.6356, 0.4953],
        [0.6808, 0.2976, 0.5713, 0.7176],
        [0.3746, 0.6088, 0.6558, 0.5062],
        [0.7384, 0.4138, 0.5352, 0.8003]], device='cuda:0',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 3
result is tensor([[0.6370, 0.5556, 0.5168, 0.6712],
        [0.5542, 0.4143, 0.5693, 0.5594],
        [0.4218, 0.3134, 0.7288, 0.5996],
        [0.5039, 0.3535, 0.6217, 0.5413],
        [0.7387, 0.3639, 0.5211, 0.7665],
        [0.5383, 0.2523, 0.6614, 0.6191],
        [0.5524, 0.2339, 0.6742, 0.6546],
        [0.2479, 0.4040, 0.7844, 0.4456],
        [0.4911, 0.4342, 0.6324, 0.5796],
        [0.5229, 0.6087, 0.5918, 0.6370],
        [0.4428, 0.3681, 0.6792, 0.5583],
        [0.5922, 0.3507, 0.5855, 0.6216],
        [0.3953, 0.5918, 0.6356, 0.4953],
        [0.6808, 0.2976, 0.5713, 0.7176],
        [0.3746, 0.6088, 0.6558, 0.5062],
        [0.7384, 0.4138, 0.5352, 0.8003]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.6370, 0.5556, 0.5168, 0.6712],
        [0.5542, 0.4143, 0.5693, 0.5594],
        [0.4218, 0.3134, 0.7288, 0.5996],
        [0.5039, 0.3535, 0.6217, 0.5413],
        [0.7387, 0.3639, 0.5211, 0.7665],
        [0.5383, 0.2523, 0.6614, 0.6191],
        [0.5524, 0.2339, 0.6742, 0.6546],
        [0.2479, 0.4040, 0.7844, 0.4456],
        [0.4911, 0.4342, 0.6324, 0.5796],
        [0.5229, 0.6087, 0.5918, 0.6370],
        [0.4428, 0.3681, 0.6792, 0.5583],
        [0.5922, 0.3507, 0.5855, 0.6216],
        [0.3953, 0.5918, 0.6356, 0.4953],
        [0.6808, 0.2976, 0.5713, 0.7176],
        [0.3746, 0.6088, 0.6558, 0.5062],
        [0.7384, 0.4138, 0.5352, 0.8003]], device='cuda:0',
       grad_fn=<SigmoidBackward0>).shape
********************
receiving microbatch 2 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 2, tensor([[0.9974, 0.9968, 0.9923, 0.9921, 0.9925],
        [0.9974, 0.9968, 0.9923, 0.9921, 0.9924],
        [0.9973, 0.9968, 0.9923, 0.9921, 0.9924],
        [0.9973, 0.9968, 0.9921, 0.9919, 0.9923],
        [0.9973, 0.9967, 0.9921, 0.9919, 0.9922],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9919, 0.9923],
        [0.9973, 0.9968, 0.9921, 0.9919, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9919, 0.9923],
        [0.9973, 0.9968, 0.9921, 0.9919, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 2
result is tensor([[0.9974, 0.9968, 0.9923, 0.9921, 0.9925],
        [0.9974, 0.9968, 0.9923, 0.9921, 0.9924],
        [0.9973, 0.9968, 0.9923, 0.9921, 0.9924],
        [0.9973, 0.9968, 0.9921, 0.9919, 0.9923],
        [0.9973, 0.9967, 0.9921, 0.9919, 0.9922],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9919, 0.9923],
        [0.9973, 0.9968, 0.9921, 0.9919, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9919, 0.9923],
        [0.9973, 0.9968, 0.9921, 0.9919, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.9974, 0.9968, 0.9923, 0.9921, 0.9925],
        [0.9974, 0.9968, 0.9923, 0.9921, 0.9924],
        [0.9973, 0.9968, 0.9923, 0.9921, 0.9924],
        [0.9973, 0.9968, 0.9921, 0.9919, 0.9923],
        [0.9973, 0.9967, 0.9921, 0.9919, 0.9922],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9919, 0.9923],
        [0.9973, 0.9968, 0.9921, 0.9919, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9919, 0.9923],
        [0.9973, 0.9968, 0.9921, 0.9919, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9795, 0.9747, 0.9446, 0.9441, 0.9438],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9966, 0.9957, 0.9900, 0.9897, 0.9900],
        [0.9954, 0.9946, 0.9869, 0.9867, 0.9871],
        [0.9944, 0.9933, 0.9839, 0.9836, 0.9842],
        [0.9936, 0.9924, 0.9818, 0.9813, 0.9820],
        [0.9935, 0.9923, 0.9815, 0.9811, 0.9817],
        [0.9944, 0.9933, 0.9840, 0.9837, 0.9842],
        [0.9944, 0.9933, 0.9839, 0.9837, 0.9842],
        [0.9944, 0.9933, 0.9839, 0.9836, 0.9842],
        [0.9936, 0.9924, 0.9818, 0.9813, 0.9820],
        [0.9935, 0.9923, 0.9815, 0.9811, 0.9817],
        [0.9944, 0.9933, 0.9840, 0.9837, 0.9842],
        [0.9944, 0.9933, 0.9839, 0.9837, 0.9842],
        [0.9944, 0.9933, 0.9839, 0.9836, 0.9842],
        [0.9936, 0.9924, 0.9818, 0.9813, 0.9820],
        [0.9935, 0.9923, 0.9815, 0.9811, 0.9817],
        [0.9944, 0.9933, 0.9840, 0.9837, 0.9842]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9974, 0.9968, 0.9923, 0.9921, 0.9925],
        [0.9974, 0.9968, 0.9923, 0.9921, 0.9924],
        [0.9973, 0.9968, 0.9923, 0.9921, 0.9924],
        [0.9973, 0.9968, 0.9921, 0.9919, 0.9923],
        [0.9973, 0.9967, 0.9921, 0.9919, 0.9922],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9919, 0.9923],
        [0.9973, 0.9968, 0.9921, 0.9919, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9919, 0.9923],
        [0.9973, 0.9968, 0.9921, 0.9919, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.6370, 0.5556, 0.5168, 0.6712],
        [0.5542, 0.4143, 0.5693, 0.5594],
        [0.4218, 0.3134, 0.7288, 0.5996],
        [0.5039, 0.3535, 0.6217, 0.5413],
        [0.7387, 0.3639, 0.5211, 0.7665],
        [0.5383, 0.2523, 0.6614, 0.6191],
        [0.5524, 0.2339, 0.6742, 0.6546],
        [0.2479, 0.4040, 0.7844, 0.4456],
        [0.4911, 0.4342, 0.6324, 0.5796],
        [0.5229, 0.6087, 0.5918, 0.6370],
        [0.4428, 0.3681, 0.6792, 0.5583],
        [0.5922, 0.3507, 0.5855, 0.6216],
        [0.3953, 0.5918, 0.6356, 0.4953],
        [0.6808, 0.2976, 0.5713, 0.7176],
        [0.3746, 0.6088, 0.6558, 0.5062],
        [0.7384, 0.4138, 0.5352, 0.8003]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)]
====================
schedule for this step is
[(3, 1)]
inputting microbatch 3 into partition 1
before moving to cuda:1: tensor([[0.6370, 0.5556, 0.5168, 0.6712],
        [0.5542, 0.4143, 0.5693, 0.5594],
        [0.4218, 0.3134, 0.7288, 0.5996],
        [0.5039, 0.3535, 0.6217, 0.5413],
        [0.7387, 0.3639, 0.5211, 0.7665],
        [0.5383, 0.2523, 0.6614, 0.6191],
        [0.5524, 0.2339, 0.6742, 0.6546],
        [0.2479, 0.4040, 0.7844, 0.4456],
        [0.4911, 0.4342, 0.6324, 0.5796],
        [0.5229, 0.6087, 0.5918, 0.6370],
        [0.4428, 0.3681, 0.6792, 0.5583],
        [0.5922, 0.3507, 0.5855, 0.6216],
        [0.3953, 0.5918, 0.6356, 0.4953],
        [0.6808, 0.2976, 0.5713, 0.7176],
        [0.3746, 0.6088, 0.6558, 0.5062],
        [0.7384, 0.4138, 0.5352, 0.8003]], device='cuda:0',
       grad_fn=<SigmoidBackward0>)
after: tensor([[-3.6893e+19,  1.8730e+00,  9.9235e-01,  9.9213e-01],
        [ 9.9246e-01,  9.9735e-01,  9.9683e-01,  9.9231e-01],
        [ 9.9210e-01,  9.9243e-01,  9.9734e-01,  9.9681e-01],
        [ 9.9227e-01,  9.9205e-01,  9.9239e-01,  9.9729e-01],
        [ 9.9676e-01,  9.9215e-01,  9.9192e-01,  9.9226e-01],
        [ 9.9727e-01,  9.9674e-01,  9.9209e-01,  9.9187e-01],
        [ 9.9221e-01,  9.9730e-01,  9.9677e-01,  9.9217e-01],
        [ 9.9195e-01,  9.9229e-01,  9.9732e-01,  9.9679e-01],
        [ 9.9221e-01,  9.9199e-01,  9.9233e-01,  9.9732e-01],
        [ 9.9679e-01,  9.9222e-01,  9.9200e-01,  9.9234e-01],
        [ 9.9730e-01,  9.9676e-01,  9.9215e-01,  9.9193e-01],
        [ 9.9227e-01,  9.9729e-01,  9.9676e-01,  9.9214e-01],
        [ 9.9193e-01,  9.9226e-01,  9.9730e-01,  9.9677e-01],
        [ 9.9218e-01,  9.9196e-01,  9.9229e-01,  9.9730e-01],
        [ 9.9677e-01,  9.9217e-01,  9.9196e-01,  9.9229e-01],
        [ 9.9731e-01,  9.9678e-01,  9.9218e-01,  9.9197e-01]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
********************
observing microbatch 3
current batch shape is torch.Size([16, 4])
batch is tensor([[-3.6893e+19,  1.8730e+00,  9.9235e-01,  9.9213e-01],
        [ 9.9246e-01,  9.9735e-01,  9.9683e-01,  9.9231e-01],
        [ 9.9210e-01,  9.9243e-01,  9.9734e-01,  9.9681e-01],
        [ 9.9227e-01,  9.9205e-01,  9.9239e-01,  9.9729e-01],
        [ 9.9676e-01,  9.9215e-01,  9.9192e-01,  9.9226e-01],
        [ 9.9727e-01,  9.9674e-01,  9.9209e-01,  9.9187e-01],
        [ 9.9221e-01,  9.9730e-01,  9.9677e-01,  9.9217e-01],
        [ 9.9195e-01,  9.9229e-01,  9.9732e-01,  9.9679e-01],
        [ 9.9221e-01,  9.9199e-01,  9.9233e-01,  9.9732e-01],
        [ 9.9679e-01,  9.9222e-01,  9.9200e-01,  9.9234e-01],
        [ 9.9730e-01,  9.9676e-01,  9.9215e-01,  9.9193e-01],
        [ 9.9227e-01,  9.9729e-01,  9.9676e-01,  9.9214e-01],
        [ 9.9193e-01,  9.9226e-01,  9.9730e-01,  9.9677e-01],
        [ 9.9218e-01,  9.9196e-01,  9.9229e-01,  9.9730e-01],
        [ 9.9677e-01,  9.9217e-01,  9.9196e-01,  9.9229e-01],
        [ 9.9731e-01,  9.9678e-01,  9.9218e-01,  9.9197e-01]], device='cuda:1',
       grad_fn=<ToCopyBackward0>)
about to be put into Sequential(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): WithDevice(
    (_module): Sigmoid()
  )
) on device cuda:1
task is partition: 1 microbatch: 3
********************
receiving microbatch 3 from partition 1
getting result from out queue
status True result (partition: 1 microbatch: 3, tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9921, 0.9925],
        [0.9974, 0.9968, 0.9924, 0.9921, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9921, 0.9925],
        [0.9974, 0.9968, 0.9924, 0.9921, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9921, 0.9925],
        [0.9974, 0.9968, 0.9924, 0.9921, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925]], device='cuda:1',
       grad_fn=<SigmoidBackward0>))
********************
observing microbatch 3
result is tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9921, 0.9925],
        [0.9974, 0.9968, 0.9924, 0.9921, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9921, 0.9925],
        [0.9974, 0.9968, 0.9924, 0.9921, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9921, 0.9925],
        [0.9974, 0.9968, 0.9924, 0.9921, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)
result shape is tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9921, 0.9925],
        [0.9974, 0.9968, 0.9924, 0.9921, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9921, 0.9925],
        [0.9974, 0.9968, 0.9924, 0.9921, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9921, 0.9925],
        [0.9974, 0.9968, 0.9924, 0.9921, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925]], device='cuda:1',
       grad_fn=<SigmoidBackward0>).shape
********************
step complete
====================
what batches looks like now
[tensor([[0.9795, 0.9747, 0.9446, 0.9441, 0.9438],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297],
        [0.8808, 0.8652, 0.7297, 0.7297, 0.7297]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9966, 0.9957, 0.9900, 0.9897, 0.9900],
        [0.9954, 0.9946, 0.9869, 0.9867, 0.9871],
        [0.9944, 0.9933, 0.9839, 0.9836, 0.9842],
        [0.9936, 0.9924, 0.9818, 0.9813, 0.9820],
        [0.9935, 0.9923, 0.9815, 0.9811, 0.9817],
        [0.9944, 0.9933, 0.9840, 0.9837, 0.9842],
        [0.9944, 0.9933, 0.9839, 0.9837, 0.9842],
        [0.9944, 0.9933, 0.9839, 0.9836, 0.9842],
        [0.9936, 0.9924, 0.9818, 0.9813, 0.9820],
        [0.9935, 0.9923, 0.9815, 0.9811, 0.9817],
        [0.9944, 0.9933, 0.9840, 0.9837, 0.9842],
        [0.9944, 0.9933, 0.9839, 0.9837, 0.9842],
        [0.9944, 0.9933, 0.9839, 0.9836, 0.9842],
        [0.9936, 0.9924, 0.9818, 0.9813, 0.9820],
        [0.9935, 0.9923, 0.9815, 0.9811, 0.9817],
        [0.9944, 0.9933, 0.9840, 0.9837, 0.9842]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.9974, 0.9968, 0.9923, 0.9921, 0.9925],
        [0.9974, 0.9968, 0.9923, 0.9921, 0.9924],
        [0.9973, 0.9968, 0.9923, 0.9921, 0.9924],
        [0.9973, 0.9968, 0.9921, 0.9919, 0.9923],
        [0.9973, 0.9967, 0.9921, 0.9919, 0.9922],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9919, 0.9923],
        [0.9973, 0.9968, 0.9921, 0.9919, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9919, 0.9923],
        [0.9973, 0.9968, 0.9921, 0.9919, 0.9923],
        [0.9973, 0.9968, 0.9922, 0.9920, 0.9923]], device='cuda:1',
       grad_fn=<SigmoidBackward0>), tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9921, 0.9925],
        [0.9974, 0.9968, 0.9924, 0.9921, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9921, 0.9925],
        [0.9974, 0.9968, 0.9924, 0.9921, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9921, 0.9925],
        [0.9974, 0.9968, 0.9924, 0.9921, 0.9925],
        [0.9974, 0.9969, 0.9924, 0.9922, 0.9925]], device='cuda:1',
       grad_fn=<SigmoidBackward0>)]
finished running cycle
=============================== warnings summary ===============================
tests/test_pipeline.py::test_forward_0[1-1]
  /home/abcheng/miniconda3/envs/hw4/lib/python3.9/site-packages/torch/nn/modules/linear.py:116: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)
    return F.linear(input, self.weight, self.bias)

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/test_pipeline.py::test_forward_0[1-1] - assert False
FAILED tests/test_pipeline.py::test_forward_0[1-16] - assert False
FAILED tests/test_pipeline.py::test_forward_0[1-32] - assert False
FAILED tests/test_pipeline.py::test_forward_0[1-64] - assert False
FAILED tests/test_pipeline.py::test_forward_0[2-1] - assert False
FAILED tests/test_pipeline.py::test_forward_0[2-16] - assert False
FAILED tests/test_pipeline.py::test_forward_0[2-32] - assert False
FAILED tests/test_pipeline.py::test_forward_0[2-64] - assert False
FAILED tests/test_pipeline.py::test_forward_0[4-1] - assert False
FAILED tests/test_pipeline.py::test_forward_0[4-16] - assert False
FAILED tests/test_pipeline.py::test_forward_0[4-32] - assert False
FAILED tests/test_pipeline.py::test_forward_0[4-64] - assert False
FAILED tests/test_pipeline.py::test_forward_0[8-1] - assert False
FAILED tests/test_pipeline.py::test_forward_0[8-16] - assert False
FAILED tests/test_pipeline.py::test_forward_0[8-32] - assert False
FAILED tests/test_pipeline.py::test_forward_0[8-64] - assert False
FAILED tests/test_pipeline.py::test_forward_0[16-1] - assert False
FAILED tests/test_pipeline.py::test_forward_0[16-16] - assert False
FAILED tests/test_pipeline.py::test_forward_0[16-32] - assert False
FAILED tests/test_pipeline.py::test_forward_0[16-64] - assert False
================ 20 failed, 14 deselected, 1 warning in 18.43s =================
